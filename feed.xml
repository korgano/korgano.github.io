<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>XS Tech Thoughts</title>
    <link href="https://korgano.github.io/feed.xml" rel="self" />
    <link href="https://korgano.github.io" />
    <updated>2023-09-24T18:27:50-04:00</updated>
    <author>
        <name>Xavier Santana</name>
    </author>
    <id>https://korgano.github.io</id>

    <entry>
        <title>The UX &amp; Cybersecurity Case for More Local AI Hardware</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/local-ai-hardware-ux-cybersecurity/"/>
        <id>https://korgano.github.io/local-ai-hardware-ux-cybersecurity/</id>
        <media:content url="https://korgano.github.io/media/posts/5/steve-johnson-_0iV9LmPDn0-unsplash.jpg" medium="image" />
            <category term="UX"/>
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC Hardware"/>
            <category term="PC"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2023-09-24T18:27:50-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/5/steve-johnson-_0iV9LmPDn0-unsplash.jpg" alt="" />
                    Earlier this week, two things crossed my path: Both of these are great things in my opinion. Let's get into&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/5/steve-johnson-_0iV9LmPDn0-unsplash.jpg" class="type:primaryImage" alt="" /></p>
                <p>Earlier this week, two things crossed my path:</p>
<ol>
<li><a href="https://chipsandcheese.com/2023/09/16/hot-chips-2023-amds-phoenix-soc/" title="Hot Chips 2023: AMD’s Phoenix SoC" target="_blank" rel="noopener noreferrer">A fantastic breakdown of AMD's latest laptop processors that feature their XDNA AI engines.</a></li>
<li>Intel's Innovation Days presentation, which showcased their new processors with local AI hardware (Neural Processing Units - NPUs).</li>
</ol>
<p>Both of these are great things in my opinion. Let's get into why.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1hauaonv2121">How Do People Use AI?</a></li>
<li><a href="#mcetoc_1hauaonv2122">The Cybersecurity Issues</a></li>
<li><a href="#mcetoc_1hauaonv2123">The UX Issues</a></li>
<li><a href="#mcetoc_1hauaonv2124">The Two Paths</a></li>
<li><a href="#mcetoc_1hauaonv2125">Wrap Up</a></li>
</ul>
</div>
<h3 id="mcetoc_1hauaonv2121">How Do People Use AI?</h3>
<p>The simplest possible explanation of the AI systems in the current zeitgeist is that they are pattern recognition and prediction machines. These Large Language Models (or LLMs) are trained on massive amounts of data. (We won't get into the ethical or artistic issues with this. If you want that, watch the following video:)</p>
<p class="align-center"><div class="post__iframe"><iframe loading="lazy" width="560" height="315" src="https://www.youtube-nocookie.com/embed/-MUEXGaxFDA?si=xcHgi-q_L6R9vt5c" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="allowfullscreen"></iframe></div>
<p>The user will use some kind of interface - typically text based, but some visual interfaces do exist. The LLM will process the input, figure out the pattern of that input, and then predict the output based on that pattern. (This, incidentally, leads to the "hallucination" problem, where LLM based AI generate stuff that has no actual basis in the prompt or fact.)</p>
<p>There are two main ways these LLM based AI operate:</p>
<ol>
<li>On massive server farms.</li>
<li>On very expensive graphics cards or special processors.</li>
</ol>
<h3 id="mcetoc_1hauaonv2122">The Cybersecurity Issues</h3>
<p>Like it or not, everyone is going to be using LLM based AI in the near future, because if you know how to use them well, you can get some impressive productivity gains.</p>
<p>It also doesn't take a genius to recognize the problems here, especially in the context of the CIA triad:</p>
<ol>
<li>Confidentiality</li>
<li>Integrity</li>
<li>Availability</li>
</ol>
<p>Confidentiality, or keeping data out of the hands of anyone who is <strong>not </strong>supposed to have access, runs into a number of problems when you have server based LLMs:</p>
<ul>
<li>Can you keep the data confidential if you have to use someone else's LLM servers?</li>
<li>Can you ensure that the LLM provider isn't going to incorporate the data into their LLM's training model?</li>
<li>Increased risk of bad actors intercepting data as it moves between the end user PC and the server.</li>
<li>Leaky or improperly routed VPNs delivering the data to outside actors.</li>
<li>Increased opportunity for corporate competitors to track your progress by assessing volume of traffic between company PCs and LLM servers.</li>
</ul>
<p>And these are just the obvious confidentiality issues that I thought of in five minutes. Things are just as bad when we look at Integrity:</p>
<ul>
<li>Data manipulation of the input to the LLM as it travels to the server from the end user.</li>
<li>Data manipulation of the output from the LLM as it travels from the server to the end user.</li>
<li>Data corruption due to connection issues between the LLM server and end user.</li>
<li>Data corruption due to unannounced changes in how the LLM processes data.</li>
</ul>
<p>When it comes to availability, we've got a whole new set of problems:</p>
<ul>
<li>Stability of connection between the end user and LLM server.</li>
<li>Availability of LLM server resources.</li>
<li>Potential DDOS of the LLM server/port.</li>
<li>Local hardware being expensive and hard to obtain.</li>
<li>Local hardware availability.</li>
<li>Local hardware connection issues.</li>
</ul>
<h3 id="mcetoc_1hauaonv2123">The UX Issues</h3>
<p>The UX issues are honestly a near 1:1 overlap with the Cybersecurity ones, plus a few new ones:</p>
<ul>
<li>The inconvenience of having to sign up for an account.</li>
<li>Limited options to customize the interface.</li>
<li>Limited ability to use LLMs optimized to specific tasks.</li>
<li>Limited ability to use LLMs with little to no training/output bias.</li>
<li>Random service issues when LLM load is high.</li>
</ul>
<p>Essentially, AI as tool has a lot of hurdles <strong>because </strong>it's not practical to run on a local machine. But what if we could?</p>
<h3 id="mcetoc_1hauaonv2124">The Two Paths</h3>
<figure ><figure class="post__image post__image--full"><img loading="lazy"  src="https://i0.wp.com/chipsandcheese.com/wp-content/uploads/2023/09/amd_phx_if.jpg?ssl=1" alt="AMD slide showcasing 7040 &quot;Phoenix&quot; Processor, via Chips and Cheese" width="2557" height="1431" data-is-external-image="true"></figure>
<figcaption >AMD Ryzen 7040 "Phoenix" processor diagram, via Chips and Cheese</figcaption>
</figure>
<p>There are two main paths to get more local AI hardware in people's hands. The first is building it into new computer processors, like AMD and Intel are doing. These are focused on "inference," which is just a fancy way of saying "taking input into a trained LLM and generating output."</p>
<p>(Note: Intel's AI solution is called Neural Processing Units, but I will refer only to AI engines, because it does the same thing and is easier to remember.)</p>
<p>There are two main ways to do this:</p>
<ol>
<li>Devote a small part of the overall processor to AI functions.</li>
<li>Devote a large part of the overall processor to AI functions.</li>
</ol>
<p>Currently, Intel and AMD are releasing products that take the first way to do things. This makes sense from a risk mitigation point of view, but also constitutes a huge bet on the part of the hardware designers. They're not only betting on getting the design right, but that the total performance is going to be good enough for most of the lifespan of the CPU.</p>
<p>When you're a corporation that can justify replacing entire computers (or just the CPUs, if you're lucky) every 3-4 years, that's an acceptable risk. For an individual or small organization, the upsides are pretty high, but if you rely on AI for a lot of tasks and the programming for LLMs suddenly changes in a way your AI engine/other PC hardware can't handle, you're in a big pickle.</p>
<p>The other way to do it is to make a big chunk of the CPU an AI engine, like so:</p>
<figure ><figure class="post__image post__image--full"><img loading="lazy"  src="https://korgano.github.io/media/posts/5/mlid-june2023-AIE.png" alt="Moore's Law is Dead slide explaining Zen 5 Turin-AI Chiplets." width="1920" height="1055" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/5/responsive/mlid-june2023-AIE-xs.png 384w ,https://korgano.github.io/media/posts/5/responsive/mlid-june2023-AIE-sm.png 600w ,https://korgano.github.io/media/posts/5/responsive/mlid-june2023-AIE-md.png 768w ,https://korgano.github.io/media/posts/5/responsive/mlid-june2023-AIE-lg.png 1200w ,https://korgano.github.io/media/posts/5/responsive/mlid-june2023-AIE-xl.png 1600w"></figure>
<figcaption >Moore's Law is Dead slide explaining Zen 5 Turin-AI performance and configurations.</figcaption>
</figure>
<p><a href="https://www.youtube.com/watch?v=QU1DFBtbRWY" title="AMD 12C Zen 5 Strix on AM5: R9 8900G instead of 8900? (+ Turin-AI Update)" target="_blank" rel="noopener noreferrer">According to tech commentary and analysis channel Moore's Law is Dead</a>, AMD has developed a chiplet that provide more than enough performance for any individual's AI tasks.</p>
<p>For those unfamiliar with how AMD's CPUs are constructed, a chiplet is a small silicon chip with features that would form part of a single larger CPU. So in AMD's case, the actual processor cores are in one or more chiplets, while all the parts that communicate with the rest of the computer are in another. AMD designs its chiplets so that they can work across server, workstation, and desktop PC products.</p>
<p>With AI hardware in high demand, AMD is certainly going to make AI chiplets for servers. But workstation and desktop are a big question mark, especially since servers get the top tier chiplets. Theoretically, even the worst tier AI engine chiplets would provide excessive amounts of LLM AI task performance for even prosumer users.</p>
<p>Will AMD make such a CPU? Maybe. Go let AMD's social media accounts know you want that.</p>
<p>And this leads us to the second path, the M.2 add-on card.</p>
<p>In case you don't know what an M.2 anything is, here's an example:</p>
<figure class="post__image"><img loading="lazy"  src="https://assetsio.reedpopcdn.com/WD-Black-SN850X-SSD_5rmoNZy.jpg?width=690&amp;quality=80&amp;format=jpg&amp;auto=webp" alt="WD Black SN850X M.2 NVMe SSD" width="690" height="388" data-is-external-image="true"></figure>
<p>If something the size of a stick of gum that could handle all your AI engine needs sounds attractive, you are not alone. <a href="https://youtu.be/tl1m09ZyfDg?t=2042" title="Meteor Lake Tech Tour Deep Dive" target="null">As it turns out, the company behind Intel's AI engine has actually <strong>made </strong>these devices</a>... they are just being sold to industrial firms. But the fact that they're aware that people want them means there's a chance we can get them down the line.</p>
<p>What's especially exciting about the idea of an AI engine on an M.2 is the fact that it gives people <strong>options</strong>. It's not a huge component, or something that's going to be stuck under a heat sink/watercooling solution that's a pain to disassemble. You could stick them in any computer with an M.2 drive, even if the performance wouldn't be great for older PCs. You could potentially have two or more side-by-side in a special adapter that would give you tons of AI processing capabilities.</p>
<p>And this is before we even get into the possibilities of combining the two paths by using <strong>both </strong>CPU integrated and M.2 add-on AI engines.</p>
<h3 id="mcetoc_1hauaonv2125">Wrap Up</h3>
<p>To sum it all up, more local AI hardware solves a number of cybersecurity problems:</p>
<table style="border-collapse: collapse; width: 100%;" border="1">
<tbody>
<tr>
<td class="align-center" style="width: 33.2853%;"><strong>Confidentiality</strong></td>
<td class="align-center" style="width: 33.2853%;"><strong>Integrity</strong></td>
<td class="align-center" style="width: 33.287%;"><strong>Availability</strong></td>
</tr>
<tr>
<td style="width: 33.2853%;">Keeping data confidential.</td>
<td style="width: 33.2853%;">Data manipulation of LLM input enroute to LLM.</td>
<td style="width: 33.287%;">Stability of connection between the end user/LLM server.</td>
</tr>
<tr>
<td style="width: 33.2853%;">Risk of LLM developer incorporating confidential data in training model.</td>
<td style="width: 33.2853%;">Data manipulation of LLM output enroute to end user.</td>
<td style="width: 33.287%;">Availability of LLM server resources.</td>
</tr>
<tr>
<td style="width: 33.2853%;">Interception of data enroute to LLM.</td>
<td style="width: 33.2853%;">Data corruption due to issues in LLM/end user connection.</td>
<td style="width: 33.287%;">Potential DDOS of the LLM server/port.</td>
</tr>
<tr>
<td style="width: 33.2853%;">Improper VPN setups leaking data/delivering data to wrong destination.</td>
<td style="width: 33.2853%;">Data corruption due to unannounced LLM changes.</td>
<td style="width: 33.287%;">Local hardware availability.</td>
</tr>
<tr>
<td style="width: 33.2853%;">Less traffic for competitors to analyze.</td>
<td style="width: 33.2853%;"> </td>
<td style="width: 33.287%;">Local hardware connection issues.</td>
</tr>
</tbody>
</table>
<p>It also provides opportunities to solve the following UX problems:</p>
<ul>
<li>The inconvenience of having to sign up for an account.</li>
<li>Limited options to customize the interface.</li>
<li>Limited ability to use LLMs optimized to specific tasks.</li>
<li>Limited ability to use LLMs with little to no training/output bias.</li>
<li>Random service issues when LLM load is high.</li>
</ul>
<p>With two main paths to getting more local AI hardware, either as parts of CPUs or in standalone add-on cards, the future looks bright for local AI. But it'll take a decent amount of yelling at the social media clouds to get everyone the hardware they deserve. And without that hardware being available, there's not much incentive or ability to work on the UX problems.</p>
<p>So let's get that social media campaign going, and </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>User Experience Report: Publii, Day 1</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/user-experience-report-publii-day-1/"/>
        <id>https://korgano.github.io/user-experience-report-publii-day-1/</id>
        <media:content url="https://korgano.github.io/media/posts/4/vadim-sherbakov-RcdV8rnXSeE-unsplash.jpg" medium="image" />
            <category term="UX"/>
            <category term="Tech"/>
            <category term="Quick Thoughts"/>

        <updated>2023-09-14T15:16:54-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/4/vadim-sherbakov-RcdV8rnXSeE-unsplash.jpg" alt="Two flat screen monitor turned on near organizer rack inside the room." />
                    This is going to be a fairly brief, by my standards, post. Rebuilding a portfolio site from scratch is a&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/4/vadim-sherbakov-RcdV8rnXSeE-unsplash.jpg" class="type:primaryImage" alt="Two flat screen monitor turned on near organizer rack inside the room." /></p>
                <p>This is going to be a fairly brief, by my standards, post.</p>
<p>Rebuilding a portfolio site from scratch is a time-consuming task. While I considered building one from scratch with plain old HTML, CSS, and Bootstrap, I realized that wasn't practical. So I looked for a Content Management System that was free and could upload to Github pages.</p>
<p>And I found Publii, which is all those things, and locally hosted!</p>
<p>In terms of User Interface and Experience, I much prefer it to WordPress. That's simply because Publii isn't at the stage where it has tons and tons of settings and add-ons to configure. It's got the essentials, some nice to have features, good layout, and good documentation.</p>
<p>It's good enough, and I'm fine with that.</p>
<p>There are some oddities when working with themes though. I'm currently using the free version of the <a href="https://marketplace.getpublii.com/themes/persona/" target="_blank" rel="noopener noreferrer">Persona theme</a>, and some of the color options don't do what you expect.</p>
<p>For one thing, I set the link hover option to a shade of green, and the text to a shade of blue. But on the nav menu and author names, the colors are reversed. (I think it looks good, so I'm not complaining, but it is weird.)</p>
<p>The animated line's color was controlled by a "Color" setting - that's right, no real details, just the word "Color". Luckily, I didn't do anything horrible in changing it, but the lack of clarity was a definite knock on the theme. The inability to further customize the animation is annoying, but perhaps that an exclusive feature of the paid themes.</p>
<p>Speed and performance of the software is good, and I <strong>greatly </strong>appreciate the ability to choose "Save draft" as an option before I save anything I post.</p>
<p>One limitation of the software is that it seems excessively blog focused. You can't actually make a standard page. You just make posts, and then use a specialized page to show off all the posts with a specific tag. It's quite odd, but not a huge negative.</p>
<p>One <strong>huge </strong>positive is the fact that I can actually set the width of various elements quickly and easily. So no huge amounts of wasted space on my site!</p>
<p>In terms of content, it's a mixed bag. Even though I have backups of most of my previous content from my old UXZone.io site, it's not in an easily importable format. So It'll be a slow process of getting the old content extracted from the backup and converted to the new site.</p>
<p>But hey, at least I didn't lose it all!</p>
            ]]>
        </content>
    </entry>
</feed>
