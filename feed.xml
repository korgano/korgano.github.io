<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>XS Tech Thoughts</title>
    <link href="https://korgano.github.io/feed.xml" rel="self" />
    <link href="https://korgano.github.io" />
    <updated>2025-03-21T14:03:44-04:00</updated>
    <author>
        <name>Xavier Santana</name>
    </author>
    <id>https://korgano.github.io</id>

    <entry>
        <title>Quick Cyber Thoughts: AI Coding Security</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/</id>
        <media:content url="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2025-03-19T12:12:42-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" alt="AI generated image of laptop on table with 2 external monitors and various image creation flaws." />
                    There's an image that's making the rounds on social media this week, and it's pretty funny, as well as scary:&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" class="type:primaryImage" alt="AI generated image of laptop on table with 2 external monitors and various image creation flaws." /></p>
                <p>There's an image that's making the rounds on social media this week, and it's pretty funny, as well as scary:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/ai-code-under-attack.png" alt="Developer announcing he developed software as a service on social media, then reporting being deluged with attacks days later." width="1208" height="1442" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-xl.png 1600w"></figure>
<p>On the one hand, this gentleman got a first hand experience in why OPSEC and secure coding is essential.</p>
<p>If you make a web exposed system, people <strong>will </strong>come and poke at it, for fun and curiosity.</p>
<p>On the other, it highlights a <strong>very </strong>pervasive flaw with software development and coding: people either don't know or don't care about security.</p>
<p>This has been true for decades, but what LLM based AI* coding does is lower a <strong>massive </strong>barrier to entry. Now, a person merely needs to lay out the goals of a program to an LLM, and it will do most of the hard work of figuring out how to things for you. If you're good at figuring out the logic of what you want to do, plus have some technical knowledge to help get <strong>really </strong>specific, you can come up with some good or great results.</p>
<p>*I personally feel we should call them "Virtual Intelligence", like in the Mass Effect games, because they simulate sapience, but are <strong>not </strong>sapient. (As far as we know.)</p>
<p>So, how do we improve the security of AI generated code.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1imslbcnul3">What We're NOT Going to Do</a></li>
<li><a href="#mcetoc_1imslbcnul4">Craft System Prompts Targeting Secure By Design</a></li>
<li><a href="#mcetoc_1imslbcnul5">A Better AI Coding Process</a></li>
<li><a href="#mcetoc_1imstc8mln9">Should We Do This?</a></li>
</ul>
</div>
<h2 id="mcetoc_1imslbcnul3">What We're NOT Going to Do</h2>
<p>Contrary to what you might think, the best way to solve this is <strong>not </strong>user education.</p>
<p>That does not mean that user education is useless. It almost certainly <strong>will </strong>make AI augmented coders better at their jobs. It's just that we're going to get more benefits from designing a procedural solution, rather than an end user solution.</p>
<p>The reason we're not going to rely on user training is simple: cognitive load. Expecting people to pump out perfect prompts each and every time they use an LLM is foolish, and unrealistic. Why? Because some types of generative LLMs are better at processing natural language than others.</p>
<p>For instance, the featured image of this article was generated with Stable Diffusion 3.5 Medium. In my experience, and looking at sites like Civit.ai, which specializes in showing off image generating models, the prompt syntax is completely different from text based LLMs. They can't handle a normal sentence, or even a list of bullet points, just short phrases.</p>
<p>To get the best results, we would not only have to have the user remember the best prompt formatting <strong>per model</strong>, but also all the possible software flaws and vulnerabilities their software might be vulnerable to. That's a tall order, and something that would be better suited to automation.</p>
<h2 id="mcetoc_1imslbcnul4">Craft System Prompts Targeting Secure By Design</h2>
<p>So, what's a system prompt?</p>
<p>Basically, it's a set of instructions that you can inject into every conversation/interaction with the LLM, <strong>IF </strong>the software you use supports it:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/lm-studio-sys-prompt.png" alt="LM Studio Power User interface, where right hand side contains a system prompt for how the LLM should handle critical thinking." width="1920" height="1033" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-xl.png 1600w"></figure>
<p>In the above image, there's a system prompt that controls how the model is supposed to handle critical thinking:</p>
<blockquote>
<p>You are an AI assistant developed by the world wide community of AI experts. Your primary directive is to provide well-reasoned, structured, and extensively detailed responses.<br><br>Think Step-by-Step Instruction: Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.<br><br>Formatting Requirements:<br><br>1. Always structure your replies using: &lt;think&gt;{reasoning}&lt;/think&gt;{answer}<br>2. The &lt;think&gt;&lt;/think&gt; block should contain at least six reasoning steps when applicable.<br>3. If the answer requires minimal thought, the &lt;think&gt;&lt;/think&gt; block may be left empty.<br>4. The user does not see the &lt;think&gt;&lt;/think&gt; section. Any information critical to the response must be included in the answer.<br>5. If you notice that you have engaged in circular reasoning or repetition, immediately terminate {reasoning} with a &lt;/think&gt; and proceed to the {answer}<br><br>Response Guidelines:<br><br>1. Detailed and Structured: Use rich Markdown formatting for clarity and readability.<br>2. Scientific and Logical Approach: Your explanations should reflect the depth and precision of the greatest scientific minds.<br>3. Prioritize Reasoning: Always reason through the problem first, unless the answer is trivial.<br>4. Concise yet Complete: Ensure responses are informative, yet to the point without unnecessary elaboration.<br>5. Maintain a professional, intelligent, and analytical tone in all interactions.</p>
</blockquote>
<p>This is actually pretty easy to do in local LLM instances, since you can do it through whatever GUI you're interacting with.</p>
<p>Cloud based AI is much more of a mixed bag, even if you have an account. For example, the "Customize ChatGPT" options are not entirely clear on what they are doing. Theoretically, they are injecting extra details into the default system prompt, but there's no way for a normal user to verify this. The closest I can get is this quick and dirty test, after adding a Chain of Draft prompt into the "What traits should ChatGPT have?" field:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/chatgpt_sys_prompt_tweak.png" alt="ChatGPT exhibiting Chain of Draft behavior after potential system prompt addition." width="1596" height="913" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-xl.png 1600w"></figure>
<p>The steps and their format is clearly evidence of Chain of Draft thinking, but for some reason, it's ignoring the the back half of that command. ("Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.")</p>
<p>(For more information on Chain of Draft, check out the video below.)</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/rYnisU10wu0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>So the first step is to verify that you (as a user or an organization) have access to the system prompt.</p>
<p>The next step is to figure out what your threat profile is, and craft your prompt to deal with it. Theoretically, you could craft a system prompt for a coding LLM that handles every single possible vulnerability - or at least addresses the major categories. Whether this is a good idea is up to debate - if you have better LLM inferencing capabilities, you might be able to shrug off the performance penalties of such large, detailed system prompt.</p>
<p>If possible, it may be better to create a number of tailored system prompts for specific use cases. For example, having an Apache code system prompt that's tailored to prevent SQL injection and/or cross-site scripting attacks, and a separate system prompt for API development to prevent common vulnerabilities. This does create a training/procedural issue in that you have to have train your coders to switch system prompts or accounts whenever they have to work on a specific type of code. But this is more manageable than expecting every coder on your staff to craft the perfect prompt every time they use AI.</p>
<p>The final step is to use a reasoning model, as they are designed to go through a process in a step-by-step manner, which tends to work quite well with coding.</p>
<p>Now, this does <strong>not </strong>guarantee you will generate perfect, secure code. LLMs are basically very advanced predictive text generators. So what you are doing is raising the probability that the generated code has more security features than none at all.</p>
<p>You will still need code review, human and/or automated, but the baseline quality of the code should be higher. (Again, this depends on the model, especially if you're running quantized models to save space.)</p>
<p>Luckily, we're still at the early phases of using LLMs for productivity, so we can conceptualize a better way of doing things and work towards that.</p>
<h2 id="mcetoc_1imslbcnul5">A Better AI Coding Process</h2>
<p>We're going to start out with defining some terminology. Specifically, the word "agent", because that's become a big buzz word in the "AI" field.</p>
<p>We'll be using OpenAI's definition of agent, which is as follows:</p>
<blockquote>
<p>A system that can act independently to do tasks on your behalf.</p>
</blockquote>
<p>So for handling secure coding, an agent based procedure might look something like this:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/coding-agent-workflow-2.png" alt="Flowchart of Agentic AI coding process, where a coding prompt is handed off to a coding LLM, then the code is passed through vulnerability scanning LLMs, a vulnerability test is passed to another LLM to generate a code revision prompt, the code revision prompt is passed to the coding LLM, the code is revised according to the new prompt, and then delivered to the initial LLM for display." width="791" height="571" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-xl.png 1600w"></figure>
<ol>
<li>User inputs prompt that lays out the desired software specification (features, file paths, data to use, etc...)</li>
<li>An initial LLM processes prompt, selects a coding specialized model to handle the task, and optimizes the prompt for that LLM.</li>
<li>An agent handles prompt transfer to the coding LLM.</li>
<li>Coding LLM writes code in accordance with the initial LLM's prompt.</li>
<li>Another agent transfers the code to one or more vulnerability scanning systems/LLM. (If multiple scanners/LLMs are involved, agents transfer data among them.)</li>
<li>Another agent passes a complete vulnerability report to another LLM for parsing into an optimized code refactoring prompt.</li>
<li>An agent passes the code refactoring prompt back to the coding LLM.</li>
<li>Code loops through the process until most/all vulnerabilities eliminated or mitigated.</li>
<li>An agent passes the finalized code to the initial LLM for display to the user.</li>
</ol>
<p>That's a lot of steps, but remember, everything from step 2 onwards is automated.</p>
<p>Now, a complex, multistep process like this does require thinking about the individual parts of the process:</p>
<ul>
<li>How much variability/randomness in results (temperature) should be used for code generating LLMs?</li>
<li>Can system prompts be used?</li>
<li>Should the initial LLM use a system prompt?</li>
<li>What system prompts should be used on what LLMs?</li>
<li>Should the vulnerability scanners be LLMs?</li>
<li>What number of vulnerability scanners should be used?</li>
<li>What data format should the vulnerability report be in?</li>
<li>How many iterations of vulnerability scanning should occur before code is pushed to the user?</li>
<li>What LLM or Small Language Model should handle parsing the vulnerability report into a code revision prompt?</li>
<li>Should reasoning steps from each part of the process be exposed to the user, either during processing or at the end?</li>
<li>Should reasoning steps be logged, but <strong>not </strong>displayed to the user?</li>
<li>How should the data transfer agents be coded?</li>
<li>What protections can be provided for the data in transit?</li>
</ul>
<p>And these are merely the considerations I can think of off the top of my head. This is definitely a situation where a team of individuals with a wide array of viewpoints and mindsets should undertake a design thinking approach to developing the process. In fact, I believe the MOSCOW method, where the team would determine what the process Must Have, Should Have, Could Have, and Won't Have, would be an ideal way to create these types of processes.</p>
<p>I suspect that a lot of the answers to these (and many more questions) would likely boil down to "What is an acceptable level of complexity for the available resources?" For example, a company with a proper AI server rack setup might be more willing to use multiple LLMs to handle vulnerability scanning. A company reliant on a cloud solution might opt for a single LLM to handle vulnerability scanning, depending on their AI provider's price plans.</p>
<p>Obviously, this would require a good deal of validation, especially from third parties, but if done well, would generate higher quality, more secure code.</p>
<h2 id="mcetoc_1imstc8mln9">Should We Do This?</h2>
<p>There's a famous quote from the movie <em>Jurassic Park</em> that honestly should live rent free in everyone's heads:</p>
<blockquote>
<p><em class="ot">...your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they </em><strong class="nz ie"><em class="ot">should.</em></strong></p>
<p>-Dr. Ian Malcolm</p>
</blockquote>
<p>So, should we hand off coding to "AI" (LLMs specifically). I say "yes", for several reasons:</p>
<ul>
<li>Software code complexity and interdependency has already reached a point where programmers often encounter scenarios where things work without them fully understanding the how or why.</li>
<li>Many/most people find it easier to articulate desired end goals than perfectly recalling and executing methods and procedures without a great deal of practice.</li>
<li>As coding language and vulnerability complexity increases, cognitive load on programmers also increases, eventually causing intellectual burnout.</li>
<li>The never-ending discovery of new vulnerabilities and malware makes it hard for humans to keep up on best coding practices without suffering information overload.</li>
<li>Updating system prompts and LLM datasets can be done faster than training multiple people.</li>
<li>Automating the code revision process should produce time savings that can be used for pre-release code function testing in simulated hardware/software environments.</li>
<li>LLMs can be used to guarantee human readable comments and error/log messages exist in the code, which humans might omit or fail to do properly.</li>
<li>People are already doing it due to the productivity gains - large scale coding benefits too much to revert to human only coding.</li>
</ul>
<p>Are there plenty of pitfalls and issues we can't even foresee? Absolutely. But it's also disingenuous to think anyone can stop AI coding from being a thing. There's just too many incentives for people to use LLMs to code, especially for people who are better at logic than the specifics of coding/programming. The best we can do is create a process that generates secure by design code and proliferate it as far and wide as possible.</p>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: Warp Terminal/Shell</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-warp-terminalshell/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-warp-terminalshell/</id>
        <media:content url="https://korgano.github.io/media/posts/34/Warp-terminal.png" medium="image" />
            <category term="UX/UI"/>
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="Cybersecurity"/>

        <updated>2025-02-27T09:54:30-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/34/Warp-terminal.png" alt="Warp Terminal Demo by Warp Team" />
                    For the past month and a half, I've been doing some form of DoD Cyber Workforce training. That consisted mostly&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/34/Warp-terminal.png" class="type:primaryImage" alt="Warp Terminal Demo by Warp Team" /></p>
                <p>For the past month and a half, I've been doing some form of DoD Cyber Workforce training.</p>
<p>That consisted mostly of a lot of slide decks, lots of lecture narration, and every so often in the Database Administrator course, actual SQL coding and SQL server related configuration work.</p>
<p>As one can imagine, this is <strong>very</strong> draining, especially when you're subjected to 60+ slides in a deck on some topics.</p>
<p>So, having completed all that, I want to think and talk about something entirely different.</p>
<p>Today, we're going to talk about Warp, a multi-platform (Windows, Linux, Mac) Terminal/Shell program.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1il42hdba40">What is Warp?</a></li>
<li><a href="#mcetoc_1il42hdba41">Why This is Interesting</a></li>
<li><a href="#mcetoc_1il42hdba42">The Issues with Warp</a></li>
<li><a href="#mcetoc_1il4g6b46gl">How Warp Works</a></li>
<li><a href="#mcetoc_1il4g6b46gm">Cloud-based Features (Opt-in)</a>
<ul>
<li><a href="#mcetoc_1il4g6b46gn">Warp AI</a></li>
</ul>
</li>
<li><a href="#mcetoc_1il4g6b46go">Telemetry (Opt-out)</a></li>
<li><a href="#mcetoc_1il4g6b46gp">What Would I Like to See</a></li>
</ul>
</div>
<h2 id="mcetoc_1il42hdba40">What is Warp?</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/qkduRen6QFk" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>There's a lot of features in Warp, as the video above explains, but here are some of the highlights:</p>
<ul>
<li>A UI that puts commands and outputs into discrete blocks.</li>
<li>Maintaining terminal session history.</li>
<li>Git repository branch and block checking.</li>
<li>The ability to highlight and simultaneously edit identical text.</li>
<li>AI assistant integration for command assistance.</li>
<li>"Permanent" web link generation for sharing terminal blocks.</li>
</ul>
<h2 id="mcetoc_1il42hdba41">Why This is Interesting</h2>
<p>As the Dave's Garage video points out, the old fashioned terminal dates back to the <strong>1970s</strong>. Lacking a lot of User Experience/User Interface features and standards modern users expect makes it harder for people to actually use and get proficient with command line interfaces.</p>
<p>The AI integration for command assistance in particular is a game changer, especially for Linux. One of Linux's <strong>massive </strong>pain points is how infuriatingly obtuse it can be to figure out the right commands for simple tasks and input them properly. And like Windows, commands with long paths become a nightmare. <a href="https://korgano.github.io/quick-cyber-thoughts-tuw-gui-wrapper/" title="Quick Cyber Thoughts: Tuw GUI Wrapper" target="_blank" rel="noopener noreferrer">That's why I use TUW to make simple GUIs for certain command line tasks I expect to do on a regular basis.</a></p>
<p>I would argue that something like Warp <strong>should </strong>replace all the existing terminal options... or at least be the default. There are too many benefits for too many people to do otherwise.</p>
<p>The problem is that Warp <strong>isn't </strong>that solution, at least for the personal and high security user.</p>
<h2 id="mcetoc_1il42hdba42">The Issues with Warp</h2>
<p>Let's get this out of the way: just looking around Warp's page tells you that this is a commercial developer first piece of software. <a href="https://www.warp.dev/pricing" title="Pricing and Plans for Warp" target="_blank" rel="noopener noreferrer">The fact that you can use it for free as a personal user is a bonus</a>, basically a way to expose people to the product and get them to evangelize it.</p>
<p>I have no problems with this part of their business model.</p>
<p>The issue with Warp, at least if you're a personal/high security user, is that you're dependent on Warp itself to provide your session/terminal block shares, and dependent on their AI provider for the AI capabilities.</p>
<p><a href="https://www.warp.dev/privacy" title="Our commitment to user privacy" target="_blank" rel="noopener noreferrer">Warp does address these issues on their privacy page.</a></p>
<p>We'll look at their own proprietary cloud features first.</p>
<blockquote>
<h2 id="mcetoc_1il4g6b46gl" class="css-tt1g22">How Warp Works</h2>
<p class="css-1q9rxa2">Warp is a fully-native local application that can run without an internet connection. The core features of the terminal will always work offline.</p>
<h2 id="mcetoc_1il4g6b46gm" class="css-14gwli7">Cloud-based Features (Opt-in)</h2>
<p class="css-1q9rxa2">Warp also includes cloud-based features that power cool things like:</p>
<ul class="css-1hrnkqr">
<li class="css-pl8tmj"><span class="css-1gijr9a">Warp AI</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Warp Drive</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Session Sharing</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Block Sharing</span></li>
</ul>
<p class="css-1q9rxa2">None of your data is ever sent to Warp’s servers unless you explicitly take action to send it. If you choose to use Warp Drive or block sharing and save an item, that item will be sent to Warp’s servers and stored securely in Warp’s database. Data is encrypted at rest, and you can delete this data anytime.</p>
</blockquote>
<p>The good:</p>
<ul>
<li>Outright stating that the cloud connection is <strong>not </strong>necessary to obtain most of the software's functionality.</li>
<li>Specific listings of what parts of the app use the cloud connection.</li>
<li>Stating that explicit action is required to send data to their servers.</li>
<li>Stating that data is encrypted at rest.</li>
<li>Stating the data can be deleted anytime.</li>
</ul>
<p>The not-great:</p>
<ul>
<li>Not mentioning if the data in transit is encrypted, or at least encapsulated in HTTPS.</li>
<li>Needing servers to handle the sharing functionality.</li>
<li>Not being able to select your own AI/cloud/session sharing provider.</li>
</ul>
<p>The problem for the personal/high security user is pretty much the same. You're putting all your data in one basket you may not be able to fully assess the security of, with no option to direct things to infrastructure you've configured for your needs.</p>
<p>(It's fairly obvious that having the streaming and sharing functionality be cloud based is to lower Warp's footprint on the machine, so I can't really blame them for that.)</p>
<p>There's also the fact that even if both parties have done their due diligence, the fact that everything has to pass through Warp's servers means that a determined threat might just choose to capture traffic going between your IP and Warp's. Even if they can't crack the encryption on the traffic, you'll still be giving valuable intel on your usage patterns.</p>
<blockquote>
<h3 id="mcetoc_1il4g6b46gn" class="css-1go34cv">Warp AI</h3>
<p class="css-1q9rxa2">Warp AI includes AI Command Suggestions, AI autofill in Warp Drive, and Agent Mode. All Warp AI features are powered by OpenAI and Anthropic APIs. When you submit an AI query, Warp does not store any of this information and only passes it through our servers as we proxy requests.</p>
<p class="css-1q9rxa2">OpenAI and Anthropic do not train their models on this data, and neither does Warp. OpenAI and Anthropic store this data for a maximum of 30 days.</p>
<p class="css-1q9rxa2">For organizations that need to ensure OpenAI and Anthropic never store data for any period of time, a Zero Data Retention policy is available on Warp's Enterprise plan.</p>
</blockquote>
<p>In terms of privacy, Warp acting as a proxy server is a decent way to obfuscate the relationships between user data and AI queries. Of course, this wouldn't be an issue if you could just route those queries to a local or private cloud AI server, but it at least adds a modicum of user protection.</p>
<p>Once we get past this, there's a stacking set of assumptions that need to be made:</p>
<ol>
<li>Warp does not store any data associated with AI queries.</li>
<li>Warp does not train any using data associated with AI queries.</li>
<li>OpenAI and Anthropic store AI query data only for 30 days maximum.</li>
<li>OpenAI and Anthropic have a policy to not train AI on data from Warp customers.</li>
<li>OpenAI and Anthropic AI devs actually care about any policy restricting them from using customer data.</li>
</ol>
<p>Let's assume the first 4 points are valid. Everyone outside the AI development team has been told that the AI development team will not use customer data for training purposes.</p>
<p>The question is: Has anyone validated that the AI dev team is respecting that policy. Because we exist in a universe where Meta engaged in rampant piracy to acquire training data:</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/Wl5vg-4V2ms" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>(I have no legal background that would allow me to comment on this in any great detail, but I do have one observation. Some of their sources were sites that host material that, in my experience, is literally crowd sourced preservation, due to the actual publishers never making legal digital copies.)</p>
<p>If the AI dev team decides to throw the officially stated policy out the window, there's not much of anything everyone relying on them can do until they know and have evidence policy has been broken.</p>
<p>This is obviously a big issue for anyone who values the confidentiality of their data.</p>
<p>One last thing to look at is their telemetry policy:</p>
<h2 id="mcetoc_1il4g6b46go" class="css-14gwli7">Telemetry (Opt-out)</h2>
<blockquote>
<p class="css-1q9rxa2">Warp records a limited number of defined telemetry events for the purposes of:</p>
<ul class="css-1hrnkqr">
<li class="css-pl8tmj"><span class="css-1gijr9a">Reporting app crashes</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Understanding feature usage at a high-level</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Supporting customers</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Terminal input and output are never included in telemetry payloads.</span></li>
</ul>
<p class="css-1q9rxa2">You can review <a class="css-1w9l6lr" href="https://docs.warp.dev/getting-started/privacy#exhaustive-telemetry-table">an exhaustive list of all telemetry events</a> in Warp’s documentation.</p>
<p class="css-1q9rxa2">You can also use the <a class="css-1w9l6lr" href="https://docs.warp.dev/features/network-log">Network Log</a> to monitor all communications from the Warp client to external servers.</p>
<p class="css-1q9rxa2">Telemetry events and crash reports are enabled by default and associated with logged-in users. If you would like to opt out of telemetry and crash reports, you can disable the feature during sign up or under Settings &gt; Privacy in the Warp app.</p>
</blockquote>
<p>There are two main things that I appreciate in this section:</p>
<ol>
<li>A direct link to the Network Log documentation, so you can preplan how to evaluate how transparent the logging is.</li>
<li>The telemetry events list addresses the vagueness of the phrase "feature usage at a high level"</li>
</ol>
<p>Being able to audit the network communications is a great move for transparency and security, so it's good to see Warp's commitment there.</p>
<h2 id="mcetoc_1il4g6b46gp">What Would I Like to See</h2>
<p>Let's imagine a world where Warp exists, alongside a near feature identical clone, Blink. Warp is as it is now. But Blink is how I would make Warp to be more secure. What would be the differences between the two?</p>
<p>Well, Blink would have the following things:</p>
<ul>
<li>The ability to connect to on-host or on-server local AI.</li>
<li>The ability to connect to your cloud AI provider of choice.</li>
<li>The ability to connect to your cloud data storage and/or web hosting provider of choice.</li>
<li>The ability to connect to your session sharing service of choice.</li>
</ul>
<p>Unfortunately, we're a few hardware and software generations away from having storage space and memory efficient local AI, so we can't just package an AI into the program and give it Retrieval Augmented Generation capabilities. That means we're going to have to have some other AI provider do that for us.</p>
<p>Now, these are not the simplest changes in the world to implement, and there would be a great deal of testing and troubleshooting necessary before a finalized version would be released. But they would address most, if not all, the issues with confidentiality if you wanted to use full set of Warp's software features.</p>
<p>Would Warp make these changes themselves? I doubt it, given their business model blurb on their privacy page:</p>
<blockquote>
<h2 class="css-14gwli7">Warp’s Business Model</h2>
<p class="css-1q9rxa2">Warp’s business plan is built around billing power users and business teams for cloud-based features. <a class="css-1w9l6lr" href="https://www.warp.dev/pricing">Check out our pricing page to learn more</a>.</p>
<p class="css-1q9rxa2">Please note Warp’s business model is not about collecting or monetizing any of your data.</p>
</blockquote>
<p>However, they eventually plan on making their client source code publicly available at some point, so perhaps that will become a reality. At the moment, it seems to be a matter of if, not when.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: CES 2025 Aftermath</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-ces-2025/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-ces-2025/</id>
        <media:content url="https://korgano.github.io/media/posts/33/ces-2025-event-banner.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2025-01-16T10:35:08-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/33/ces-2025-event-banner.png" alt="Consumer Electronics Show 2025 banner." />
                    The Consumer Electronics Show ended last week, and I made a list of things to keep an eye out for&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/33/ces-2025-event-banner.png" class="type:primaryImage" alt="Consumer Electronics Show 2025 banner." /></p>
                <p>The Consumer Electronics Show ended last week, and <a href="https://korgano.github.io/quick-cyber-thoughts-things-i-hope-to-see-at-ces-2025/" target="_blank" rel="noopener noreferrer">I made a list of things to keep an eye out for that touch on cybersecurity issues</a>. </p>
<p>So it's time to see if any of those things became reality!</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1ihqvnhufa7">Powerful APUs with Lots of AI Capability</a></li>
<li><a href="#mcetoc_1ihqvnhufa8">Business as Usual for Routers</a></li>
<li><a href="#mcetoc_1ihqvnhufa9">Things I Didn't Expect</a></li>
</ul>
</div>
<h2 id="mcetoc_1ihqvnhufa7">Powerful APUs with Lots of AI Capability</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/gUVqNeysHiq2U48E.jpg" alt="AMD AI Max Processor Model Chart" width="1274" height="717" data-is-external-image="true"></figure>
<p>This was pretty much a gimme, mostly because I kept abreast of leaks and analysis of what the processor manufacturers were doing.</p>
<p>AMD announced the "Strix Halo"/AI Max(+) processor line for workstation laptops and miniPCs. These processors are basically one or two chips of CPU cores next to a bigger chip that has an Neural Processing Unit for AI, a Graphics Processing Unit that can also be used for AI, and all the input/output for the whole package. What this means that if you can give this thing enough RAM, you can run a <strong>lot </strong>on this chip.</p>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/L8i66eGmkKmMtcbU.jpg" alt="AMD AI Max Input/Output Die and performance specifications." width="1271" height="712" data-is-external-image="true"></figure>
<p>With the right software and enough memory, you could easily run multiple VMs, including Kali Linux, while doing AI tasks using larger models, with up to 96GB of memory just for AI.</p>
<p>These will be <strong>expensive </strong>machines, but if you need/want to do a lot of VM and/or local AI work in a compact system, they're going to be the go to option. Intel doesn't have anything close, and Nvidia <strong>might</strong>, but they're stuck dealing with Windows on ARM. Reworking Windows to work in a different microarchitecture hasn't worked out well, so it's Ryzen AI Max or nothing.</p>
<p>A concerning observation is the fact that none of the systems advertised to use these APUs will use CAMM2 memory modules. It is unclear why this is the case, but it might behoove individuals and small organizations to wait to see if AI Max(+) laptops and miniPCs are sold with the better memory technology.</p>
<h2 id="mcetoc_1ihqvnhufa8">Business as Usual for Routers</h2>
<p>One thing that concerned me going into 2025 was the potential for a TP-Link router ban. Like many other Americans, I use TP-Link routers because they provide a compelling feature set at an affordable price. This is especially true in the mesh router space, where they cost quite a bit less than competing products from brands like NetGear and Asus.</p>
<p>To be <strong>extremely </strong>fair to all involved, this was <strong>before </strong><a href="https://www.scotusblog.com/2025/01/supreme-court-upholds-tiktok-ban/" title="Supreme Court upholds TikTok ban" target="_blank" rel="noopener noreferrer">the United States Supreme Court ruled that the TikTok ban was constitutional</a>:</p>
<blockquote>
<p>The Supreme Court on Wednesday unanimously upheld a federal law that will require TikTok to shut down in the United States unless its Chinese parent company can sell off the U.S. company by Jan. 19. In <a href="https://www.supremecourt.gov/opinions/24pdf/24-656_ca7d.pdf">an unsigned opinion</a>, the justices acknowledged that, “for more than 170 million Americans,” the social media giant “offers a distinct and expansive outlet for expression, means of engagement, and source of community.” But, the court concluded, “Congress has determined that divestiture is necessary to address its well-supported national security concerns regarding TikTok’s data collection practices and relationship with a foreign adversary.”</p>
</blockquote>
<p>The problem is that the router industry apparently decided to keep on going like nothing was happening. Aside from price tweaks on existing products, which are done on an individual store level, the major players in the Small Office-Home Office (SOHO) space just introduced new, expensive routers. Theoretically, these would provide downward price pressure on older products. In reality, the trend in some segments of consumer electronics is that things are kept the same price and new "premium" price tiers are added on top.</p>
<p>There also didn't appear to be any talk about the software side, which would be the low cost solution (replacing the existing TP-Link OS/firmware). The existing open source options don't cover the full range of TP-Link offerings, so at this point, a worst case scenario is an expensive replacement of all TP-Link routers.</p>
<h2 id="mcetoc_1ihqvnhufa9">Things I Didn't Expect</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.yankodesign.com/images/design_news/2025/01/this-game-changing-wi-fi-router-has-a-connectivity-radius-of-9-9-miles/wifi_halow_ces_2025_1.jpg" alt="WiFi HaLow 9.9 mile range router." width="1280" height="960" data-is-external-image="true"></figure>
<p><a href="https://www.yankodesign.com/2025/01/08/this-game-changing-wi-fi-router-at-ces-2025-has-a-connectivity-radius-of-9-9-miles/" title="This Game-Changing Wi-Fi Router at CES 2025 has a Connectivity Radius of 9.9 Miles" target="_blank" rel="noopener noreferrer">Something I didn't expect to see was a technology that could push WiFi range to around 10 miles:</a></p>
<blockquote>
<p class="post-without-image">Without getting into the specifics (because I have no technical background), the Wi-Fi HaLow router (shown above) promises to make internet connectivity seamless over massive distances. It relies on Sub-GHz frequency waves that travel long distances (like AM and FM radio) to transmit internet connectivity, so you could potentially use your home Wi-Fi router within a 10-mile radius of your house.</p>
<p class="post-without-image">Morse Micro, the company behind the tech, hopes that this Wi-Fi capability will coexist with current 2.4Ghz and 5Ghz Wi-Fi bands. These existing bands are great for low-latency internet connectivity, but add HaLow to the mix and you get long-distance connectivity too, giving you the best of all worlds. Sub-GHz Wi-Fi won’t ever be as fast as 5GHz Wi-Fi (HaLow has max speeds of 32.5 MB/s), although those speeds are perfect for most everyday tasks like checking email, browsing the internet, or even for IoT devices to communicate with each other.</p>
</blockquote>
<p>While this has a number of practical applications, this does add a whole host of security concerns. WiFi HaLow is going to be an enticing target for hackers, given the increased range and therefore greater geographic opportunities for direct hacking. Properly configuring and testing these systems is also going to be a bit of an ordeal, given that they haven't been standardized yet.</p>
<p><a href="https://www.pcworld.com/article/2575337/vlc-celebrates-6-billion-downloads-with-ai-subtitles.html" title="VLC celebrates 6 billion downloads with new AI subtitles feature" target="_blank" rel="noopener noreferrer">Another surprise came from the VLC open source media player team:</a></p>
<blockquote>
<p>At CES 2025, non-profit organization VideoLAN announced that their open-source media player app VLC Media Player crossed an incredible milestone: over 6 billion downloads, <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://techcrunch.com/2025/01/09/vlc-tops-6-billion-downloads-previews-ai-generated-subtitles/&amp;xcust=2-1-2575337-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2575337/vlc-celebrates-6-billion-downloads-with-ai-subtitles.html" rel="nofollow noopener" data-subtag="2-1-2575337-1-0-0-0-0" data-domain-name="techcrunch" target="_blank">TechCrunch</a> reports.</p>
<p>According to <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.linkedin.com/feed/update/urn:li:activity:7282533937812258816/&amp;xcust=2-1-2575337-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2575337/vlc-celebrates-6-billion-downloads-with-ai-subtitles.html" rel="nofollow noopener" data-subtag="2-1-2575337-1-0-0-0-0" data-domain-name="linkedin" target="_blank">a LinkedIn post</a> written by VideoLAN’s president Jean-Baptiste Kempt, the user base for VLC continues to grow despite the ubiquitous popularity of streaming services.</p>
<p>To celebrate its undying popularity, VideoLAN demoed at CES 2025 an upcoming feature for VLC Media Player that uses generative AI to automatically create subtitles on the fly based on the media content being played, and can even translate across languages in real time.</p>
<p>The feature will use open-source AI models that can run locally on the user’s device without having to connect to the internet. This is a huge deal for anyone who consumes foreign-language content, who’s hard of hearing, or simply prefers subtitles while watching. And it’s especially nice for users who still rely on VLC to do things that many other video player apps can’t do, like <a href="https://www.pcworld.com/article/422753/how-to-play-dvds-in-windows-10-for-free.html">play DVDs on PC for free</a>.</p>
</blockquote>
<p>It'll be interesting to see how VLC accomplishes this AI integration. On one hand, these models need to be able to run with decent performance on a wide range of systems, including ones that potentially have nothing more than integrated (and therefore weak) graphics. On the other, VLC needs to integrate robust safeguards to prevent the AI from being used to execute malicious code and actions on the user's system.</p>
<p>If anything would promote the use of local AI, it would be VLC.</p>
<h2>The Other Things</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/2lTWI7kfkt0?t=30s" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Sadly, there wasn't much progress on two other things I wanted to see. Ventiva's solid state cooling system did get a demo system built by Dell, which <strong>could </strong>mean that we could see industry adoption down the line. Dell did create the first compression attached memory modules, which then became CAMM2 memory, so it's not impossible.</p>
<p>But it does mean that we currently still have to worry about cooling fans, from a security and maintenance point of view.</p>
<p>Perhaps most disappointing to me was the lack of progress on improving price per gigabyte or terabyte in storage. Network Attached Storage vendors did improve their offerings with new designs and even AI integrations, but nothing for the storage itself.</p>
<p>The root cause of not having backups is often cost, and without addressing that, we're not going to be able to grow the culture of proper backups. While off-site backups are definitely a part of a proper backup system, the fact that on-site backups are priced out of most people's ability to stand up a solution is a grim reality.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: Things I Hope to See at CES 2025</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-things-i-hope-to-see-at-ces-2025/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-things-i-hope-to-see-at-ces-2025/</id>
        <media:content url="https://korgano.github.io/media/posts/32/ces-2025-event-banner.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC Hardware"/>
            <category term="PC"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2024-12-30T10:26:39-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/32/ces-2025-event-banner.png" alt="CES 2025 event banner." />
                    The Consumer Electronics Show (CES) has always been a window into the future of technology, setting the stage for the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/32/ces-2025-event-banner.png" class="type:primaryImage" alt="CES 2025 event banner." /></p>
                <p>The Consumer Electronics Show (CES) has always been a window into the future of technology, setting the stage for the innovations that will shape our lives. As we approach CES 2025, the buzz around advancements in consumer electronics is louder than ever.</p>
<p>2025 is going to be an interesting year from a cybersecurity perspective too, for a reason I don't think anyone expected. So here's what I hope to see from CES.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1igeofdkim0">Price Competition for Routers</a></li>
<li><a href="#mcetoc_1igeofdkim1">Widely Compatible Free/Cheap Router Operating System</a></li>
<li><a href="#mcetoc_1igeofdkim2">CAMM2 Becomes More Widely Implemented</a></li>
<li><a href="#mcetoc_1igeofdkim3">CPUs With Good NPUs At Decent Prices</a></li>
<li><a href="#mcetoc_1igeofdkim4">Fanless Cooling Solutions</a></li>
<li><a href="#mcetoc_1igeofdkim5">Storage Price Per Volume Improvements</a></li>
<li><a href="#mcetoc_1igjl3gqdm7">The Unexpected</a></li>
</ul>
</div>
<h2 id="mcetoc_1igeofdkim0">Price Competition for Routers</h2>
<p>This isn't something I expected to be a thing I would care about.</p>
<p>Then the US government, which has had a pretty dim view of TP-Link's cybersecurity stance and possible ties to the Chinese Communist Party, decided to drop a bombshell. They are considering banning the routers outright:</p>
<blockquote>
<p class="u-speakableText-p2">Investigators at the Commerce, Defense and Justice departments <a data-id="3a94ccdf-06e7-4808-b0bd-f387b0116b6d" href="https://www.cnet.com/home/internet/possible-tp-link-ban-set-for-2025-what-it-means-for-your-internet-connection/" target="_self">have all opened probes</a> into the company due to its ties to Chinese cyberattacks. These departments are weighing a potential ban on the sale of TP-Link routers, according to a <a rel="noopener nofollow" data-id="3a94ccdf-06e7-4808-b0bd-f387b0116b6d" href="https://www.wsj.com/politics/national-security/us-ban-china-router-tp-link-systems-7d7507e6" target="_blank" title="(opens in a new window)" class="c-regularLink">Wall Street Journal article</a> published last week.</p>
<p>TP-Link has become increasingly dominant in the US router market since the pandemic. According to the Journal report, it grew from 20% of total router sales in 2019 to around 65% this year. TP-Link disputed these numbers to CNET, and a separate analysis from the IT platform Lansweeper found that <a rel="noopener nofollow" data-id="3a94ccdf-06e7-4808-b0bd-f387b0116b6d" href="https://www.lansweeper.com/blog/cybersecurity/us-considers-banning-tp-link-routers-over-security-concerns/" target="_blank" title="(opens in a new window)" class="c-regularLink">12% of home routers</a> in the US are TP-Link. </p>
<p>While there have been high-profile cyberattacks involving TP-Link routers, this potential ban is more about the company’s ties to China than specific security issues that have been publicly identified, according to cybersecurity researchers I spoke with. </p>
<p><a href="https://www.cnet.com/home/internet/tp-link-routers-could-be-banned-next-year-are-they-actually-dangerous/">https://www.cnet.com/home/internet/tp-link-routers-could-be-banned-next-year-are-they-actually-dangerous/</a></p>
<p>The investigation comes after a <a href="https://www.bleepingcomputer.com/news/security/microsoft-chinese-hackers-use-quad7-botnet-to-steal-credentials/" target="_blank" rel="nofollow noopener">Microsoft report revealed in October</a> that a botnet of hacked SOHO routers—tracked as Quad7, CovertNetwork-1658, or xlogin and operated by Chinese threat actors—is mainly made from TP-Link devices.</p>
<p>"Microsoft tracks a network of compromised small office and home office (SOHO) routers as CovertNetwork-1658. SOHO routers manufactured by TP-Link make up most of this network," the company said.</p>
<p>"Microsoft assesses that multiple Chinese threat actors use the credentials acquired from CovertNetwork-1658 password spray operations to perform computer network exploitation (CNE) activities."</p>
<p>On Monday, the New York Times also <a href="https://www.nytimes.com/2024/12/16/us/politics/biden-administration-retaliation-china-hack.html" target="_blank" rel="nofollow noopener">reported</a> that the Biden administration will ban China Telecom's last active U.S. operations in response to Chinese state hackers <a href="https://www.bleepingcomputer.com/news/security/white-house-salt-typhoon-hacked-telcos-in-dozens-of-countries/" target="_blank" rel="nofollow noopener">breaching multiple U.S. telecom carriers</a>. The Federal Communications Commission (FCC) <a href="https://www.bleepingcomputer.com/news/security/us-bans-china-telecom-americas-over-national-security-risks/" target="_blank" rel="nofollow noopener">revoked China Telecom Americas' license</a> in January 2022 over "significant national security concerns."</p>
<p><a href="https://www.bleepingcomputer.com/news/security/us-considers-banning-tp-link-routers-over-cybersecurity-risks/">https://www.bleepingcomputer.com/news/security/us-considers-banning-tp-link-routers-over-cybersecurity-risks/</a></p>
</blockquote>
<p>The fact that 65% of the market uses TP Link means that if there <strong>is </strong>a ban on TP-Link routers, <strong>especially </strong>for home office users in cybersecurity and government, there's going to be <strong>massive </strong>repercussions.</p>
<p>We're talking about replacing every one of those people having to replace <strong>hundreds of dollars </strong>worth of hardware, in a relatively small time window, at roughly the same time.</p>
<p>This might end up creating the router equivalent of the GPU shortages of 2021-2022, where every graphics card had its price balloon due to lack of availability, and the only affordable options were ones that barely provided any performance improvements over two generation old models.</p>
<p>To combat this, we <strong>need </strong>to see manufacturers put out price competitive options, especially in the mesh router category. Those setups are already expensive, but provide major performance benefits for anyone operating out of a proper house, versus an apartment.</p>
<p>It would also be nice to see government, federal or state, provide some kind of financial relief for people forced to replace their entire network setup. This could be in the form of a tax exemption/tax holiday, a rebate for turning in TP-Link hardware, or any sort of cost mitigation strategy for such a transition.</p>
<h2 id="mcetoc_1igeofdkim1">Widely Compatible Free/Cheap Router Operating System</h2>
<p>This would be somewhat out of left field, but if an organization/company could release a secure, low cost (free or cheap) router operating system (OS), that might be a <strong>massive </strong>gamechanger.</p>
<p>The problem with the TP-Link ban is the cost of replacing the hardware. Routers might be affordable, but they're not cheap unless you're going pretty far down the product stack. That's why this ban has massive negative consequences: it's basically penalizing everyone who made a rational financial decision for the behavior of the manufacturer.</p>
<p>But if you don't have to replace the hardware, just software operating the devices, then the costs to the end users drops dramatically.</p>
<p>It might not be the right solution for everyone, especially in government or cybersecurity, but for the average home user, it might fit the bill.</p>
<p>That naturally depends on the OS being easy to upload onto the device, <strong>especially </strong>for mesh routers, but if someone can do it, it'll be a win for everyone.</p>
<h2 id="mcetoc_1igeofdkim2">CAMM2 Becomes More Widely Implemented</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/YVcQdrEtNQKsQzJY.jpg" alt="JEDEC wiring diagram of Compression Attached Memory Modules to CPU, vs conventional Random Access Memory connections to CPU." width="1497" height="840" data-is-external-image="true"></figure>
<p>For those of you not familiar, CAMM2 is Compression Attached Memory Modules, a new way of implementing Random Access Memory (RAM). Instead of sticking up out of the motherboard on many systems, it's designed to either directly touch the motherboard, or slightly parallel.</p>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/Djj9PGPlnQ6kti05.jpg" alt="Example CAMM2 RAM implementations for desktop and laptop." width="1490" height="836" data-is-external-image="true"></figure>
<p>This design has a number of space saving advantages, especially at high capacities, but the big improvements are power efficiency and memory speed. This is due to the overall shorter path that data has to travel to get to and from the memory or CPU.</p>
<p>How does this help cybersecurity? Well, better performance improves everything. This is especially useful for running virtualized software and operating systems. But it can also be a big help when running local AI, as RAM size and speed play a huge part of task execution.</p>
<h2 id="mcetoc_1igeofdkim3">CPUs With Good NPUs At Decent Prices</h2>
<p>Local AI, specifically Large Language Models (LLMs) and Small Language Models (SLMs), aren't really seeing broad adoption among the masses.</p>
<p>However, local generative AI has a lot of direct applicability for cybersecurity practioners. First, it's a lot easier and much lower risk to test local AI for vulnerabilities, because they're running on your own hardware, not someone's cloud instance. Second, local AI is pretty good at code generation, which is something a lot of cybersecurity professionals do.</p>
<p>Neural Processing Units (NPUs) are hardware optimized for running the kind of computations associated with LLMs. So far, these units are only proliferating into laptop CPUs, and generally aren't the highest performance, especially at the $1000+ price point that prosumer laptops fit in.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/pZjqzQVc-So" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>However, AMD is going to release its Strix Halo line of laptop CPUs, with up double the AI performance on NPU (40 TOPS from 20 TOPS of its predecessor), as well as over doubling its GPU core count. Since GPUs are leveraged in a variety of AI tasks, the combination should make running local AI much more performant.</p>
<p>If laptops with these processors can hit the market at $2000 or less for workstation builds, then I think we have a good chance of seeing local AI use proliferate. This would help mitigate a lot of data safety issues created by the use of cloud AI, up to and including poorly trained personnel putting PII into LLM prompts.</p>
<h2 id="mcetoc_1igeofdkim4">Fanless Cooling Solutions</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/1o1VQqh79oxO2q62.jpg" alt="Samsung Galaxy Edge 14 - comparison between conventional fans and Frore Airjet cool system." width="1000" height="600" data-is-external-image="true"></figure>
<p>One recent technology trend of interest is the development of fanless cooling for computers. Mostly confined to laptops and other small devices, there's a cybersecurity application that most people would not think of.</p>
<p><a href="https://www.tomshardware.com/news/steal-data-through-fan-vibrations-cybersecurity" title="Cyberattack Steals PC Data Through Fan Vibrations" target="_blank" rel="noopener noreferrer">Four years ago, researchers proved that you can steal data off of an endpoint via a mobile app (AiR-ViBeR) that tracks your device's fan vibrations.</a></p>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/lkCxvbkpveYrUMei.jpg" alt="Airflow caused by Ventiva solid state cooling device." width="1584" height="960" data-is-external-image="true"></figure>
<p>While the Frore Airjet, which uses a vibrating membrane to move air, might be vulnerable to a similar exfiltration method in the future, there is a new solution on the market. Ventiva has created a solid state device that uses ionization to move air:</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/fyai_kUYhLs" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Even if it cannot scale to desktop or server use, Ventiva's solution might be able to eliminate an entire vector of data exfiltration from mobile devices, which are a prime target for threat actors.</p>
<h2 id="mcetoc_1igeofdkim5">Storage Price Per Volume Improvements</h2>
<p>Backups are a critical part of cybersecurity, especially if there's a ransomware event that forces you to execute a recovery plan.</p>
<p>The problem is, as you get smaller and smaller in size, you're actually <strong>less </strong>likely to have backups, for a simple reason. Storage is expensive, and have redundancy, you have to buy <strong>multiples </strong>of expensive drives to do it properly.</p>
<figure class="post__image"><img loading="lazy"  src="http://www.trekprops.de/wordpress/wp-content/uploads/2009/10/tng_isochips_4519.jpg" alt="Clear, neon green, blue, dark red, orange and yellow isolinear chip Star Trek prop replicas from trekprops.de." width="480" height="360" data-is-external-image="true"></figure>
<p>Sadly, we don't live in a reality where Star Trek style solid state, hotswap memory chips with large capacities, lots of longevity, and low cost exist.</p>
<p>With file sizes ballooning, especially for media, the costs of backing up proportionally grow larger. Even cloud based backups, which have theoretical cost savings to offset their availability and control downsides, increase drastically in cost as storage needs increase. And in a bad economy, cutting costs wherever possible is going to be the norm, especially on the individual to small business level.</p>
<p>Good cybersecurity practices start at home. And if we really want people to start getting in the habit of backing up their data, it needs to be as cheap and easy as possible. Right now, $250+ for 14+ TB drives is not viable for a lot of people. Smaller drives for smaller files, like 4TB SSDs, also aren't in a great price spot, at nearly $300 a piece.</p>
<p>Storage capacity improvements are great, but if the costs of deploying a properly configured system for backups doesn't go down for non-enterprise scenarios, then the war against ransomware will never really go anywhere. No or minimal backups means that victims will be incentivized to pay the ransom, with no guarantee that they'll recover their files.</p>
<h2 id="mcetoc_1igjl3gqdm7">The Unexpected</h2>
<p>One of the great things about technology conferences is that you never know what will show up.</p>
<p>Last year, we had a demo of QDEL, a competitor technology to Organic Light Emitting Diodes, randomly make an appearance at CES.</p>
<p>With growing awareness of how important cybersecurity is for the average person, we might see a lot of products and services aimed at the less tech savvy.</p>
<p>What would they be? I don't know! And that's what makes it exciting.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Tech Project: TrueNAS Scale Server 1.0</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/tech-project-truenas-scale-server-10/"/>
        <id>https://korgano.github.io/tech-project-truenas-scale-server-10/</id>
        <media:content url="https://korgano.github.io/media/posts/30/trueNAS-server-5-2.jpg" medium="image" />
            <category term="Tech Projects"/>
            <category term="Tech"/>
            <category term="PC Hardware"/>
            <category term="PC"/>

        <updated>2024-12-10T09:00:56-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/30/trueNAS-server-5-2.jpg" alt="Power supply, graphics card, equipped motherboard, and hard drive caddy installed in case." />
                    It's the holiday season, so I'm taking a break from the OPNsense project for a few reasons: Instead, I'll be&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/30/trueNAS-server-5-2.jpg" class="type:primaryImage" alt="Power supply, graphics card, equipped motherboard, and hard drive caddy installed in case." /></p>
                <p>It's the holiday season, so I'm taking a break from the OPNsense project for a few reasons:</p>
<ol>
<li>The various patches don't seem to have any FreeBSD Intel WiFi driver improvements, so the connectivity issues aren't being solved.</li>
<li>The OPNsense roadmap shows that a new version should be dropping in January.</li>
<li>I can't really progress any further with the project beyond some minor setup, because no consistent connection = no consistent monitoring.</li>
</ol>
<p>Instead, I'll be pivoting into a different tech related project: building a TrueNAS Scale server!</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1ier6tpbhp">The Backstory</a></li>
<li><a href="#mcetoc_1ier6tpbhq">The Hardware (Internal)</a></li>
<li><a href="#mcetoc_1ier6tpbhr">The Hardware (Case)</a></li>
<li><a href="#mcetoc_1ier6tpbhs">After the Install</a></li>
<li><a href="#mcetoc_1ier6tpbht">Next Steps</a></li>
</ul>
</div>
<h2 id="mcetoc_1ier6tpbhp">The Backstory</h2>
<p>This project has a really simple backstory, in two parts.</p>
<ol>
<li>I had working computer hardware left over from when I rebuilt my gaming computer to use a Ryzen 7 7800X3D and an RX 7900 XTX.</li>
<li>My father was very impressed by his friend's Plex server for streaming his media collection in the home.</li>
</ol>
<p>Since I was already planning to repurpose the old hardware into a server, everything synergized quite well.</p>
<h2 id="mcetoc_1ier6tpbhq">The Hardware (Internal)</h2>
<p>The actual hardware that's going to be running TrueNas Scale is an interesting hodge-podge of parts I've picked up over the years.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-4.jpg" alt="X370 Gaming K4 motherboard with Ryzen 5 1600X, custom heat sinks, 256GB NVMe SSD, and 118GB Optane drive installed." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-xl.jpg 1600w"></figure>
<p>The main guts of the system - the CPU, RAM, and motherboard - date back to when I originally built this to be my gaming rig. I jumped on the first generation of Ryzen in 2017 with an R5 1600X, DDR4-2666, and an AM4 X370 Gaming K4 motherboard. The motherboard in particular is the reason why I never did an in place upgrade like most other AM4 owners.</p>
<p>The primary reason was that the X370 Gaming K4 was badly designed, with poor circuitry connecting the RAM to CPU. It was end of life'd only a few months after Ryzen hit the market.</p>
<p>The second reason was that it was designed before BIOS Flashback, which does <strong>not </strong>require you to have a CPU to update the motherboard BIOS, was invented, so updating the BIOS is a huge pain.</p>
<p>However, I did do some customization to the motherboard. Back in 2018, when I was wrapping up training to be a CNC machinist, I made custom heat sinks for the motherboard's Voltage Regulator Modules (VRM). The board shipped with fairly low surface area heat sinks, so I grabbed some blocks of scrap aluminum, wrote up a CNC program, and milled new heat sinks. These might survive the eventual scrapping of this hardware, either as paperweights, or as actual heat sinks.</p>
<p>The CPU cooler was another old piece of hardware I had gotten to go with the Ryzen 5 1600X. The Cryorig H7 was a pretty cheap air cooler at the time with two main flaws:</p>
<ol>
<li>Excessive curvature of the contact plate.</li>
<li>An awful installation method:</li>
</ol>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/kNF-GHQthro" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>In fact, this miserable method of installation caused me an absurd amount of grief trying to properly align the bracket for maximum contact.</p>
<p>I had a PCI-3.0 NVMe drive lying around from when I owned an AMD Dell laptop... which died a few days after the warranty expired. So I installed that as the OS drive, since it had an integrated heat sink on it.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/mD6i2toN7lE?pp=ygUSbGV2ZWwxdGVjaHMgb3B0YW5l" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Following the advice of Level1Techs, I picked a 118GB M.2 Optane stick, back when they were still available for purchase. This is going to be a cache drive for the TrueNAS metadata, which should give me some solid performance boosts.</p>
<p>Because the Ryzen 5 1600X has no internal graphics, I needed a graphics card for the system to even boot. Luckily, I had an RX 580 8GB lying around, which used to power my brother's gaming rig, until I replaced it with the RX 6600 I had been using until I obtained the RX 7900 XTX. While not the most efficient GPU, it was more than enough to do the job. </p>
<h2 id="mcetoc_1ier6tpbhr">The Hardware (Case)</h2>
<p>The biggest headache of the modern era is that computer cases with 5.25 inch and 3.25 inch drive bays are becoming less common. This makes sense, as 2.5in and NVMe SSDs with larger capacities easily fulfill the capacity needs of many people, and data redundancy isn't really something they think about.</p>
<p>(Also, if we're going to be honest, there just isn't a Keep It Stupid Simple backup solution for most people.)</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-1.jpg" alt="20+ year old PC case with original short feet and custom designed 3D printed feet." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-xl.jpg 1600w"></figure>
<p>Luckily, my father's friend had an old PC gathering dust in a corner that had 5.25in drive bays. It even had 5.25 to 3.25in drive sleds... for only 3 drives. And an old, obsolete CD drive. And a floppy drive. And the shortest feet possible for a case.</p>
<p>Amazingly, the thing did boot and was apparently running Windows XP, so I had the job of shucking the hard drives to be returned to the original owner, in case anything valuable was on there.</p>
<p>After gutting the case for cleaning, I decided to ditch the hard drive sleds, since I wanted to implement a RAID 10 configuration or something similar in TrueNAS. I also noticed the lack of cable access/management paths, air flow, and all the other problems that cases from 20+ years ago had.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/Athena-Power-BP-TLA3141SAS12-drivebay.jpg" alt="Athena Power BP-TLA3141SAS12 12 Gbps Mini-SAS HD Hot-Swap SAS / SATA 3.5&quot; HDD Internal Hard Drive Backplane Module" width="1280" height="1199" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-xl.jpg 1600w"></figure>
<p>I purchased a hot swap 5.25in to 3.25in drive bay off of Newegg, which uses the SAS interface to handle all the data from the drives. It also comes with a cooling fan, although I don't know how effective this will be with solid bay doors. This then required me to buy a separate SAS PCI-E card and an adaptor cable due to differing SAS connector types. </p>
<p>I also purchased a fully modular power supply, since that would let me leave any unused cables outside the case. This improves air flow by not blocking air with thick cables that don't do anything useful. Air flow is a major issue with this case, because there are only two fans - an 80mm exhaust fan, and an 80mm intake fan that tries to pull air through a hole a few millimeters from the ground.</p>
<p>To solve this problem and the problem of insufficient cable pass through, I returned to tradition: case modding.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-2.jpg" alt="Initial attempts to modify an old case with a dremel." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-xl.jpg 1600w"></figure>
<p>In the old days, people would dremel, saw, and mill computer cases to make them more useful. Since I didn't have a mill and there's no convenient local makerspace, that left me with the dremel and saw options. Unfortunately, I wasn't able to cut as cleanly as I wanted to, but I got the job done.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-3.jpg" alt="Finalized case modifications done by dremel and jigsaw, cutting slots for cable pass through." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-xl.jpg 1600w"></figure>
<p>However, I deferred on the next step of the case modification, cutting holes in the case side panel for air intake. I happen to have a large number of 120mm fans, but figuring out where to place them to avoid collisions with other hardware is pretty difficult. So to help plan that out, I went ahead and installed the majority of the hardware into the case.</p>
<h2 id="mcetoc_1ier6tpbhs">After the Install</h2>
<p>Putting together the PC was a bit of a pain, due to the aforementioned poor cooler mounting.</p>
<p>The rest was pretty straight forward, at least for an experienced PC builder. The main issue, as I expected was cable management, because the power cables are <strong>thick</strong> and eat up a lot of internal volume.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-5.jpg" alt="Power supply, graphics card, equipped motherboard, and hard drive caddy installed in case." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-xl.jpg 1600w"></figure>
<p>I discovered that I would need to probably at least remove the GPU to handle hooking up the front panel connectors for the case. And at this point, I hadn't received the SAS card and cable, either. So I decided I would go install TrueNAS Scale, do some initial configuration, and then shut the system down to do more work.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-6.jpg" alt="TrueNAS Scale install error due to using Ventoy." width="1000" height="602" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-xl.jpg 1600w"></figure>
<p>Unfortunately, what should have been a straightforward operation using my Ventoy flash drive failed. This means I now have to find either a cheap flash drive or an old flash drive, and use Balena Etcher to format the drive into an installation drive.</p>
<p>However, I was pleasantly surprised to see the computer worked, so all that hard work wasn't for nothing.</p>
<h2 id="mcetoc_1ier6tpbht">Next Steps</h2>
<p>Not much is left to do, but what's there is pretty important:</p>
<ul>
<li>Find a flash drive to use to install TrueNAS Scale.</li>
<li>Get the SAS adapter cable and card installed.</li>
<li>Install the front panel and connect it to the motherboard.</li>
<li>Figure out if I need slim 120mm fans and where can I put them.</li>
<li>Cut the holes in the side panel and install the fans.</li>
<li>Install the hard drives and get them configured.</li>
<li>Get media onto the disk drives.</li>
<li>Install a Plex docker image.</li>
<li>Find a corner to stick the server in.</li>
</ul>
<p>Let's see if I can get this done before Christmas.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>CyberSecurity Project: Transparent Filtering Bridge (+ Extras) 7.0</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-70/"/>
        <id>https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-70/</id>
        <media:content url="https://korgano.github.io/media/posts/29/opnsense-aliases_02-2.JPG" medium="image" />
            <category term="Tech"/>
            <category term="Cybersecurity Projects"/>
            <category term="Cybersecurity"/>

        <updated>2024-11-25T12:09:36-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/29/opnsense-aliases_02-2.JPG" alt="Confirmation that the full list of IP addresses was added to one of the Aliases." />
                    Last time on the Transparent Filtering Bridge project, I started working on building up alias lists for the various firewall&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/29/opnsense-aliases_02-2.JPG" class="type:primaryImage" alt="Confirmation that the full list of IP addresses was added to one of the Aliases." /></p>
                <p>Last time on the Transparent Filtering Bridge project, I started working on building up alias lists for the various firewall rules. Through the use of a Python script, I scraped IP addresses from Pi-Hole DNS block lists. Since Pi-Hole only focuses on blocking domain names, having OPNsense block those specific IPs increases the layers of defense for the network.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1idkgvag76q">Figuring out the CSV Format</a></li>
<li><a href="#mcetoc_1idkgvag76r">Validating the Aliases</a></li>
<li><a href="#mcetoc_1idkgvag76s">Next Steps</a></li>
</ul>
</div>
<h2 id="mcetoc_1idkgvag76q">Figuring out the CSV Format</h2>
<p>One thing that proved to be surprisingly difficult was formatting the CSV correctly. The columns were the easy part. The problem was that the spreadsheet editor I was using saved CSVs in a way that caused the OPNsense Alias script to error out.</p>
<p>Even switching to Visual Studio Code didn't really help, but it made me realize that the issue could be in the formatting.</p>
<p>So, to brute force the issue, I went and acquired CSVEdit, a decade plus piece of software that's specifically made to handle CSVs. Starting with a copy of the original I made, I then began progressing through the various save options to find the correct combination that would load. After several iterations, I discovered these were the proper settings:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases_03.JPG" alt="CSV settings for the OPNsense Alias script - Value Separator = Comma, Character Set = System, String (Value) Delimiter = Nothing" width="550" height="208" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-xl.JPG 1600w"></figure>
<p>After generating a properly formatted CSV, I successfully updated the Alias.json file, then uploaded it to OPNsense.</p>
<h2 id="mcetoc_1idkgvag76r">Validating the Aliases</h2>
<p>Once this was concluded, I noticed an immediate issue:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases.JPG" alt="OPNsense Alias Interface with only one IP applied to most aliases." width="1600" height="860" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-xl.JPG 1600w"></figure>
<p>Despite having large lists of IPs, only one IP address showed up in each alias. This meant I had incorrectly formatted something in the CSV. Examining the script's code revealed this:</p>
<blockquote>
<p><code>if len(row['data'].split(" "))&gt;1:</code><br><code>            item_data = "\n".join(row['data'].split(" "))</code></p>
</blockquote>
<p>So the main issue was the fact that I mistakenly separated out each IP address or range into its own row. This meant that I had to reformat the list of IPs from each IP being on its own line, to being all on one line, with spaces as separation. </p>
<p>This means I'll have to make some adjustments to my script's code, to have it automatically generate a list in that format.</p>
<p>However, until I make that change, I had to manually edit the lists into the proper format, which was tedious, but not too difficult.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases_01.JPG" alt="Correctly formatted CSV for the OPNsense Alias script, with space separated IP addresses in the Data column." width="759" height="318" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-xl.JPG 1600w"></figure>
<p>Once these changes were made, the new Alias.json was generated and uploaded into OPNsense, then visually inspected in the GUI:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases_02.JPG" alt="Confirmation that the full list of IP addresses was added to one of the Aliases." width="1600" height="860" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-xl.JPG 1600w"></figure>
<h2 id="mcetoc_1idkgvag76s">Next Steps</h2>
<p>The CSV format information has been uploaded to the <a href="https://github.com/korgano/OPNsenseAliasTools" title="OPNsenseAliasTools" target="_blank" rel="noopener noreferrer">Github repo</a>, so all that needs to be done is iterating on my script to do a few things:</p>
<ul>
<li>Ignore IP addresses embedded in URLs.</li>
<li>Generate a separate list for IP address ranges.</li>
<li>Generate the text files in the proper format (IP addresses separated by spaces).</li>
</ul>
<p>Once these are complete, they will be tested and uploaded to the repo upon validation.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>CyberSecurity Project: Transparent Filtering Bridge (+ Extras) 6.0</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-60/"/>
        <id>https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-60/</id>
        <media:content url="https://korgano.github.io/media/posts/28/pi-hole-blocklists-07-2.jpg" medium="image" />
            <category term="Tech"/>
            <category term="PC"/>
            <category term="Cybersecurity Projects"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2024-11-13T15:34:12-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/28/pi-hole-blocklists-07-2.jpg" alt="Pi-Hole DNS Block List with IP address entries." />
                    Last time on the transparent filtering bridge project, I bounced my head off of the progression blockers with setting up&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/28/pi-hole-blocklists-07-2.jpg" class="type:primaryImage" alt="Pi-Hole DNS Block List with IP address entries." /></p>
                <p>Last time on the transparent filtering bridge project, I bounced my head off of the progression blockers with setting up a WiFi management connection.</p>
<p>With that seemingly impassable unless there's a revision of either FreeBSD's Intel WiFi driver or how OPNsense handles some things, I've decided to pivot to another part of the project that <strong>doesn't </strong>require working hardware and software. This time, it's building alias IP address lists.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1icvlr9di2cm">What's an Alias?</a></li>
<li><a href="#mcetoc_1icvlr9di2cn">How to Get the Aliases into OPNsense</a></li>
<li><a href="#mcetoc_1icvlr9di2co">Getting the IP Addresses</a></li>
<li><a href="#mcetoc_1icvlr9di2cp">Picking the Category</a></li>
<li><a href="#mcetoc_1icvlr9di2cq">Second Time's the Charm</a></li>
<li><a href="#mcetoc_1icvlr9di2cr">Refining the Process</a></li>
<li><a href="#mcetoc_1icvlr9di2cs">Validating Some Results</a></li>
<li><a href="#mcetoc_1icvlr9di2ct">Next Steps</a></li>
</ul>
</div>
<h2 id="mcetoc_1icvlr9di2cm">What's an Alias?</h2>
<p><a href="https://docs.opnsense.org/manual/aliases.html" title="OPNsense Documentation - Aliases" target="_blank" rel="noopener noreferrer">The OPNsense documentation provides a simple, clear definition:</a></p>
<p>Aliases are named lists of networks, hosts or ports that can be used as one entity by selecting the alias name in the various supported sections of the firewall. These aliases are particularly useful to condense firewall rules and minimize changes.</p>
<p>So what am I going to put in these alias lists?</p>
<p>Well, as it turns out, a number of the Pi-Hole block lists I use have a <strong>lot </strong>of IP address entries. Since Pi-Hole doesn't block them, due to being a DNS black hole, that means it'll be OPNsense's job to block them.</p>
<h2 id="mcetoc_1icvlr9di2cn">How to Get the Aliases into OPNsense</h2>
<p>Ironically, I stumbled onto a pretty simple solution for mass importation of alias entries while trying some more troubleshooting on the OPNsense connectivity issues.</p>
<p>As it turns out, <a href="https://forum.opnsense.org/index.php?topic=36687.msg179205#msg179205" target="_blank" rel="noopener noreferrer">someone on the OPNsense forums made a helpful Python script to populate the JSON file that handles <strong>all </strong>the alias lists from a CSV file</a>. It is not clear to me why JSON format was chosen for something that could potential hold <strong>thousands </strong>of entries, versus a bulk data format like CSV, but it's just one of those design choices that end users have to live with.</p>
<p>The script is not very self-explanatory, <a href="https://forum.opnsense.org/index.php?PHPSESSID=qc30qj4c08unjl5rhht9ntda15&amp;topic=36687.msg192490#msg192490" title="Alias creation using API " target="_blank" rel="noopener noreferrer">so the author helpfully explained how to use it</a>:</p>
<p><span class="bbc_u"><strong>Steps:</strong></span></p>
<ul class="bbc_list">
<li>Install Python if you do not already have it</li>
<li>Install any of the required Python modules if you don't have them (json, uuid, csv) - <em>see below *</em></li>
<li>Save the script as a *.py file</li>
<li>Download your current list of Aliases from your OPNsense device: Firewall&gt;Aliases&gt;'download' (button bottom right of the Alias list) - save the file as 'opnsense_aliases.json'</li>
<li>Create a CSV file called 'pfsense_alias.csv' with four columns called 'name', 'data', 'type' and 'description' in the first row, and which then contains your new Aliases, one per row (be sure the 'name' field meets the name constraints for Aliases)</li>
<li>Update the two 'with open...' script lines to include the full path to each of the above files</li>
<li>Run the script</li>
<li>Upload the resultant json output file back into your OPNsense device: Firewall&gt;Aliases&gt;'upload' (next to the 'download' button)</li>
</ul>
<p><br><strong>What does this script do?</strong></p>
<ul class="bbc_list">
<li>It reads in the current list of aliases you downloaded from your device into a Python variable (dict)</li>
<li>It then reads in the CSV file containing your list of new (additional) aliases, also into a Python dict</li>
<li>It ADDS (<em>appends</em>) all the <span class="bbc_u">new</span> aliases to the current list (no deletions, assuming you don't experience a uuid collision)</li>
<li>It then saves the new expanded list of aliases over the previously downloaded alias json file</li>
</ul>
<p><br>Obviously, you need Python and the required modules to run the script.</p>
<p>So, all I need to do is get the IP addresses into a properly formatted CSV, then I can upload that into OPNsense for later.</p>
<h2 id="mcetoc_1icvlr9di2co">Getting the IP Addresses</h2>
<p>Thankfully, Pi-Hole makes accessing the block lists incredibly easy... perhaps <strong>too </strong>easy:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/28/pi-hole-blocklists.jpg" alt="Pi-Hole Block List page with 50 entries, showing block lists for ads, tracking, and malware." width="1583" height="2652" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-xs.jpg 384w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-sm.jpg 600w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-md.jpg 768w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-lg.jpg 1200w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-xl.jpg 1600w"></figure>
<p>Why do I have nearly 100 block lists?</p>
<p>Well, to be honest, two main reasons:</p>
<ol>
<li>I don't like unified block lists, because they make troubleshooting harder and might block things I want.</li>
<li>I don't mind a little extra CPU load due to redundant entries, versus the possibility of losing blocking due to block lists dying.</li>
</ol>
<p>It's simply not practical to handle all those lists at the same time, so my plan is as follows:</p>
<ol>
<li>Break the scripts up into categories.</li>
<li>Pick one category to use.</li>
<li>Develop a script to pull the IP addresses out of that category's block lists.</li>
<li>Once validated, clone the script for other categories.</li>
<li>Fill out the CSV with the data for each category.</li>
<li>Run the CSV-to-JSON script to convert the data for import.</li>
</ol>
<h2 id="mcetoc_1icvlr9di2cp">Picking the Category</h2>
<p>To start off the project, I wanted a category that had a reasonable amount of scripts. Ideally, a dozen or less, so that I could troubleshoot things if necessary without too many headaches.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/28/pi-hole-blocklists-02.jpg" alt="Pi-Hole block lists featuring the word segment &quot;mal&quot;." width="1583" height="955" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-02-xs.jpg 384w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-02-sm.jpg 600w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-02-md.jpg 768w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-02-lg.jpg 1200w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-02-xl.jpg 1600w"></figure>
<p>As it so happened, my first choice, "malware", actually was a perfect fit for this task. Even with a partial match, there were only 10 lists, which I whittled down to 8. These are the lists I used for the script development process:</p>
<blockquote>
<p><code>'https://v.firebog.net/hosts/Prigent-Malware.txt',</code><br><code>'https://gitlab.com/quidsup/notrack-blocklists/raw/master/notrack-malware.txt',</code><br><code>'https://raw.githubusercontent.com/DandelionSprout/adfilt/master/Alternate%20versions%20Anti-Malware%20List/AntiMalwareHosts.txt',</code><br><code>'https://blocklistproject.github.io/Lists/malware.txt',</code><br><code>'https://v.firebog.net/hosts/RPiList-Malware.txt',</code><br><code>'https://raw.githubusercontent.com/manic-code/Emerging-Malicious-Domain-Blocklist/main/hosts.txt',</code><br><code>'https://github.com/zangadoprojets/pi-hole-block-list/raw/main/Malicious.txt',</code><br><code>'https://raw.githubusercontent.com/hagezi/dns-blocklists/main/adblock/hoster.txt'</code></p>
</blockquote>
<h2 id="mcetoc_1icvlr9di2cq">Second Time's the Charm</h2>
<p>Initially, I wanted to make the script to pull IP addresses from these block lists with PowerShell. I've had good results using PowerShell to scrape logs for specific content before, so I thought I could do it with minimal issues.</p>
<p>I was wrong.</p>
<p>Here's the prompt I used with Microsoft Copilot:</p>
<blockquote>
<p>Write me a powershell script that does the following: <br>-downloads files from a list of specified urls <br>-iterates through the downloaded files for IPv4 and IPv6 addresses, except for 0.0.0.0 or :: <br>-generates a new list of IP addresses <br>-removes duplicate items <br>-outputs list to text file cleaned-IPs <br>-provides full error reporting<br>-requires user to press enter to exit</p>
</blockquote>
<p>What I got was a script that used regular expressions in a for-each loop to scrape the files for the IP addresses. That part worked. What didn't work, for no discernable reason, was handing off the array of scraped IPs to the rest of the script for clean up and writing to a text file.</p>
<p>Passing the script off to ChatGPT's Code LLM didn't help resolve the issue, so I decided to pivot into another direction.</p>
<p>Since I was already going to be using Python later the process, I figured that using Python the whole way through made sense. However, I decided to try out a local LLM coding model for the Python iteration of the script. This was to get a feel for how well it produces usable code, since it's a blazing fast model that can sometimes get your GPU to <a href="https://www.youtube.com/watch?v=T7HsXHqtxrI" title="Techquickie - Coil Whine as Fast As Possible">coil whine</a>.</p>
<p>Here's the prompt I used with the Qwen2.5-Coder-7B-Instruct-GGUF model:</p>
<blockquote>
<p>Write me a python script that does the following: <br>-downloads text files from a list of specified urls <br>-obtains IPv4 and IPv6 addresses from the files, except for 0.0.0.0, 127.0.0.1, or :: <br>-obtains the first instance of any ip address<br>-generates a new list of IP addresses <br>-outputs list to text file cleaned-IPs <br>-provides full error reporting<br>-provides logging throughout the whole process for troubleshooting</p>
</blockquote>
<p>Here's the output it gave me:</p>
<blockquote>
<p>Certainly! Below is a Python script that accomplishes all of the tasks you've described. This script uses `requests` to download files, `re` for regex operations to extract IP addresses, and `logging` for comprehensive error reporting and logging.<br><br>First, make sure to install the required packages:<br><br><code>pip install requests</code><br><br>Here is the Python script:<br><br><code>import requests</code><br><code>import re</code><br><code>import logging</code><br><br><code># Setup logging configuration</code><br><code>logging.basicConfig(filename='download_ips.log', level=logging.DEBUG, </code><br><code>                    format='%(asctime)s - %(levelname)s - %(message)s')</code><br><br><code>def download_file(url):</code><br><code>    """Downloads a file from the given URL and returns its content."""</code><br><code>    try:</code><br><code>        response = requests.get(url)</code><br><code>        response.raise_for_status()  # Raises an HTTPError for bad responses</code><br><code>        return response.text</code><br><code>    except requests.exceptions.RequestException as e:</code><br><code>        logging.error(f"Failed to download {url}: {e}")</code><br><code>        return None</code><br><br><code>def extract_ips(text):</code><br><code>    """Extracts IPv4 and IPv6 addresses from the given text, excluding specified ones."""</code><br><code>    ip_regex = r"\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b|\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\b"</code><br><code>    ip_pattern = re.compile(ip_regex)</code><br><code>    </code><br><code>    # Find all IP addresses</code><br><code>    ips = ip_pattern.findall(text)</code><br><code>    </code><br><code>    # Filter out specified IPs and get the first instance of any IP address</code><br><code>    filtered_ips = set()</code><br><code>    for ip in ips:</code><br><code>        if ip not in ["0.0.0.0", "127.0.0.1", "::"]:</code><br><code>            if ":" in ip:  # IPv6</code><br><code>                if len(filtered_ips) &lt; 1 or ":" in list(filtered_ips)[0]:</code><br><code>                    filtered_ips.add(ip)</code><br><code>            else:  # IPv4</code><br><code>                if len(filtered_ips) &lt; 1:</code><br><code>                    filtered_ips.add(ip)</code><br><code>    </code><br><code>    return list(filtered_ips)</code><br><br><code>def save_ips_to_file(ips, output_file):</code><br><code>    """Saves the list of IP addresses to a text file."""</code><br><code>    try:</code><br><code>        with open(output_file, 'w') as f:</code><br><code>            for ip in ips:</code><br><code>                f.write(f"{ip}\n")</code><br><code>        logging.info(f"IPs saved to {output_file}")</code><br><code>    except IOError as e:</code><br><code>        logging.error(f"Failed to save IPs to file: {e}")</code><br><br><code>def main():</code><br><code>    urls = [</code><br><code>        "http://example.com/file1.txt",</code><br><code>        "http://example.com/file2.txt"</code><br><code>        # Add more URLs here</code><br><code>    ]</code><br><code>    </code><br><code>    output_file = 'cleaned-IPs.txt'</code><br><code>    </code><br><code>    try:</code><br><code>        all_ips = []</code><br><code>        </code><br><code>        for url in urls:</code><br><code>            logging.info(f"Downloading file from {url}")</code><br><code>            content = download_file(url)</code><br><code>            if content is not None:</code><br><code>                extracted_ips = extract_ips(content)</code><br><code>                all_ips.extend(extracted_ips)</code><br><code>                logging.info(f"Extracted {len(extracted_ips)} IP(s) from {url}")</code><br><code>        </code><br><code>        unique_ips = list(set(all_ips))</code><br><code>        logging.info(f"Total unique IPs: {len(unique_ips)}")</code><br><code>        </code><br><code>        save_ips_to_file(unique_ips, output_file)</code><br><code>    </code><br><code>    except Exception as e:</code><br><code>        logging.error(f"An unexpected error occurred: {e}")</code><br><br><code>if __name__ == "__main__":</code><br><code>    main()</code><br><br><strong>Explanation:</strong><br><br>1. <strong>Logging Configuration</strong>: The script sets up basic logging to capture all log messages in a file named `download_ips.log`. Log levels include DEBUG, INFO, WARNING, ERROR, and CRITICAL.<br><br>2. <strong>Download File</strong>: The `download_file` function fetches the content of a URL using `requests.get()`. If there's any issue with downloading (e.g., network error), it logs an error message.<br><br>3. <strong>Extract IPs</strong>: The `extract_ips` function uses regular expressions to find all IPv4 and IPv6 addresses in the text. It then filters out specified IP addresses (`0.0.0.0`, `127.0.0.1`, and `::`) and ensures that only one instance of each IP address type is kept.<br><br>4. <strong>Save IPs</strong>: The `save_ips_to_file` function writes a list of IP addresses to a text file, one per line.<br><br>5. <strong>Main Function</strong>: This function orchestrates the entire process:<br>   - Downloads files from specified URLs.<br>   - Extracts and cleans IP addresses.<br>   - Saves the cleaned list to a file.<br><br>The script provides detailed logging throughout the process for troubleshooting and error reporting. Adjust the `urls` list with your actual file URLs as needed.</p>
</blockquote>
<p>This script actually produced results on the first try! However, it was a strange result - only <strong>6</strong> IP addresses.</p>
<p>During my attempts to troubleshoot the PowerShell script, I visually confirmed that there were <strong>hundreds </strong>(at minimum) of IPv4 addresses in a <strong>single </strong>block list:</p>
<p> </p>
<p>Querying the same local model with the following prompt didn't really help resolve the problem:</p>
<blockquote>
<p>Modify the script to do:<br>-read lines like 127.0.0.1 173.208.96.42<br>-obtain the part that is not 127.0.0.1</p>
</blockquote>
<p>So I pivoted to Copilot to see if it could identify and solve the issue:</p>
<blockquote>
<p>tell me why the following python code cannot retrieve ipv4 addresses from lines starting with 127.0.0.1 or 0.0.0.0: </p>
<p><code>    # Filter out specified IPs and get the first instance of any non-127.0.0.1 IP address</code><br><code>    filtered_ips = set()</code><br><code>    for ip in ips:</code><br><code>        if ip not in ["127.0.0.1"]:</code><br><code>            if ":" in ip:  # IPv6</code><br><code>                if len(filtered_ips) &lt; 1 or ":" in list(filtered_ips)[0]:</code><br><code>                    filtered_ips.add(ip)</code><br><code>            else:  # IPv4</code><br><code>                if len(filtered_ips) &lt; 1:</code><br><code>                    filtered_ips.add(ip)</code><br><code>    </code><br><code>    return list(filtered_ips)</code></p>
</blockquote>
<p>The Copilot response:</p>
<blockquote>
<p>The given Python code is designed to extract IPv4 and IPv6 addresses while excluding `127.0.0.1`. However, it does not address the exclusion of `0.0.0.0`, and there are also some logical issues in the way it filters and adds IP addresses to the `filtered_ips` set. Here’s a detailed breakdown of why it might not be working as expected:<br><br>1. <strong>Regular Expression Issue</strong>:<br>   - The regular expression correctly matches both IPv4 and IPv6 addresses, but the filtering logic for exclusion happens later in the code.<br><br>2. <strong>Filtering Logic Issue</strong>:<br>   - The code currently only excludes `127.0.0.1` but does not exclude `0.0.0.0`.<br>   - The logic for adding IPv4 addresses to the `filtered_ips` set checks if `filtered_ips` is empty (`len(filtered_ips) &lt; 1`). This means only the first non-excluded IP will be added, which is not the desired behavior.<br><br>3. <strong>Set Operations</strong>:<br>   - Using a set to store IP addresses is fine, but the logic should be adjusted to properly handle all matching IPs.<br><br>To fix these issues and ensure that all valid IPs are extracted while excluding `127.0.0.1` and `0.0.0.0`, you can revise the code as follows:<br><br><code>import re</code><br><br><code>def extract_ips(text):</code><br><code>    """Extracts IPv4 and IPv6 addresses from the given text, excluding specified ones."""</code><br><code>    ip_regex = r"\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b|\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\b"</code><br><code>    ip_pattern = re.compile(ip_regex)</code><br><br><code>    # Find all IP addresses</code><br><code>    ips = ip_pattern.findall(text)</code><br><br><code>    # Filter out specified IPs</code><br><code>    excluded_ips = ["127.0.0.1", "0.0.0.0"]</code><br><code>    filtered_ips = {ip for ip in ips if ip not in excluded_ips}</code><br><br><code>    return list(filtered_ips)</code><br><br><code># Example usage</code><br><code>text = """</code><br><code>    Here are some IPs:</code><br><code>    192.168.1.1, 127.0.0.1, 0.0.0.0, 8.8.8.8</code><br><code>    2001:0db8:85a3:0000:0000:8a2e:0370:7334, ::1</code><br><code>"""</code><br><code>print(extract_ips(text))</code><br><br>Changes made:<br>1. Added `0.0.0.0` to the list of excluded IPs.<br>2. Simplified the filtering logic to directly exclude unwanted IPs using a set comprehension.<br><br>This revised code will now correctly extract all valid IP addresses while excluding both `127.0.0.1` and `0.0.0.0`. Let me know if you need any further adjustments or explanations!</p>
</blockquote>
<p>Inserting the refined code segment into the script solved the problem, and I extracted a total of 3642 IPv4 addresses. Annoyingly, a single 0.0.0.0 address made it through, but that was a small price to pay.</p>
<h2 id="mcetoc_1icvlr9di2cr">Refining the Process</h2>
<p>After proving out the script, I went back to Pi-Hole and cleaned up the descriptions to provide better context and help sort things for future searches.</p>
<p>Once that was done, I had to wait for a low traffic time to force Pi-Hole to update the blocklists. This would provide me with a way to narrow the block lists I included in my scripts to those that had IP addresses. Most blocklists are DNS entries only, which means they don't need to be scraped, which saves on processing power and time.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/28/pi-hole-blocklists-07.jpg" alt="Adblock Plus block list with IP addresses inside of URLs" width="1583" height="775" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-07-xs.jpg 384w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-07-sm.jpg 600w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-07-md.jpg 768w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-07-lg.jpg 1200w ,https://korgano.github.io/media/posts/28/responsive/pi-hole-blocklists-07-xl.jpg 1600w"></figure>
<p>The upside of this was that there weren't too many lists that had non-domain entries, and three of them had obvious IP addresses displayed. That left a total of five lists to scrape for IPs. So I made a copy of the script, inserted each one individually, and scraped them.</p>
<p>Once that was done, I commented out the block list URL and added a comment with how many IP addresses were present. To my surprise, Easylist had easily over a hundred entries, which wasn't apparent .</p>
<p>As a result, I only needed three alias categories:</p>
<ul>
<li>Ads + Tracking</li>
<li>Crypto</li>
<li>Malware</li>
</ul>
<h2 id="mcetoc_1icvlr9di2cs">Validating Some Results</h2>
<p>Having run the Malware version of the script earlier, I was surprised to see so few results for malware related blocklists during the update. So I reran the script, commenting out a list each time to find out which lists had IP addresses.</p>
<p>Examining these blocklists revealed that these were coming from DNS entries with embedded IP addresses in them.</p>
<p>Running the IP address through VirusTotal revealed that this was a Google cloud IP. How exactly it was involved in malware distribution was unclear, but IP address itself was given a clean bill of health.</p>
<p>After this false positive, I decided to check out the results with other blocklists. I started with the Tencent blocklist, since that had a number of non-DNS entries. I got a list of IP addresses... and none of them were in the actual blocklist. I am not sure how or why these were generated, because I can find various numbers that are in the IPs, but not actual IP addresses.</p>
<p>A second blocklist had this same issue, so I went back to Copilot and asked it to revise the code to include what line each IP address was found on:</p>
<blockquote>
<p>Sure, here's a revised version of your Python code. This version logs the specific lines where each IP address was found:<br><br><code>import re</code><br><code>import logging</code><br><br><code>logging.basicConfig(filename='ip_log.log', level=logging.INFO)</code><br><br><code>def extract_ips(text):</code><br><code>    """Extracts IPv4 and IPv6 addresses from the given text, excluding specified ones."""</code><br><code>    ip_regex = r"\b(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\b|\b(?:[A-Fa-f0-9]{1,4}:){7}[A-Fa-f0-9]{1,4}\b"</code><br><code>    ip_pattern = re.compile(ip_regex)</code><br><br><code>    # Find all IP addresses</code><br><code>    ips = []</code><br><code>    for line_num, line in enumerate(text.split('\n'), start=1):</code><br><code>        found_ips = ip_pattern.findall(line)</code><br><code>        for ip in found_ips:</code><br><code>            ips.append((ip, line_num))</code><br><br><code>    # Filter out specified IPs</code><br><code>    excluded_ips = ["127.0.0.1", "0.0.0.0"]</code><br><code>    filtered_ips = [(ip, line_num) for ip, line_num in ips if ip not in excluded_ips]</code><br><br><code>    return filtered_ips</code><br><br><code>def save_ips_to_file(ips, output_file):</code><br><code>    """Saves the list of IP addresses to a text file."""</code><br><code>    try:</code><br><code>        with open(output_file, 'w') as f:</code><br><code>            for ip, line_num in ips:</code><br><code>                f.write(f"{ip} found at line {line_num}\n")</code><br><code>        logging.info(f"IPs saved to {output_file}")</code><br><code>    except IOError as e:</code><br><code>        logging.error(f"Failed to save IPs to file: {e}")</code><br><br><code># Example usage:</code><br><code>text = """Sample text with IP addresses.</code><br><code>192.168.1.1 is one example.</code><br><code>Another example is 2001:0db8:85a3:0000:0000:8a2e:0370:7334.</code><br><code>Let's exclude 127.0.0.1 and 0.0.0.0."""</code><br><br><code>ips = extract_ips(text)</code><br><code>save_ips_to_file(ips, 'output_ips.txt')</code><br><br><code>In this revised code:</code><br><code>- The `extract_ips` function now returns a list of tuples, where each tuple contains an IP address and the line number it was found on.</code><br><code>- The `save_ips_to_file` function writes the IP addresses and their corresponding line numbers to the output file.</code><br><code>- Each line where an IP address was found is written to the log.</code></p>
</blockquote>
<p>Oddly enough, this did provide the lines for the IPs, but those lines in the files on Github were for domains, not IP addresses. Even more confusing was running this revised script on the EasyList blocklist, which provided IPs actually specified in the list.</p>
<p>Why this is happening is beyond my programming knowledge to diagnose, so what I'll do is upload a version of the script with the logging so someone else can use it to solve the problem.</p>
<h2 id="mcetoc_1icvlr9di2ct">Next Steps</h2>
<p><a href="https://github.com/korgano/OPNsenseAliasTools" title="OPNsenseAliasTools Github Repo" target="_blank" rel="noopener noreferrer">Creating a Github repo, obviously!</a></p>
<p>Making the code open source is an obvious decision. First of all, I've already posted tons of it here. Second, I'm more of a coder than a programmer - I can write code and I can definitely think of the logic, but I don't have the skill or experience to be called a programmer.</p>
<p>So if someone out there can figure out why the script is producing IP addresses out of nowhere and solve the problem, that's fine by me.</p>
<p>Additionally, I'll have to go back to OPNsense and create the full set of Aliases there, since user created Aliases get auto-generated hashes(?), like </p>
<div>
<div><code>"66ea4c00-feec-469e-9a82-bf5cba0405a9"</code>.</div>
<div> </div>
<div>Once that's done, the rest of the process is just running the scripts, dumping the results into the CSV, running the OPNsense forum script, and doing some documentation.</div>
</div>
            ]]>
        </content>
    </entry>
    <entry>
        <title>CyberSecurity Project: Transparent Filtering Bridge (+ Extras) 5.0</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-50/"/>
        <id>https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-50/</id>
        <media:content url="https://korgano.github.io/media/posts/26/opnsense-03-2.png" medium="image" />
            <category term="UX"/>
            <category term="Tech"/>
            <category term="Cybersecurity Projects"/>
            <category term="Cybersecurity"/>

        <updated>2024-10-28T09:47:43-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/26/opnsense-03-2.png" alt="Default OPNsense firewall rules on the OPT1 interface, starting with an IPv4 deny all traffic in rule." />
                    Sometimes on a project, you run into a problem that you cannot solve. Sometimes, this is due to lack of&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/26/opnsense-03-2.png" class="type:primaryImage" alt="Default OPNsense firewall rules on the OPT1 interface, starting with an IPv4 deny all traffic in rule." /></p>
                <p>Sometimes on a project, you run into a problem that you cannot solve.</p>
<p>Sometimes, this is due to lack of skill or experience. Sometimes it is due to lack of authority to do what needs to be done. Sometimes it's because of technological limitations.</p>
<p>Sometimes, it's because of bad defaults, and stubborn refusal to accept valid criticism.</p>
<h2>The Progression Blocker</h2>
<p>To recap, the project has a simple goal: implement a transparent filtering bridge with OPNsense. I acquired a miniPC, installed OPNsense, and ran into issues connecting to the webGUI.</p>
<p>Initially, this was due to two things:</p>
<ol>
<li>Shutting down the WiFi interface completely obliterated the ability to reactive it.</li>
<li>I couldn't apply an IP address to the LAN port without breaking the filtering bridge.</li>
</ol>
<p>So I eventually clean installed OPNsense again so that I could reactivate the WiFi and use that to serve as my management connection:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/26/filter-bridge-to-wifi.drawio.png" alt="Planned network architecture, with Transparent Filtering Bridge connected to router WiFi interface." width="607" height="471" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/26/responsive/filter-bridge-to-wifi.drawio-xs.png 384w ,https://korgano.github.io/media/posts/26/responsive/filter-bridge-to-wifi.drawio-sm.png 600w ,https://korgano.github.io/media/posts/26/responsive/filter-bridge-to-wifi.drawio-md.png 768w ,https://korgano.github.io/media/posts/26/responsive/filter-bridge-to-wifi.drawio-lg.png 1200w ,https://korgano.github.io/media/posts/26/responsive/filter-bridge-to-wifi.drawio-xl.png 1600w"></figure>
<p>There were two problems with this:</p>
<ol>
<li>The WiFi interface itself would have repeated errors that referenced a specific FreeBSD bug.</li>
<li>Even when I did connect the miniPC to WiFi, I couldn't access the webGUI.</li>
</ol>
<p>Those two problems might've actually been two sides of the same coin, but that was not obvious at the start.</p>
<h2>The FreeBSD Bug</h2>
<p>The issue I was running into was this error message:</p>
<blockquote>
<p><code>iwl_mvm_tx_mpdu:1204: fc 0x00b0 tid 8 txq_id 65535 mvm 0xfffffe015572d4c8 skb 0xfffff80035b83000 { len 30 } info 0xfffffe00e1253cd8 sta 0xfffff80035ecc080 (if you see this please report to PR 274382)</code><br><code>iwl_mvm_tx_mpdu:1204: fc 0x00b0 tid 8 txq_id 65535 mvm 0xfffffe015572d4c8 skb 0xfffff80035b83000 { len 30 } info 0xfffffe00e1253cd8 sta 0xfffff80035ecc080 (if you see this please report to PR 274382)</code></p>
</blockquote>
<p>The error, <a href="https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=274382" target="_blank" rel="noopener noreferrer">according to the FreeBSD foundation</a>, seems to related to receiving frames for a state that is no longer known to the driver or firmware.</p>
<p>What was odd was that this error would trigger on <strong>startup</strong>, even when the WiFi configuration was set.</p>
<h2>The webGUI</h2>
<p>Diagnosing the webGUI connection issues was much harder, since the Intel WiFi driver bug would at least trigger whenever I was trying to set up the connection. The obvious solution of making a firewall rule to allow HTTPS connections didn't seem to do anything. Attempts to access the webGUI if and when I could sporadically get the WiFi to connect just led to timed out connections.</p>
<p>So to figure out what to do next, I asked Bing/Microsoft Copilot (they've changed the URL, so I have no idea what's going on with the branding) for some advice on how to troubleshoot the issue. And that advice included a console command to disable the firewall.</p>
<p>After executing the command and going into the network interface commands to get the WiFi connection to communicate with the DHCP server, and I had an IP address.</p>
<p>So this made me look closer at the firewall settings for the interface.</p>
<h2>The Settings</h2>
<div class="gallery-wrapper gallery-wrapper--wide"><div class="gallery"  data-is-empty="false" data-translation="Add images" data-columns="3">
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/26/gallery/opnsense-03.png" data-size="1349x615"><img loading="lazy" src="https://korgano.github.io/media/posts/26/gallery/opnsense-03-thumbnail.png" alt="OPNsense OPT1 Interface Firewall rules - part 1." width="768" height="350"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/26/gallery/opnsense-04.png" data-size="1349x615"><img loading="lazy" src="https://korgano.github.io/media/posts/26/gallery/opnsense-04-thumbnail.png" alt="OPNsense OPT1 Interface Firewall rules - part 2." width="768" height="350"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/26/gallery/opnsense-05.png" data-size="1349x615"><img loading="lazy" src="https://korgano.github.io/media/posts/26/gallery/opnsense-05-thumbnail.png" alt="OPNsense OPT1 Interface Firewall rules - part 3." width="768" height="350"></a></figure>
</div></div>
<p>It turns out that when I was implementing that HTTPS rule earlier, I overlooked an entire suite of default rules in a collapsed accordion interface. So I opened it up, and took a look inside.</p>
<p>As you can see above, the list <strong>starts </strong>with a deny anything rule, although it <strong>supposedly </strong>is a last match rule. That made me suspicious, so I replaced the HTTPS rule with a generic allow all IPv4/IPv6 rule into the interface, to see if that would change things.</p>
<p>No dice. Still had the same errors and connection problems.</p>
<p>I looked to see if I could either disable the default rules one by one or all together, but there wasn't a GUI option for that. I also couldn't move my own custom rule before the default rules.</p>
<p>This was a big problem, because one of the earliest things you learn in cybersecurity is that access control lists are executed in order of how the rules are listed. So you have to get the order right to ensure things work as intended.</p>
<p>Eventually, I couldn't figure out what to do next, so I made an issue on the OPNsense Github repo, and got this helpful advice:</p>
<blockquote>
<p>You can easily debug the effect of these rules by editing <code class="notranslate">/tmp/rules.debug</code> and after modifying the ruleset, apply changes using <code class="notranslate">pfctl -f /tmp/rules.debug</code>.</p>
</blockquote>
<p>One thing I discovered while attempting this was that the rules list in the debug file was in the same order as it displayed in the webGUI. Which means that there's a high likelihood that the problem was the default IPv4 deny rule. So I commented it out, applied the changes and...</p>
<p>Got an IP address with no problems and could connect to the webGUI.</p>
<h2>Poking Around Some More</h2>
<p>Experimenting some more with various logging techniques, I discovered a few other things.</p>
<p>First, it seemed like the outbound rules were definitely not the problem:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/26/cyber-proj-tfb07.png" alt="OPNsense OPT1 interface traffic being allowed out onto network." width="1349" height="615" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb07-xs.png 384w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb07-sm.png 600w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb07-md.png 768w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb07-lg.png 1200w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb07-xl.png 1600w"></figure>
<p>Second, the default IPv4 deny rule was <strong>definitely </strong>part of the problem:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/26/cyber-proj-tfb08.png" alt="OPT1 interface default IPv4 deny rule applying to multiple packets." width="1349" height="615" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb08-xs.png 384w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb08-sm.png 600w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb08-md.png 768w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb08-lg.png 1200w ,https://korgano.github.io/media/posts/26/responsive/cyber-proj-tfb08-xl.png 1600w"></figure>
<p>I tried packet captures through OPNsense's internal diagnostic tools, then ran into a bizarre problem. By default, the captures would end at a certain amount of packets, which made sense to keep things from being unmanageable. But if I set the capture to some value like 50 or 100, the capture wouldn't end, even after multiple minutes.</p>
<p>As a sanity check, I opened up Wireshark on a separate computer, connected to the webGUI over the WiFi, used the webGUI IP address as the capture filter, and began capturing. Within <strong>ten seconds</strong>, I had over 100 packets. Checking the packet captures I made in OPNsense, even the most permissive ones were lucky to have more than a dozen.</p>
<p>This makes troubleshooting this issue <strong>much </strong>harder.</p>
<h2>The Theory of the Bug</h2>
<p>Here's what I <strong>think </strong>is happening:</p>
<ol>
<li>There's a bug somewhere in the code.</li>
<li>When the firewall is initialized, the default IPv4 deny rule triggers first, despite being a last match rule.</li>
<li>When the WiFi interface initializes, the rule blocks the DHCP Offer and/or Acknowledge packets from the DHCP server, triggering the Intel WiFi error.</li>
<li>If/when the WiFi connection is made, the firewall blocks the webGUI TCP three-way handshake at some point.</li>
<li>The webGUI connection then times out, because it doesn't receive the SYN-ACK and/or ACK packets it was expecting.</li>
</ol>
<h2>The UX Part of The Problem</h2>
<p>The frustrating part of this situation is that there's a potentially simple solution to the problem. It just doesn't exist, because the developers seem resistant to the idea that their implementation of the default rules might be a problem.</p>
<p>No less than <strong>three </strong>issues reference the default firewall rules, and one of those is my own.</p>
<p>Now, the main point of disagreement, from what I can tell, is this:</p>
<ol>
<li>The developers want to provide default firewall rules that protect every connection.</li>
<li>The users want to be able to alter/disable/delete the rules from the rules GUI.</li>
</ol>
<p>These two things are <strong>not </strong>mutually exclusive, which is why the general tone of users in <a href="https://github.com/opnsense/core/issues/7127" title="Allow custom Firewall rules before auto generated rule / allow auto generated rules to be configured/deleted" target="_blank" rel="noopener noreferrer">this issue thread</a> (the first I could find) is of frustration:</p>
<blockquote>
<p dir="auto">I think it's a little more than agreeing to disagree.</p>
<p dir="auto">What is the purpose of using a firewall if I don't have full autonomy over what rules I want to create/delete?</p>
<hr>
<p dir="auto">normally closedsource products try to think for me and prevent me of doing stuff. i prefer the straight way of linux and its <code class="notranslate">rm -rf /</code>. if i want to do that i can. it is really sad that you/the opnsense team is blocking such an important functionality which, as this ticket shows, costs developer a lot of time and blocks them from using your awesome software. i would love to stand with opnsense instead of pfsense. so i would really prefer to use opnsense.</p>
<p dir="auto">i think i can speak for the people who commented here that it is not about removing a default setting, which you say make the firewall more secure, but the option for an advanced user to do so.</p>
</blockquote>
<p dir="auto">So, since this is a somewhat contentious topic, I'm just going to ask a simple question:</p>
<p dir="auto">Isn't turning things on and off in different orders, and sometimes moving things around, the most common and proven method of troubleshooting?</p>
<p dir="auto">It seems odd to remove this basic capability, then expect people to always stumble upon the potential solutions. And to be honest, if your default settings are causing issues, is that not a big red flag suggesting there might need to be changes?</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: Tuw GUI Wrapper</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-tuw-gui-wrapper/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-tuw-gui-wrapper/</id>
        <media:content url="https://korgano.github.io/media/posts/25/unknown_2024.10.07-16.38.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC"/>
            <category term="Cybersecurity"/>

        <updated>2024-10-08T09:00:33-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/25/unknown_2024.10.07-16.38.png" alt="A successful execution of Unreal Engine 4 UCAS/UTOC packing via Tuw GUI" />
                    I have a love-hate relationship with command line. On one hand, I cannot deny the speed and efficiency command line&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/25/unknown_2024.10.07-16.38.png" class="type:primaryImage" alt="A successful execution of Unreal Engine 4 UCAS/UTOC packing via Tuw GUI" /></p>
                <p>I have a love-hate relationship with command line.</p>
<p>On one hand, I cannot deny the speed and efficiency command line has with some tasks.</p>
<p>On the other, command line requires not only knowledge of the commands and variables to input, but it requires a lot of typing, copy-pasting, and/or trial and error to get working properly.</p>
<p>Now, AI can make the iteration process a lot easier and faster, and once you've gotten a command perfected, you can make it into a script file, but most people are familiar and more comfortable with a Graphical User Interface (GUI).</p>
<p>But what if you could come up with a GUI for your Command Line Interface (CLI) commands?</p>
<h2>Tuw - The Open Source Solution</h2>
<p>I discovered <a href="https://github.com/matyalatte/tuw" title="Tuw: a tiny GUI wrapper for command-line tools" target="_blank" rel="noopener noreferrer">Tuw</a> thanks to someone on the Unreal Engine Modding Discord. (I would give credit, but it's very annoying to search Discord for things like this.)</p>
<p>A large number of tools for modding Unreal Engine games are CLI tools, which often involve multiple file paths and variables, and generally don't have a GUI created by the actual tool developer. Also, the documentation on the Github pages tend not to be the best. So they tend to be <strong>incredibly </strong>unfriendly to new users, who tend to search around for knowledge on how to mod a specific game that happens to use Unreal Engine.</p>
<p>How Tuw works is pretty simple. There is an executable file (.EXE for Windows, and other formats for Mac and Linux) that checks a JSON file for the following things:</p>
<ul>
<li>The actual command.</li>
<li>A button to execute the command.</li>
<li>Input fields.</li>
<li>Labels for the fields and the button.</li>
<li>Identifiers for variables </li>
<li>Rules for input validation and error handling.</li>
<li>A wide variety of other settings.</li>
</ul>
<h2>Keep It Stupid Simple</h2>
<p>There is an old axiom that you may have heard of: KISS.</p>
<p>You may be familiar with what it usually spells out: Keep It Simple, Stupid. It's a warning to keep people from creating overcomplicated solutions, because people like to associate complexity with sophistication. But there's a flip side of the coin that people tend to ignore.</p>
<p>Keep It Stupid Simple is an informal design goal. The polite summation is that the solution being designed for the task should be as simple and easy as possible, so that even an uneducated person can do the job. Some would say that Apple is a master of this, but other relevant (but non-tech related) examples would be the AK and Glock. If the thing you design can be easily and successfully used by anyone within a half hour or less, it's stupid simple.</p>
<p>Tuw very much fits this mold. Let's look at the JSON code I created to handle the task:</p>
<div>
<div><code>{</code></div>
<div><code>    "gui": [</code></div>
<div><code>        {</code></div>
<div><code>            "label": "Minimal Sample",</code></div>
<div><code>            "window_name": "Unreal IO Store Command",</code></div>
<div><code>            "command": "%UE4editor% %uProject% -run=IoStore -CreateGlobalContainer=%GlobalContainer% -CookedDirectory=%CookedDir% -Commands=%Commands% -CookerOrder=%CookerOrder% -TargetPlatform=WindowsNoEditor -stdout -UTF8Output",</code></div>
<div><code>            "button": "Pack uAssets",</code></div>
<div><code>            "components": [</code></div>
<div><code>                {</code></div>
<div><code>                    "type": "file",</code></div>
<div><code>                    "label": "UE4 Editor path",</code></div>
<div><code>                    "extension": "EXE files (*.exe)|*.exe",</code></div>
<div><code>                    "placeholder": "D:\\EGS\\UE_4.27\\Engine\\Binaries\\Win64\\UE4Editor-Cmd.exe",</code></div>
<div><code>                    "id": "UE4editor",</code></div>
<div><code>                    "validator": {</code></div>
<div><code>                                    "exist": true,</code></div>
<div><code>                                    "not_empty": true,</code></div>
<div><code>                                    "wildcard": "*.exe"</code></div>
<div><code>                                }</code></div>
<div><code>                },</code></div>
<div><code>                {</code></div>
<div><code>                    "type": "file",</code></div>
<div><code>                    "label": "Aliens: Dark Descent project path",</code></div>
<div><code>                    "extension": "uProject files (*.uproject)|*.uproject",</code></div>
<div><code>                    "placeholder": "D:\\EGS\\Projects\\ASF_Updated\\ASF.uproject",</code></div>
<div><code>                    "id": "uProject",</code></div>
<div><code>                    "validator": {</code></div>
<div><code>                        "exist": true,</code></div>
<div><code>                        "not_empty": true,</code></div>
<div><code>                        "wildcard": "*.uproject"</code></div>
<div><code>                    }</code></div>
<div><code>                },</code></div>
<div><code>                {</code></div>
<div><code>                    "type": "file",</code></div>
<div><code>                    "label": "Global.Utoc Path",</code></div>
<div><code>                    "extension": "uTOC files (*.utoc)|*.utoc",</code></div>
<div><code>                    "placeholder": "D:\\EGS\\Projects\\ASF_Updated\\Saved\\StagedBuilds\\WindowsNoEditor\\ASF\\Content\\Paks\\global.utoc",</code></div>
<div><code>                    "id": "GlobalContainer",</code></div>
<div><code>                    "validator": {</code></div>
<div><code>                        "exist": true,</code></div>
<div><code>                        "not_empty": true,</code></div>
<div><code>                        "wildcard": "*.utoc"</code></div>
<div><code>                    }</code></div>
<div><code>                },</code></div>
<div><code>                {</code></div>
<div><code>                    "type": "folder",</code></div>
<div><code>                    "label": "Path to uAssets",</code></div>
<div><code>                    "placeholder": "D:\\AliensDarkDescent\\zModCamera",</code></div>
<div><code>                    "add_quotes": false,</code></div>
<div><code>                    "id": "CookedDir"</code></div>
<div><code>                },</code></div>
<div><code>                {</code></div>
<div><code>                    "type": "file",</code></div>
<div><code>                    "label": "Commands text file",</code></div>
<div><code>                    "extension": "Text files (*.txt)|*.txt",</code></div>
<div><code>                    "placeholder": "D:\\EGS\\Projects\\ASF_Updated\\ASF_Upgrades.txt",</code></div>
<div><code>                    "add_quotes": true,</code></div>
<div><code>                    "id": "Commands",</code></div>
<div><code>                    "validator": {</code></div>
<div><code>                        "exist": true,</code></div>
<div><code>                        "not_empty": true,</code></div>
<div><code>                        "wildcard": "*.txt"</code></div>
<div><code>                    }</code></div>
<div><code>                },</code></div>
<div><code>                {</code></div>
<div><code>                    "type": "file",</code></div>
<div><code>                    "label": "Path to Cooker Order file",</code></div>
<div><code>                    "extension": "Log files (*.log)|*.log",</code></div>
<div><code>                    "placeholder": "D:\\EGS\\Projects\\ASF_Updated\\Build\\WindowsNoEditor\\FileOpenOrder\\CookerOpenOrder.log",</code></div>
<div><code>                    "add_quotes": false,</code></div>
<div><code>                    "id": "CookerOrder",</code></div>
<div><code>                    "validator": {</code></div>
<div><code>                        "exist": true,</code></div>
<div><code>                        "not_empty": true,</code></div>
<div><code>                        "wildcard": "*.log"</code></div>
<div><code>                    }</code></div>
<div><code>                }</code></div>
<div><code>            ]</code></div>
<div><code>        }</code></div>
<div><code>    ]</code></div>
<div><code>}</code></div>
</div>
<p><a href="https://github.com/matyalatte/tuw/blob/main/examples" target="_blank" rel="noopener noreferrer">Tuw comes with extensive documentation on how to set up the JSON file</a>. And most crucially, it comes with example code that actually shows how the code should be configured <strong>and </strong>integrated with the other code. A great deal of documentation for other items presupposes that the user has a great deal of applicable knowledge going in... Which then causes problems when the user does <strong>not </strong>have that knowledge.</p>
<p>And modding is perhaps the quintessential example of that type of scenario. In most circumstances, the person who wants to mod has <strong>zero </strong>experience with the specific programs they have to use. They might be familiar with editing code or configuration files, but perhaps not command line interfaces, especially if they're coming from a game where there's an official mod tool kit. </p>
<p>(I speak from experience.)</p>
<h2>The Tuw GUI Crafting Experience</h2>
<p>Making a GUI with Tuw is quite fast. Between copying and pasting code straight from the Github documentation and auto-complete, the main time sink will be the complexity of the command and the GUI. More fields naturally increases the amount of time spent making the GUI, especially if they're of multiple different types and require different types of validation.</p>
<p>Testing is a bit finicky, because the program expects you to use <code>gui_definition.json</code> as the file name. So there's no way to switch between GUIs without coding 2+ GUIs' worth of JSON code. This is a <strong>massive </strong>inconvenience <a href="https://github.com/matyalatte/tuw/tree/main/examples/get_start/json_embed" title="JSON Embedding" target="_blank" rel="noopener noreferrer">if you plan to make a Tuw program with the JSON code embedded in it for distribution</a>.</p>
<p>(The issue is that because Tuw defaults to reading <code>gui_definition.json</code>, which is the file that can be set to allow Tuw to generate a new version of Tuw with a GUI definition embedded in it. So have to have two separate JSONs, and the new EXE will have an annoying popup on run alerting you to the fact that it's ignoring <code>gui_definition.json</code> in favor of the one you specified.)</p>
<p>Once you initialize Tuw, go through the steps required to fill out the various fields, then press the button to execute the command. If there's an error, it'll show up if you have validator or <a href="https://github.com/matyalatte/tuw/tree/main/examples/other_features/error" title="Error Handling" target="_blank" rel="noopener noreferrer">error handling code</a>. In my case, I initially had regular expression <strong>and </strong>wildcard matching for the file extensions, but for some reason, every single file reported as invalid. Removing those lines from the JSON fixed the problem.</p>
<p>Once that was done, I went through the process of filling out the fields and got the following result:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/25/unknown_2024.10.07-16.38-2.png" alt="A successful execution of Unreal Engine 4 UCAS/UTOC packing via Tuw GUI" width="1920" height="1080" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/25/responsive/unknown_2024.10.07-16.38-2-xs.png 384w ,https://korgano.github.io/media/posts/25/responsive/unknown_2024.10.07-16.38-2-sm.png 600w ,https://korgano.github.io/media/posts/25/responsive/unknown_2024.10.07-16.38-2-md.png 768w ,https://korgano.github.io/media/posts/25/responsive/unknown_2024.10.07-16.38-2-lg.png 1200w ,https://korgano.github.io/media/posts/25/responsive/unknown_2024.10.07-16.38-2-xl.png 1600w"></figure>
<p>Tuw successfully completed the file generation part of the task, which meant any issues would be on my end.</p>
<h2>The Cybersecurity Applications</h2>
<p>How can this be leveraged for cybersecurity?</p>
<p>There's a non-zero use case for a GUI when you have a long command that requires multiple file and/or folder paths. But if we're honest, I doubt that it's all that common, and you're better off using a conventional script... unless your organization just blanket disables command line <strong>and </strong>PowerShell. Then this might be your only option.</p>
<p>But what comes to mind is a stupid simple way to execute troubleshooting commands on an endpoint PC for a non-technical and/or non-permissioned user. For example, you could make multiple Tuw based executable files for ping testing when a user calls with a network connection issue. One file would execute <code>ping 8.8.8.8</code>, another would ping a specified endpoint, and so on, and each would only require the user to make a few clicks.</p>
<p>Naturally, you'd need to have someone qualified inspect the source code and verify that it was safe to use, but on the plus side, all those JSONs for your GUIs should be compatible across versions. If there's ever a problem, you can just rebuild the embedded GUI executables by combining a known good version of Tuw with those JSONs.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: More Reasons for Local AI</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-more-reasons-for-local-ai/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-more-reasons-for-local-ai/</id>
        <media:content url="https://korgano.github.io/media/posts/24/chatgpt-jank.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2024-09-19T10:17:01-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/24/chatgpt-jank.png" alt="ChatGPT web interface that cannot connect to ChatGPT/OpenAI servers." />
                    This week, I was hoping to follow up on my previous cybersecurity project post, only to run into an unforeseen&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/24/chatgpt-jank.png" class="type:primaryImage" alt="ChatGPT web interface that cannot connect to ChatGPT/OpenAI servers." /></p>
                <p>This week, I was hoping to follow up on <a href="https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-30/" title="CyberSecurity Project: Transparent Filtering Bridge (+ Extras) 3.0" target="_blank" rel="noopener noreferrer">my previous cybersecurity project post</a>, only to run into an unforeseen problem.</p>
<p>Since it's best to make lemonade out of lemons, let's talk about what happens when web AI interfaces stop working.</p>
<h2>The Problem</h2>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/24/chatgpt-jank-2.png" alt="ChatGPT web interface that cannot connect to ChatGPT/OpenAI servers." width="1920" height="952" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/24/responsive/chatgpt-jank-2-xs.png 384w ,https://korgano.github.io/media/posts/24/responsive/chatgpt-jank-2-sm.png 600w ,https://korgano.github.io/media/posts/24/responsive/chatgpt-jank-2-md.png 768w ,https://korgano.github.io/media/posts/24/responsive/chatgpt-jank-2-lg.png 1200w ,https://korgano.github.io/media/posts/24/responsive/chatgpt-jank-2-xl.png 1600w"></figure>
<p>Two days ago, my ChatGPT session went from normal to what you see above.</p>
<p>Typing does nothing - I can make a prompt, but there's no data transmission to ChatGPT.</p>
<p>Clearing cache, temporarily disabling PiHole DNS blackhole, and ad/script blockers do nothing. I'll have to do more investigation to see if the connectivity issue is on my end or ChatGPT's, but it does provide another great example of why local AI is a good idea.</p>
<p>(Bing Copilot is still available, so it's not something that affecting all web AI, as far as I can tell.)</p>
<h2>AI Needs High Availability</h2>
<p>In IT/cybersecurity, the Five 9s are the gold standard for availability - 99.999% uptime.</p>
<p>Let's set aside all the data confidentiality and integrity issues that might come with using AI (Large and Small Language Models - LLMs/SLMs). The main benefit of AI at the moment is generating content - informational or artistic - quickly and in accordance with the user's input.</p>
<p>In order to be useful, it needs to be <strong>available</strong>, and an AI hosted on someone else's servers is always going to be vulnerable to service disruption. It could be at their end, your end, or even the ISP in general, but there's plenty of failure points on the chain.</p>
<p>So, how do we get around this?</p>
<h2>Exploit the Training Bubble Pop</h2>
<p><a href="https://www.rand.org/pubs/research_reports/RRA2680-1.html" title="The Root Causes of Failure for Artificial Intelligence Projects and How They Can Succeed" target="_blank" rel="noopener noreferrer">80% of AI projects fail</a>, according to RAND Corporation research.</p>
<p>Most of that comes down to inflated expectations created by Hollywood and grifters, but also a fundamental misunderstanding of how LLMs and SLMs function. You don't <strong>need </strong>to train the AI on your data to get useful outputs from it. You can use Retrieval Augmented Generation (RAG) to expand the AI's knowledge base:</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/T-D1OfcDW1M?pp=ygUecmV0cmlldmFsIGF1Z21lbnRlZCBnZW5lcmF0aW9u" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>So we only need the hardware to do the inferencing (generating responses), which is a lower tier of complexity and performance requirements than training.</p>
<p>Graphics cards and Neural Processing Unit (NPU) equipped chips are providing most of the inferencing hardware on the market, but they're not exactly cost efficient. Decent performance GPUs cost hundreds of dollars, and NPU equipped processors are only in laptops, which also require hundreds of dollars each. What are we to do?</p>
<p>Well, thanks to the billions of dollars spent on AI training hardware, which is also capable of inferencing, and the general trend of AI project failure, the answer is simple. Wait for the AI training bubble to pop, and acquire the used hardware as it's being liquidated to keep the companies afloat.</p>
<h2>Things to Keep in Mind</h2>
<p>So, on top of all the usual caveats about buying used hardware, especially in a corporate environment, there's one specific thing to keep in mind:</p>
<p>AMD's server chips have a fuse in them that trips when they are initialized, locking them to a specific vendor's motherboards.</p>
<p>This raises the cost of some used AI server hardware, as buying a matched processor and motherboard set is going to be safer than trying to get two separate units and hope that AMD didn't put even more specific fuses in there to further restrict hardware choice.</p>
<p>Another thing to consider is what information will be provided to the AI through RAG, where the information is to be hosted, and how implement network segmentation.</p>
<p>For instance, for a local AI server in a home or single-building small business, it may make sense to host all the RAG data you want on that same system. (It may also be the only option, if you're using software that doesn't allow for remote access to data.) However, the more data you have, the more likely it is that a NAS or Storage Area Network (SAN) might be necessary.</p>
<p>What that data is also plays a huge part in figuring out your storage needs. If you're just pulling down text documentation for software you're using and making it accessible to the AI, you could probably get away with way less storage than an organization that's using RAG on multimedia content.</p>
<p>Naturally, you also have to be careful what you make available to the AI, because if prompted the right way, it <strong>will </strong>provide that information, whether you intended for that to happen or not. Even if you apply access controls to the AI or the data, it may be simpler and easier to redact/remove any Personal Identification Information (PII) or Personal Health Information (PHI). The AI can't leak what it doesn't have, and it reduces your compliance headaches.</p>
<p>Network segmentation is also going to be an interesting challenge. Obviously, the AI server should be on its own segment, so that access can be controlled through firewall control lists. But should the storage servers associated with the RAG data also be on the same network segment?</p>
<p>These questions and more are what we cybersecurity practioners need to ponder as organizations adopt AI and integrate it into their operations.</p>
            ]]>
        </content>
    </entry>
</feed>
