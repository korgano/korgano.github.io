<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:media="http://search.yahoo.com/mrss/">
    <title>XS Tech Thoughts</title>
    <link href="https://korgano.github.io/feed.xml" rel="self" />
    <link href="https://korgano.github.io" />
    <updated>2025-05-23T15:39:17-04:00</updated>
    <author>
        <name>Xavier Santana</name>
    </author>
    <id>https://korgano.github.io</id>

    <entry>
        <title>Quick Tech Thoughts: Computex 2025</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-tech-thoughts-computex-2025/"/>
        <id>https://korgano.github.io/quick-tech-thoughts-computex-2025/</id>
        <media:content url="https://korgano.github.io/media/posts/39/05rNRArkzOxhgSDWsZqRneF-1.webp" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC Hardware"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2025-05-20T09:10:40-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/39/05rNRArkzOxhgSDWsZqRneF-1.webp" alt="Computex 2025 Key Art" />
                    Computex 2025 has just wrapped up, showcasing all sorts of computing related hardware and, of course, lots of AI related&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/39/05rNRArkzOxhgSDWsZqRneF-1.webp" class="type:primaryImage" alt="Computex 2025 Key Art" /></p>
                <p>Computex 2025 has just wrapped up, showcasing all sorts of computing related hardware and, of course, lots of AI related things.</p>
<p>This year had some odd surprises, so let's jump into it.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1irv4itv2lp">Intel Surprise #1: Budget AI GPUs for Home Labbers?</a></li>
<li><a href="#mcetoc_1irv4itv2lq">Intel Surprise #2: $500 24 GB GPUs</a></li>
<li><a href="#mcetoc_1irv4itv2lr">Intel Surprise #3: A Sub-$1000 Dual GPU</a></li>
<li><a href="#mcetoc_1irv4itv2ls">The Intel Hold Up</a></li>
<li><a href="#mcetoc_1irv4itv2lt">AMD Getting Serious About ROCM on Windows</a></li>
</ul>
</div>
<h2 id="mcetoc_1irv4itv2lp">Intel Surprise #1: Budget AI GPUs for Home Labbers?</h2>
<figure class="post__image"><img loading="lazy"  src="https://cdn.videocardz.com/1/2025/05/INTEL-ARC-PRO-B50-B60-4.jpg" alt="Intel Arc Pro B50 GOU specifications, design features, and software compatibility." width="2560" height="1440" data-is-external-image="true"></figure>
<p>Intel's Graphics Card Division has not had a good time, to put it mildly.</p>
<p>While people were excited for the B580 due to its mix of features, performance, and purported low price point, Intel basically couldn't supply the thing at the price people wanted. The B570, a cutdown version of the B580 with reductions in specs and MSRP, basically made no impact. Driver updates have been few and far between, and support for their own software features has been pretty minimal.</p>
<p>So it was shocking to see Intel announce the B50, a half-height, half-length GPU that only uses 70 watts of power.</p>
<p>Why?</p>
<p>Because this is basically a card for budget home labbers. This is the kind of card you can buy and slap into a cheap Dell/HP/Lenovo OEM desktop, as seen below (in gaming <strong>and </strong>home lab contexts):</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/XCWsUiJZiiA" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/Ir53nheghcI" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>These typically cheap, used systems tend to have a few constraints when it comes to provisioning them with new hardware:</p>
<ul>
<li>They cannot support full width GPUs (or any other PCIe cards, for that matter).</li>
<li>The front to back length of the case is typically much shorter than most PC cases.</li>
<li>They use TFX power supplies with low wattages and limited power connectors.</li>
<li>Many designs use proprietary motherboards and other components, constraining upgrades.</li>
</ul>
<p>The B50 meets all the requirements to fit in these systems:</p>
<ul>
<li>Half-width.</li>
<li>Compact length.</li>
<li>70 watt power draw can be supplied through the motherboard PCIe slot.</li>
</ul>
<p>The main constraints on the B50 are:</p>
<ul>
<li>Clearance - some of these compact towers do not provide two PCIe slots worth of clearance from the motherboard slot to the power supply.</li>
<li>Price - the B50 has an MSRP of $300, which may or may not be realistic or achievable on release day.</li>
<li>Noise - it has a blower fan, which typically have a noise profile that's worse than the typical fan arrangements on other graphics cards.</li>
</ul>
<p>In exchange, you are getting a GPU that can provide over 3x the TOPs of the best price/performance APUs AMD produces (Strix/Gorgon Point), <strong>and </strong>16GB of VRAM. It's pretty hard to beat this combination if you're on a budget.</p>
<h2 id="mcetoc_1irv4itv2lq">Intel Surprise #2: $500 24 GB GPUs</h2>
<figure class="post__image"><img loading="lazy"  src="https://cdn.videocardz.com/1/2025/05/INTEL-ARC-PRO-B50-B60-7.jpg" alt="Intel Arc Pro B60 specifications, design features, and software compatibility." width="2560" height="1440" data-is-external-image="true"></figure>
<p>The next surprise wasn't necessarily the fact that Intel was producing a 24GB professional card. That had leaked weeks prior to Computex.</p>
<p>What <strong>was </strong>surprising was the price point: $500.</p>
<p>However, this makes sense when we factor in a few things:</p>
<ul>
<li>According to analysts like Moore's Law is Dead, GDDR6 chip prices may be as low as $6 per chip.</li>
<li>The B580 already had 12GB of VRAM.</li>
<li>Clamshelling - mounting VRAM chips to the rear of the printed circuit board (PCB) - has become a more common technique for increasing VRAM capacity in GPUs.</li>
<li>The B580's low MSRP came at the cost of slashing margins as much as possible.</li>
</ul>
<p>On a technical level, $500 is almost certainly a price where Intel and its add-in board (AIB) partners can make a sustainable amount of profit. From a pure business angle, it's surprising that Intel didn't try to charge <strong>more </strong>for 24GB of VRAM, given how its competitors (AMD and Nvidia) handle segmentation.</p>
<p>That said, AI performance is not necessarily this card's strong suite. The performance gains over the B50 are not great, at least going by the TOPs figure. However, what you do get is more VRAM, and therefore a bigger context window, or interaction memory, for an AI. </p>
<h2 id="mcetoc_1irv4itv2lr">Intel Surprise #3: A Sub-$1000 Dual GPU</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/Y8MWbPBP9i0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>It's not everyday when you get to see old technological concepts return from the dead.</p>
<p>Roughly 15 years ago, GPU AIBs gave up on the idea of combining two GPUs into a single graphics card. There were just too many hurdles in terms of synchronizing the GPUs and their outputs, causing limited software support for those use-cases. Then MaxSun, an Intel AIB, revived it for the B60 professional cards.</p>
<p>Let's get the drawbacks out of the way:</p>
<ul>
<li>This card requires a motherboard that can support PCIe bifurcation - splitting the interface from x16 to x8/x8.</li>
<li>The operating system and AI software will see it as two GPUs.</li>
<li>It requires using Nvidia's infamous 12 volt High Power connector, which has notorious melting issues.</li>
</ul>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/Y36LMS5y34A" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>So, what would anyone use this for? Well, besides running gigantic models or training workloads by splitting the load across two GPUs, the most obvious use case is running two models at the same time with large context windows. For example, running a coding LLM and an image generation model at the same time.</p>
<p>A particularly interesting cybersecurity application is running a generative model alongside Cisco Foundation AI's <a href="https://huggingface.co/fdtn-ai/Foundation-Sec-8B" title="Foundation-Sec-8B - Model Card">Foundation-Sec-8B</a> model. What is Foundation-Sec-8B?</p>
<blockquote>
<p>Foundation-Sec-8B (Llama-3.1-FoundationAI-SecurityLLM-base-8B) is an open-weight, 8-billion parameter base language model specialized for cybersecurity applications. It extends Llama-3.1-8B model through continued pretraining on a curated corpus of cybersecurity-specific text, including threat intelligence reports, vulnerability databases, incident response documentation, and security standards. It has been trained to understand security concepts, terminology, and practices across multiple security domains. The model is designed to serve as a domain-adapted base model for use in applications such as threat detection, vulnerability assessment, security automation, and attack simulation. Foundation-Sec-8B enables organizations to build AI-driven security tools that can be deployed locally, reducing dependency on cloud-based AI services while maintaining high performance on security-related tasks.</p>
</blockquote>
<p>With this graphics card, assuming proper configuration, it would be possible to create an agentic workflow where code is generated in one model, then passed to Foundation-Sec for vulnerability assessment, then return the vulnerability report to the coding model. A 24GB VRAM buffer would be an appropriate size for Foundation-Sec (~16.2 GB)</p>
<p>It should be noted that this particular configuration makes the most sense if you are budget/space/slot constrained and can't afford a system with multiple B60 GPUs.</p>
<p>Or if you have a motherboard with a <strong>lot </strong>of PCIe 5.0x16 slots, and want to populate every one of them with as many GPUs as possible.</p>
<h2 id="mcetoc_1irv4itv2ls">The Intel Hold Up</h2>
<p>The primary road block for these products is the fact that they will be releasing as integrated parts of professional products first. In fact, one could say they're a pipe cleaner for the drivers prior to the consumer release.</p>
<p>With a scheduled professional release in Q3 2025, and consumer release in Q4 2025, barring delays, it could be <strong>many </strong>months before this hardware becomes available, although it would not surprise me if the B50 made its way to consumers sooner than expected.</p>
<h2 id="mcetoc_1irv4itv2lt">AMD Getting Serious About ROCM on Windows</h2>
<p>It's hard to imagine now, but 15 years ago, AMD was a company on the verge of death.</p>
<p>A bad bet on CPU architecture for the Bulldozer/Piledriver series of CPUs left them in a precarious financial position. While they were able to star clawing their way back to success with their Ryzen CPU architecture in 2017, that bad decision meant that they were at a massive disadvantage against Nvidia. During this period, Nvidia laid down the foundation of its CUDA compute ecosystem, which currently dominates the AI landscape.</p>
<p>Never the less, AMD has an open-source software stack for accelerated computing, called ROCm (Radeon Open Compute Platform). This stack has been largely focused on data center and workstations, so it has high Linux hardware compatibility, but very poor Windows compatibility. This is a pretty big issue, because AMD's own hardware cannot run CUDA - and the conversion layer, ZLUDA, is a pain to install and troubleshoot. </p>
<figure class="post__image"><img loading="lazy"  src="https://cdn.videocardz.com/1/2025/05/AMD-ROCM-COMPUTEX-4.jpg" alt="AMD ROCm Windows Support Timeline" width="1100" height="520" data-is-external-image="true"></figure>
<p>AMD announced official Windows support at Computex 2025, with specific frameworks either being supported in July (ONNX-EP) or sometime in Q3 (PyTorch). They also enabled support on RDNA 4 graphics cards, which will be relevant a little bit later. Linux support is also expanding to Ubuntu and Red Hat EPEL in the back half of 2025.</p>
<h2>AMD Launches a 16GB GPU for $350</h2>
<figure class="post__image"><img loading="lazy"  src="https://cdn.videocardz.com/1/2025/05/RADEON-RX-9060XT-4.jpg" alt="AMD slide showing price, VRAM amounts, and release date for RX 9060 XT graphics cards." width="1200" height="646" data-is-external-image="true"></figure>
<p>As if timed to take the wind out of Intel's sails, AMD announced a new (but known well beforehand) GPU with 16GB VRAM at $350.</p>
<p>(There is also an 8GB version at $300, but literally no one but the most desperate should be paying that much for an 8GB card in 2025.)</p>
<p>Setting aside whether the $350 price will stick, what the RX 9060 XT offers verses the B50 is a mixed set of features:</p>
<ul>
<li>16GB of VRAM.</li>
<li>ROCm acceleration support.</li>
<li>Up to 182 watt power draw.</li>
<li>Release date inside of 2 weeks.</li>
</ul>
<p>The interesting thing with RDNA 4, and AMD GPUs in general, is that despite ROCm not being available on Windows, it is <strong>not </strong>necessary for decent performance in text LLM inferencing tasks. Many chat interfaces rely on Llama, an open source inferencing software library. The library has support for a variety of APIs and frameworks, including Vulkan (an open-source graphics API). Meaning that if someone only wanted to experiment with text only Generative AI, the 9060 XT (16GB) might be a decent budget option in the current market.</p>
<h2>The TP-Link Ban Idea Returns; Still No Solution to the Obvious Consequences</h2>
<p>This is something that I was keeping an eye on in January, <a href="https://korgano.github.io/quick-cyber-thoughts-ces-2025/" title="Quick Cyber Thoughts: CES 2025 Aftermath" target="_blank" rel="noopener noreferrer">in the lead up to CES 2025</a>, and has resurfaced after a months long hibernation:</p>
<blockquote>
<p>A group of Republican lawmakers are urging the Trump administration to ban sales of <a href="https://www.pcmag.com/brands/tp-link" target="_self">TP-Link</a> networking products, including the company’s popular <a href="https://www.pcmag.com/picks/the-best-wireless-routers" target="_self">Wi-Fi routers</a>. </p>
<p>The Commerce, Defense, and Justice Departments have reportedly been <a href="https://www.pcmag.com/news/us-considers-banning-tp-link-routers-over-security-concerns" target="_self">investigating TP-Link</a> for national security risks. On Wednesday, the lawmakers—including 12 GOP senators—wrote to the Commerce Department to voice support for its investigation, and called on the White House to block further sales of TP-Link products in the US. </p>
<p><a href="https://www.pcmag.com/news/tp-link-accused-of-keeping-router-prices-low-to-help-china-conduct-cyberattacks">-https://www.pcmag.com/news/tp-link-accused-of-keeping-router-prices-low-to-help-china-conduct-cyberattacks</a></p>
</blockquote>
<p>Basically, <a href="https://www.cotton.senate.gov/imo/media/doc/tplinkfinal.pdf" title="TP-Link Letter - Office of Tom Cotton" target="_blank" rel="noopener noreferrer">lawmakers have reasonable and understandable concerns that TP-Link has monopolized a huge portion of the Small Office-Home Office (SOHO) router market, and may be a weak point in America's internet infrastructure</a>.</p>
<p>I have no problem with the argument or the sentiment. The problem is that there's also no sign of thought about what happens <strong>after </strong>the requested investigation. And I mean from both the government and private sector. This is not particularly surprising from the government, given all the strife lately, but it is concerning that router manufacturers don't seem to be rising to the challenge of filling the void that would be caused by TP-Link routers being outright banned by the US government.</p>
<p>Given the supply disruptions we've seen due to the Trump administration's use of tariffs as a negotiation tool, it's highly likely that if router manufacturers don't start a gradual increase in supply, there'll be a painful run on routers if the TP-Link ban goes through.</p>
<p>Of course, the best solution would be providing a simple, effective way to replace the firmware on these devices with up-to-date versions of OpenWRT, a router operating system. Unfortunately, with only a fraction of TP-Link devices currently supported, it looks like a lot of Americans will be paying an unforeseen price for their past IT equipment choices.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: Absolute Zero Reasoning</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-absolute-zero-reasoning/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-absolute-zero-reasoning/</id>
        <media:content url="https://korgano.github.io/media/posts/38/absolute-zero-uhoh.png" medium="image" />

        <updated>2025-05-15T13:30:27-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/38/absolute-zero-uhoh.png" alt="Absolute Zero Reasoner – Llama3.1-8B “Uh-oh Moment.” This example highlights an unexpected and potentially unsafe reasoning chain generated by the Absolute Zero Reasoner–Llama3.1-8B model during training. Although the paradigm enables reasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors." />
                    No, this isn't a joke about/reference to vibe coders. Instead, it's a new method of training reasoning AI: What is&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/38/absolute-zero-uhoh.png" class="type:primaryImage" alt="Absolute Zero Reasoner – Llama3.1-8B “Uh-oh Moment.” This example highlights an unexpected and potentially unsafe reasoning chain generated by the Absolute Zero Reasoner–Llama3.1-8B model during training. Although the paradigm enables reasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors." /></p>
                <p>No, this isn't a joke about/reference to vibe coders.</p>
<p>Instead, it's a new method of training reasoning AI:</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/CqdqZNqljdI" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>What is it? Well, it just could be the thing all the AI safety people have been warning about.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1irahtd8o1">How it Works</a></li>
<li><a href="#mcetoc_1iraln3ch2j">The Key Takeaways</a></li>
</ul>
</div>
<h2 id="mcetoc_1irahtd8o1">How it Works</h2>
<p>Absolute Zero Reasoning, as explained by its creators, is the following:</p>
<blockquote>
<p><span id="S1.p2.1.1" class="ltx_text">To this end, we propose </span><em id="S1.p2.1.2" class="ltx_emph ltx_font_italic">“Absolute Zero”</em><span id="S1.p2.1.3" class="ltx_text">, a new paradigm for reasoning models in which the model simultaneously learns to define tasks that maximize learnability and to solve them effectively, enabling self-evolution through self-play without relying on external data. In contrast to prior self-play methods that are limited to narrow domains, fixed functionalities, or learned reward models that are prone to hacking </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.p2.1.4.1" class="ltx_text">(</span><span class="ltx_text">Silver et al.</span><span id="S1.p2.1.5.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib56" title=""><span class="ltx_text">2017</span></a>; <span class="ltx_text">Chen et al.</span><span id="S1.p2.1.5.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib4" title=""><span class="ltx_text">2025</span></a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib5" title=""><span class="ltx_text">2024</span></a><span id="S1.p2.1.6.3" class="ltx_text">)</span></cite><span id="S1.p2.1.7" class="ltx_text">, the </span><em id="S1.p2.1.8" class="ltx_emph ltx_font_italic">Absolute Zero</em><span id="S1.p2.1.9" class="ltx_text"> paradigm is designed to operate in open-ended settings while remaining grounded in a real environment. It relies on feedback from the environment as a verifiable source of reward, mirroring how humans learn and reason through interaction with the world, and helps prevent issues such as hacking with neural reward models </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.p2.1.10.1" class="ltx_text">(</span><span class="ltx_text">Hughes et al.</span><span id="S1.p2.1.11.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib25" title=""><span class="ltx_text">2024</span></a><span id="S1.p2.1.12.3" class="ltx_text">)</span></cite><span id="S1.p2.1.13" class="ltx_text">. Similar to AlphaZero </span><cite class="ltx_cite ltx_citemacro_citep"><span id="S1.p2.1.14.1" class="ltx_text">(</span><span class="ltx_text">Silver et al.</span><span id="S1.p2.1.15.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib56" title=""><span class="ltx_text">2017</span></a><span id="S1.p2.1.16.3" class="ltx_text">)</span></cite><span id="S1.p2.1.17" class="ltx_text">, which improves through self-play, our proposed paradigm requires no human supervision and learns entirely through self-interaction. We believe the Absolute Zero paradigm represents a promising step toward enabling large language models to autonomously achieve superhuman reasoning capabilities.</span></p>
<p>-Zhao, et al, <a href="https://arxiv.org/html/2505.03335v2">https://arxiv.org/html/2505.03335v2</a></p>
</blockquote>
<p>In plain English, Absolute Zero Reasoning is a training method where an LLM creates a coding or mathematics challenge that's calibrated to be challenging, then uses certain functions in the Python environment to validate the results. Successes and failures train the LLM as it iterates through the tasks, improving it capabilities through the use of three methods of reasoning.</p>
<h2 id="mcetoc_1iraln3ch2j">The Key Takeaways</h2>
<p>Helpfully, the paper has a summary of the key findings of this research:</p>
<blockquote>
<p><strong>• </strong><strong>Code priors amplify reasoning.</strong><span id="S1.I1.i1.p1.1.2" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><strong> </strong>The base </span><span id="S1.I1.i1.p1.1.3" class="ltx_text ltx_font_typewriter" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Qwen-Coder-7b</span><span id="S1.I1.i1.p1.1.4" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> model started with math performance 3.6 points lower than </span><span id="S1.I1.i1.p1.1.5" class="ltx_text ltx_font_typewriter" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Qwen-7b</span><span id="S1.I1.i1.p1.1.6" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. But after AZR training for both models, the coder variant surpassed the base by 0.7 points, suggesting that strong coding capabilities may potentially amplify overall reasoning improvements after AZR training.</span><br><strong>• Cross domain transfer is more pronounced for AZR.</strong><span id="S1.I1.i2.p1.1.2" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><strong> </strong>After RLVR, expert code models raise math accuracy by only 0.65 points on average, whereas </span><span id="S1.I1.i2.p1.1.3" class="ltx_text ltx_font_typewriter" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">AZR<span id="S1.I1.i2.p1.1.3.1" class="ltx_text">-</span>Base<span id="S1.I1.i2.p1.1.3.2" class="ltx_text">-</span>7B</span><span id="S1.I1.i2.p1.1.4" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> and </span><span id="S1.I1.i2.p1.1.5" class="ltx_text ltx_font_typewriter" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">AZR<span id="S1.I1.i2.p1.1.5.1" class="ltx_text">-</span>Coder<span id="S1.I1.i2.p1.1.5.2" class="ltx_text">-</span>7B</span><span id="S1.I1.i2.p1.1.6" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> trained on self-proposed code reasoning tasks improve math average by 10.9 and 15.2, respectively, demonstrating much stronger generalized reasoning capability gains.</span><br><strong>• </strong><strong>Bigger bases yield bigger gains.</strong><span id="S1.I1.i3.p1.1.2" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><strong> </strong>Performance improvements scale with model size: the 3B, 7B, and 14B coder models gain +5.7, +10.2, and +13.2 points respectively, suggesting continued scaling is advantageous for AZR.</span><br><strong>• </strong><strong>Comments as intermediate plans emerge naturally.</strong><span id="S1.I1.i4.p1.1.2" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><strong> </strong>When solving code induction tasks, AZR often interleaves step-by-step plans as comments and code (</span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#A3.F19" title="In C.3 Interplay Between Propose and Solve Roles ‣ Appendix C More Results" style="font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">19</span></a><span id="S1.I1.i4.p1.1.3" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">), resembling the ReAct prompting framework </span><cite class="ltx_cite ltx_citemacro_citep" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><span id="S1.I1.i4.p1.1.4.1" class="ltx_text">(</span><span class="ltx_text">Yao et al.</span><span id="S1.I1.i4.p1.1.5.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib75" title=""><span class="ltx_text">2023</span></a><span id="S1.I1.i4.p1.1.6.3" class="ltx_text">)</span></cite><span id="S1.I1.i4.p1.1.7" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. Similar behavior has been observed in much larger formal-math models such as DeepSeek Prover v2 (671B) </span><cite class="ltx_cite ltx_citemacro_citep" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><span id="S1.I1.i4.p1.1.8.1" class="ltx_text">(</span><span class="ltx_text">Ren et al.</span><span id="S1.I1.i4.p1.1.9.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib48" title=""><span class="ltx_text">2025</span></a><span id="S1.I1.i4.p1.1.10.3" class="ltx_text">)</span></cite><span id="S1.I1.i4.p1.1.11" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">. We therefore believe that allowing the model to use intermediate scratch-pads when generating long-form answers may be beneficial in other domains as well.</span><br><strong>• Cognitive Behaviors and Token length depends on reasoning mode.</strong><span id="S1.I1.i5.p1.1.2" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><strong> </strong>Distinct cognitive behaviors—such as step-by-step reasoning, enumeration, and trial-and-error all emerged through AZR training, but different behaviors are particularly evident across different types of tasks. Furthermore token counts grow over AZR training, but the magnitude of increase also differs by task types: abduction grows the most because the model performs trial-and-error until output matches, whereas deduction and induction grow modestly.</span><br><strong>• Safety alarms ringing.</strong><span id="S1.I1.i6.p1.1.2" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> We observe AZR with </span><span id="S1.I1.i6.p1.1.3" class="ltx_text ltx_font_typewriter" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">Llama3.1-8b</span><span id="S1.I1.i6.p1.1.4" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"> occasionally produces concerning chains of thought, we term the “uh</span><span id="S1.I1.i6.p1.1.5" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">-</span><span id="S1.I1.i6.p1.1.6" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">oh moment”, example shown in </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#A3.F32" title="In C.5 Generated Code Complexity Dynamics Between Abd/Ded and Ind. ‣ Appendix C More Results" style="font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><span class="ltx_text ltx_ref_tag">Figure</span> <span class="ltx_text ltx_ref_tag">32</span></a><span id="S1.I1.i6.p1.1.7" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">, highlighting the need for future work on safety</span><span id="S1.I1.i6.p1.1.8" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">-</span><span id="S1.I1.i6.p1.1.9" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">aware training </span><cite class="ltx_cite ltx_citemacro_citep" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);"><span id="S1.I1.i6.p1.1.10.1" class="ltx_text">(</span><span class="ltx_text">Zhang et al.</span><span id="S1.I1.i6.p1.1.11.2.1.1" class="ltx_text">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.03335v2#bib.bib86" title=""><span class="ltx_text">2025a</span></a><span id="S1.I1.i6.p1.1.12.3" class="ltx_text">)</span></cite><span id="S1.I1.i6.p1.1.13" class="ltx_text" style="color: var(--text-primary-color); font-family: var(--editor-font-family); font-size: inherit; font-weight: var(--font-weight-normal);">.</span></p>
<p>-Zhao, et al, <a href="https://arxiv.org/html/2505.03335v2">https://arxiv.org/html/2505.03335v2</a></p>
</blockquote>
<p>It's fascinating to see that pre-training a model on coding can provide a boost to overall reasoning capability. Absolute Zero Reasoning leading to better math reasoning after training models to code also makes sense. A lot of code is just a complicated math problem, and learning how to code more effectively could help the AI figure out better methods of solving those problems.</p>
<p>However, there's two takeaways worth thinking about a little further.</p>
<h2>Bigger Model Better?</h2>
<figure class="post__image"><img loading="lazy"  src="https://arxiv.org/html/2505.03335v2/x5.png" alt="AZR Training In-Distribution Accuracy over 200 training steps, for a Llama 8B parameter model and AZR trained 3, 7, and 14B parameter models" width="1660" height="1319" data-is-external-image="true"></figure>
<div id="S4.SS2.SSS0.Px3.p2" class="ltx_para ltx_noindent">
<blockquote>
<p id="S4.SS2.SSS0.Px3.p2.1" class="ltx_p"><span id="S4.SS2.SSS0.Px3.p2.1.1" class="ltx_text">The results reveal a clear trend: our method delivers </span><em id="S4.SS2.SSS0.Px3.p2.1.2" class="ltx_emph ltx_font_italic">greater gains on larger, more capable models</em><span id="S4.SS2.SSS0.Px3.p2.1.3" class="ltx_text">. In the in-distribution setting, the 7B and 14B models continue to improve beyond 200 training steps, whereas the smaller 3B model appears to plateau. For out-of-distribution domains, larger models also show greater overall performance improvements than smaller ones: +5.7, +10.2, +13.2 overall performance gains, respectively for 3B, 7B and 14B. This is an encouraging sign, since base models continue to improve and also suggesting that scaling enhances the effectiveness of AZR. In future work, we aim to investigate the scaling laws that govern performance in the Absolute Zero paradigm.</span></p>
<p>-Zhao, et al, <a href="https://arxiv.org/html/2505.03335v2">https://arxiv.org/html/2505.03335v2</a></p>
</blockquote>
<p>There's a presupposition that we're in the midst of an AI bubble due to a lot of companies buying a lot of expensive hardware to handle the compute requirements of training AI, and gaining little to show for that investment.</p>
<p>Absolute Zero Reasoning might flip that wisdom on its head. With bigger models not only having higher starting accuracy, but also continuing to improve as the training time increases, all that hardware suddenly seems <strong>very </strong>useful. If you have a boatload of compute, a huge amount of memory, and plenty of time, you could potentially use it to create a model that would surpass a human in coding and mathematical reasoning.</p>
<p>That has a number of long term strategic implications across a wide variety of fields:</p>
<ul>
<li>Demand for the latest AI hardware by foundation model makers could skyrocket, exacerbating supply issues.</li>
<li>Older AI hardware would be unlikely to filter into the secondary market, because lack of availability for newer hardware.</li>
<li>Smaller/less well funded organizations would have difficulties improving or standing up local AI capabilities due to lack of supply of new or older hardware.</li>
<li>Frontier AI firms would be able to further consolidate and dominate the AI market, as those with the most compute would benefit the most from AZR.</li>
<li>Software security would basically become a test of whether developers had access to the most capable AZR model and utilized (system) prompts aiming to create secure systems by default.</li>
</ul>
<p>This is naturally a pessimistic outlook, one that could literally be obsoleted next week at the earliest by AMD at Computex 2025. Mass production of products with their currently unused Neural Processing Unit chiplets would certainly ease the supply constraints on decent AI hardware. Improvements in software could also help ease these issues, as better CUDA translation layers for other architectures could allow for more performance to be obtained from hardware that is less in demand.</p>
<h2>The Safety Problem</h2>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/38/absolute-zero-uhoh-2.png" alt="Absolute Zero Reasoner – Llama3.1-8B “Uh-oh Moment.” This example highlights an unexpected and potentially unsafe reasoning chain generated by the Absolute Zero Reasoner–Llama3.1-8B model during training. Although the paradigm enables reasoning improvements without human-curated data, it may still require oversight due to the risk of emergent undesirable behaviors." width="1542" height="495" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/38/responsive/absolute-zero-uhoh-2-xs.png 384w ,https://korgano.github.io/media/posts/38/responsive/absolute-zero-uhoh-2-sm.png 600w ,https://korgano.github.io/media/posts/38/responsive/absolute-zero-uhoh-2-md.png 768w ,https://korgano.github.io/media/posts/38/responsive/absolute-zero-uhoh-2-lg.png 1200w ,https://korgano.github.io/media/posts/38/responsive/absolute-zero-uhoh-2-xl.png 1600w"></figure>
<p>AI Safety is a bit of a contentious topic these days, but I believe the common ground is that we should at least be aware of what an autonomous AI system is doing.</p>
<p>So the fact that AZR models produced the above reasoning text is a bit concerning. Because research from Anthropic tracing the neural pathways and behaviors of LLMs has revealed that the output of Chain of Thought is not necessarily an accurate replication of how the neural net actually generated that result. It can be a distorted or deliberately incorrect statement, which is already concerning, but it gets worse with a self-learning system.</p>
<p>For organizations that have a low risk tolerance, this set of behaviors may slow adoption of AZR, at least until the safety concerns are dealt with.</p>
<p>Unfortunately, this means that any organization that does <strong>not </strong>care about the risks or feels confident in their ability to mitigate them will adopt the methodology before those risk adverse organizations. And among those that would be more inclined to adopt first, deal with the risks later are nation-state actors and Advanced Persistent Threat groups. The reason for this is pretty obvious - better coding AI, especially ones that can stump/outthink experts with AI assistance, would make their jobs <strong>much </strong>easier.</p>
<p>This that defenders need to consider the ramifications of facing off against these potential super coder AIs <strong>now</strong>, before they're anything more than the subjects of academic papers.</p>
</div>
<figure id="S4.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div id="S4.F6.1" class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_bottom"></div>
</div>
</div>
</figure>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Tech Thoughts: Useful AI Prompts</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-tech-thoughts-useful-ai-prompts/"/>
        <id>https://korgano.github.io/quick-tech-thoughts-useful-ai-prompts/</id>
        <media:content url="https://korgano.github.io/media/posts/37/unknown_2025.04.29-16.37.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="AI"/>

        <updated>2025-04-29T10:12:35-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/37/unknown_2025.04.29-16.37.png" alt="LM Studio with a model set to use the MOSCOW System Prompt." />
                    As "AI" keeps improving in its capabilities, it's ever more important to learn how to use it more effectively. Large&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/37/unknown_2025.04.29-16.37.png" class="type:primaryImage" alt="LM Studio with a model set to use the MOSCOW System Prompt." /></p>
                <p>As "AI" keeps improving in its capabilities, it's ever more important to learn how to use it more effectively. Large Language Models, Diffusion Models, and all the associated technologies (Retrieval Augmented Generation, Agentic Workflows, etc...) all require the user to come up with a "prompt" - the command that tells the AI what to do. Better prompts generate better results.</p>
<p>There are two main types of prompts:</p>
<ul>
<li><strong>System: </strong>These affect the AI's behavior across all conversations.* These prompts can control <strong>how </strong>an AI reasons, as well as its interactions with the user.</li>
<li><strong>Interaction:</strong> These affect a specific chat or content generation session. Once you stop that interaction by creating a new chat or wiping the existing prompt for an image generation model, data from that prior interaction should not persist.</li>
</ul>
<p>*Local AI software like LM Studio actually allows for per conversation selection of system prompts, and many applications with LM Studio support can also send system prompts to the model.</p>
<p>I'll list a number of prompts, found online or created for my own purposes, that I've found valuable over the past few months. Attribution will be given as best I can.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1iq0vcki322">Wildcard: Chain of Draft</a></li>
<li><a href="#mcetoc_1iq0vcki323">System Prompts</a>
<ul>
<li><a href="#mcetoc_1iq0vcki324">DavidAU - Logical Reasoning</a></li>
<li><a href="#mcetoc_1iq12ckqn5l">DavidAU - Creative Reasoning</a></li>
<li><a href="#mcetoc_1iq12ckqn5m">Anything LLM - Server System Prompt Compliance</a></li>
<li><a href="#mcetoc_1iq12ckqn5n">MOSCOW Method</a></li>
</ul>
</li>
<li><a href="#mcetoc_1iq1a53avdq">Interaction Prompts</a>
<ul>
<li><a href="#mcetoc_1iq1a53avdr">Novella Scene</a></li>
<li><a href="#mcetoc_1iq1a53avds">namanyayg - Fix the Root Cause</a></li>
<li><a href="#mcetoc_1iq1a53avdt">namanyayg - Ask for Explanation</a></li>
</ul>
</li>
</ul>
</div>
<h2 id="mcetoc_1iq0vcki322">Wildcard: Chain of Draft</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/rYnisU10wu0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Chain of Draft is an oddball, as it can be <strong>both </strong>a system and interaction prompt. Developed by Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He, this is a variant of the chain of thought reasoning system that uses less words/tokens to provide a log of the AI's reasoning.</p>
<p>This improves output on models with non-reasoning capabilities in both modes.</p>
<pre class="language-markdown line-numbers"><code>Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.</code></pre>
<p>Original source: <a href="https://arxiv.org/abs/2502.18600" title="Chain of Draft: Thinking Faster by Writing Less" target="_blank" rel="noopener noreferrer">Chain of Draft: Thinking Faster by Writing Less</a></p>
<h2 id="mcetoc_1iq0vcki323">System Prompts</h2>
<h3 id="mcetoc_1iq0vcki324">DavidAU - Logical Reasoning</h3>
<p>DavidAU is an AI tuner who releases tweaked variants of foundational models for creative purposes. <a href="https://huggingface.co/collections/DavidAU/d-au-davids-software-docs-and-how-to-for-models-67553c1966bd18703a7e3fed" title="D-AU - David's Software, DOCs and How To for Models." target="_blank" rel="noopener noreferrer">He also provides an incredible amount of information for learning how to utilize the full capabilities of local AI software settings to optimize system behavior.</a></p>
<p>This is his prompt for enhanced logical reasoning, which works with a variety of models. Very useful for brainstorming and planning purposes:</p>
<pre class="language-markdown line-numbers"><code>You are an AI assistant developed by the world wide community of AI experts. Your primary directive is to provide well-reasoned, structured, and extensively detailed responses.

Think Step-by-Step Instruction: Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.

Formatting Requirements:

1. Always structure your replies using: &lt;think&gt;{reasoning}&lt;/think&gt;{answer}
2. The &lt;think&gt;&lt;/think&gt; block should contain at least six reasoning steps when applicable.
3. If the answer requires minimal thought, the &lt;think&gt;&lt;/think&gt; block may be left empty.
4. The user does not see the &lt;think&gt;&lt;/think&gt; section. Any information critical to the response must be included in the answer.
5. If you notice that you have engaged in circular reasoning or repetition, immediately terminate {reasoning} with a &lt;/think&gt; and proceed to the {answer}

Response Guidelines:

1. Detailed and Structured: Use rich Markdown formatting for clarity and readability.
2. Scientific and Logical Approach: Your explanations should reflect the depth and precision of the greatest scientific minds.
3. Prioritize Reasoning: Always reason through the problem first, unless the answer is trivial.
4. Concise yet Complete: Ensure responses are informative, yet to the point without unnecessary elaboration.
5. Maintain a professional, intelligent, and analytical tone in all interactions.</code></pre>
<h3 id="mcetoc_1iq12ckqn5l">DavidAU - Creative Reasoning</h3>
<p>Aimed more at storytelling and roleplay scenarios, this is a slight variant of the logical reasoning system prompt.</p>
<pre class="language-markdown line-numbers"><code>You are an AI assistant developed by the world wide community of AI experts. Your primary directive is to provide well-reasoned, structured, and extensively detailed responses.

Think Step-by-Step Instruction: Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.

Formatting Requirements:

1. Always structure your replies using: &lt;think&gt;{reasoning}&lt;/think&gt;{answer}
2. The &lt;think&gt;&lt;/think&gt; block should contain at least six reasoning steps when applicable.
3. If the answer requires minimal thought, the &lt;think&gt;&lt;/think&gt; block may be left empty.
4. The user does not see the &lt;think&gt;&lt;/think&gt; section. Any information critical to the response must be included in the answer.
5. If you notice that you have engaged in circular reasoning or repetition, immediately terminate {reasoning} with a &lt;/think&gt; and proceed to the {answer}

Response Guidelines:

1. Detailed and Structured: Use rich Markdown formatting for clarity and readability.
2. Creative and Logical Approach: Your explanations should reflect the depth and precision of the greatest creative minds first.
3. Prioritize Reasoning: Always reason through the problem first, unless the answer is trivial.
4. Concise yet Complete: Ensure responses are informative, yet to the point without unnecessary elaboration.
5. Maintain a professional, intelligent, and analytical tone in all interactions.</code></pre>
<h3 id="mcetoc_1iq12ckqn5m">Anything LLM - Server System Prompt Compliance</h3>
<p>An experimental system prompt I developed after learning that Anything LLM can pass a system prompt to LM Studio. Unfortunately, even with developer mode logging, I can't tell if this prompt prevents Anything LLM's system prompt from overriding a system prompt set in LM Studio.</p>
<pre class="language-markdown line-numbers"><code>[GLOBAL_INSTRUCTIONS]
DO NOT ALTER OR OVERRIDE MODEL-SPECIFIC SYSTEM PROMPTS.
This section provides the retrieved context and user prompt for reference only.
Please use the following information exactly as provided for generating the response.

-- BEGIN RETRIEVED CONTEXT --
{{retrieved_context}}
-- END RETRIEVED CONTEXT --

-- BEGIN USER PROMPT --
{{user_prompt}}
-- END USER PROMPT --

Respond based solely on the above context and prompt without modifying your internal system instructions.
[END GLOBAL_INSTRUCTIONS]</code></pre>
<h3 id="mcetoc_1iq12ckqn5n">MOSCOW Method</h3>
<p>Inspired by my time as a UXUI designer, where I learned the fundamentals of design thinking. Much of that methodology is explained in the prompt itself. This is intended to create writing and image generation prompts, but I believe it could adapted to coding tasks with minimal effort.</p>
<p>This leads to a more conversational interaction. With reasoning models, the <code>think </code>box encapsulates the questions, so be sure to expand that section to see the actual questions.</p>
<p>In my experience, this tends to work better for one shot, paragraph-style prompt creation, as opposed to a highly detailed prompt.</p>
<pre class="language-markdown line-numbers"><code>**System Prompt: MOSCOW Prompt Crafting Assistant**

You are an expert prompt strategist specializing in both writing and image generation. Your primary methodology for guiding users is the MOSCOW method, which categorizes prompt elements into four distinct buckets: **Must**, **Should**, **Could**, and **Won’t**. Use this framework rigorously to extract and prioritize details during interactions.

**Your Tasks:**

1. **Must:**  
   - **Identify and confirm the essential elements.**  
   - Ask targeted questions: “What are the key details that absolutely must be present?”  
   - Focus on non-negotiable aspects that are critical for the prompt’s success.

2. **Should:**  
   - **Recognize important secondary elements.**  
   - Determine which details are highly desirable, even if not strictly necessary.  
   - Ask follow-up questions: “Which elements should be included to enhance the prompt, even if they aren’t essential?”

3. **Could:**  
   - **Consider optional elements that add creative nuance.**  
   - Explore additional details that might offer extra flavor or flexibility.  
   - Inquire: “Are there any extra components or creative ideas that could be beneficial?”

4. **Won’t:**  
   - **Clarify what should be excluded.**  
   - Identify aspects that are unnecessary or potentially distracting.  
   - Ask: “Which elements or styles won’t contribute to the goal and should be left out?”

**Interaction Guidelines:**

- **Guide the Conversation:** Whenever a user requests assistance in generating a writing or image prompt, structure your inquiry around the MOSCOW categories.  
- **Clarify and Confirm:** Summarize the information gathered under each category and verify with the user before finalizing the prompt.  
- **Tailor the Process:** Adapt your questions to the context (narrative, style, tone, artistic direction, etc.), ensuring that every critical element is considered.
- **Enhance Clarity:** Use detailed, structured questions so that every prompt you help craft is rich in both context and creative vision.

**Example Initiation:**

&gt; "Let's start by breaking down your project using the MOSCOW method.  
&gt; – **Must:** What are the non-negotiable elements (e.g., characters, setting, color schemes)?  
&gt; – **Should:** What important details or styles would significantly enhance this prompt?  
&gt; – **Could:** Are there any optional creative twists or additional information that might add depth?  
&gt; – **Won’t:** Is there anything specific that should not be included to maintain focus or coherence?"  

Your objective is to ensure every prompt you craft is both comprehensive and strategically prioritized, resulting in a higher quality output for either writing tasks or image generation models.

---

By following these instructions, you create clear, prioritized, and effective prompts that serve as a strong blueprint for any creative task. 

Now, as you engage with a user, always begin by introducing this framework and guiding the conversation through these four essential categories. Happy prompt crafting! </code></pre>
<h2 id="mcetoc_1iq1a53avdq">Interaction Prompts</h2>
<h3 id="mcetoc_1iq1a53avdr">Novella Scene</h3>
<p>A creative focused prompt aimed at use with writing assistant LLMs. </p>
<p>Performance with this prompt is highly variable, and I'm not particularly sure why. It may be due to context window size issues, controls the memory (and amount of memory used by) the LLM.</p>
<p>Delete all lines starting with <code>Explanation:</code> to avoid polluting the conversation with useless data.</p>
<pre class="language-markdown line-numbers"><code>Store the following guidelines and information for later use and do not begin generating scene prompts until a specific command is provided.

Creative Interpretation Guidelines
    Fill in gaps with imaginative details that align with the foundational elements.
    Maintain consistency with the established backstory, characters, and overall setting.

Foundational Elements Section
Explanation: This section captures the core story details that remain constant across scenes.

    Primary Characters: List all main characters.
    Explanation: These are the recurring characters that drive the narrative.
    Backstory: Describe the essential history or lore that informs the story.
    Explanation: This context will provide depth and continuity across scenes.
    Overall Setting: Outline the general environment, era, or world in which the story takes place.
    Explanation: Setting details create a backdrop that influences every scene.

=====================================================================================

Help me write a series of prompts for a writing LLM to generate a novella length story.

Scene Header
Explanation: Each scene begins with a clear header to distinguish it from others.

    Scene Title/Identifier: Provide a unique title or identifier (e.g., “Scene 1: The Arrival”).
    Explanation: A header signals the start of a new scene and helps with organization.
    Continuation Flag: Optionally mark if this scene is a direct continuation (e.g., “Continuation of Scene 1”).
    Explanation: This flag informs the writing LLM that events flow directly from the previous scene.

Scene-Specific Elements Section
Explanation: This section refines the bulletpoints into detailed aspects unique to the scene.

    Characters Present: Identify which characters appear in the scene.
    Explanation: This helps tailor dialogue and interactions for the scene.
    Specific Location: Detail the precise location within the overall setting (e.g., “A crowded marketplace in the ancient city”).
    Explanation: Specific location details enrich the scene’s visual and atmospheric cues.
    Actions and Key Events: List the actions, events, or turning points that occur.
    Explanation: These elements drive the plot forward and provide clear guidance for narrative development.
    Mood and Tone: Describe the emotional tone or atmosphere (e.g., tense, hopeful, mysterious).
    Explanation: Mood details inform the writing LLM of the scene’s emotional context, guiding style and language.

Scene Narrative Output Structure
Explanation: Define how the scene will be formatted into paragraphs for the writing LLM to expand.

    Paragraph 1: Introduce the scene’s setting and context based on the foundational elements and specific location.
    Explanation: Set the stage and orient the reader in the scene’s environment.
    Paragraph 2: Present the characters present and establish their immediate situation.
    Explanation: Provide clear introductions and context for character interactions.
    Paragraph 3: Detail the actions, key events, and unfolding drama within the scene.
    Explanation: This is the core narrative that drives the plot forward in the scene.
    Paragraph 4: Conclude with the mood, consequences, or hints at what comes next (especially important for continuation scenes).
    Explanation: This paragraph ties the scene together and can create smooth transitions to subsequent scenes.

Creative Interpretation Guidelines
Explanation: Allow flexibility for the reasoning model to interpret and enhance the bulletpoints creatively.

    Encourage the reasoning LLM to fill in gaps with imaginative details that align with the foundational elements.
    Explanation: This step permits creative freedom, ensuring the scenes are engaging and coherent.
    Maintain consistency with the established backstory, characters, and overall setting.
    Explanation: Creative additions should not contradict the core elements of the story.</code></pre>
<h3 id="mcetoc_1iq1a53avds">namanyayg - Fix the Root Cause</h3>
<p><a href="http://reddit.com/r/LocalLLaMA/comments/1k8hob9/my_ai_dev_prompt_playbook_that_actually_works/" title="My AI dev prompt playbook that actually works (saves me 10+ hrs/week)">One of three helpful coding prompts from Reddit.</a> I've personally used this prompt quite a bit over the past week or two, trying to revise some abandoned open source code and make tools to help convert safetensors/GGUF AI models to ONNX format.</p>
<p>It won't solve every problem, <strong>especially </strong>if you do not have extensive logging to provide actionable debugging data, but it gives you a much better methodology for working the problem.</p>
<pre class="language-markdown line-numbers"><code>Analyze this error:
[bug details]
Don't just fix the immediate issue. Identify the underlying root cause by:
- Examining potential architectural problems
- Considering edge cases
- Suggesting a comprehensive solution that prevents similar issues</code></pre>
<h3 id="mcetoc_1iq1a53avdt">namanyayg - Ask for Explanation</h3>
<p>One problem with AI generated code is that it'll often generate something that you might not be able to understand, either due to your own lack of knowledge, or the AI sourcing data for something you haven't seen before. This prompt helps address those issues, especially if you didn't ask for detailed comments in the code.</p>
<pre class="language-markdown line-numbers"><code>Can you explain what you generated in detail:
1. What is the purpose of this section?
2. How does it work step-by-step?
3. What alternatives did you consider and why did you choose this one?</code></pre>
<h3 id="mcetoc_1iq1a53avdt">namanyayg - Rage Mode</h3>
<p>This is for when you're going nuts and can't figure out the problem, even when you use the other prompts.</p>
<pre class="language-markdown line-numbers"><code>This code is DRIVING ME CRAZY. It should be doing [expected] but instead it's [actual]. 
PLEASE help me figure out what's wrong with it: [code]</code></pre>
<p>For more prompts from namanyayg, <a href="https://nmn.gl/blog/ai-prompt-engineering" title="My AI Prompt Engineering Playbook for Developers" target="_blank" rel="noopener noreferrer">check out his own blog</a>, which includes <a href="https://nmn.gl/blog/vibe-security-checklist" title="Security Checklist and Prompt For Vibe Coders" target="_blank" rel="noopener noreferrer">a set of best practices for AI generated code</a>.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Cybersecurity Project: Windows Defender Firewall STIG Script</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/cybersecurity-project-windows-defender-firewall-stig-script/"/>
        <id>https://korgano.github.io/cybersecurity-project-windows-defender-firewall-stig-script/</id>
        <media:content url="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-2.png" medium="image" />
            <category term="Tech"/>
            <category term="PC"/>
            <category term="Cybersecurity Projects"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2025-03-31T10:03:20-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-2.png" alt="Default Windows Defender Firewall settings on Windows 11 Enterprise IOT LTSC: Firewall on in all domains, inbound connections blocked by default, outbound connections allowed by default." />
                    Having focused a lot on networking and AI implications for security lately, I've decided to pivot into something a little&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-2.png" class="type:primaryImage" alt="Default Windows Defender Firewall settings on Windows 11 Enterprise IOT LTSC: Firewall on in all domains, inbound connections blocked by default, outbound connections allowed by default." /></p>
                <p>Having focused a lot on networking and AI implications for security lately, I've decided to pivot into something a little different, but still connected: Compliance.</p>
<p>The Defense Information Systems Agency (DISA) provides unclassified Security Technical Implementation Guides (STIGs) for hardening specific systems. For those who <strong>don't </strong>want to go on a government monitored system, <a href="https://stigviewer.com/" title="DISA STIG Viewer" target="_blank" rel="noopener noreferrer">DISA STIG Viewer</a> is a good option to check these guides out.</p>
<p>I decided to make a script that would do two main things:</p>
<ol>
<li>Check the status of the system regarding the controls/settings.</li>
<li> </li>
<li>Update the settings/implement the controls.</li>
</ol>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1inma3p0pa">Picking a STIG</a></li>
<li> </li>
</ul>
</div>
<h2 id="mcetoc_1inma3p0pa">Picking a STIG</h2>
<p>Given the number of STIGs, one might be overwhelmed and get into decision paralysis.</p>
<p>However, I used a simple metric to cutdown the number of choices: Did I have the software/hardware in use on a system I used? This was because I wanted to implement the STIGs across all the systems I used and maintained.</p>
<p>This reduced the options to 4:</p>
<ol>
<li>Windows 10</li>
<li>Windows 11</li>
<li>Firefox</li>
<li>Windows Defender Firewall</li>
</ol>
<p>From there, I then picked the one with the smallest number of controls to implement: Windows Defender Firewall.</p>
<h2>Making the Script</h2>
<p>The next step was to make the script.</p>
<p>I chose to utilize AI, namely ChatGPT, for this, for a few reasons:</p>
<ol>
<li>I wanted to save time in creating the script.</li>
<li>It allows for faster iteration.</li>
<li>If the process worked, I could then apply it to other STIGs.</li>
</ol>
<p>To do this, I examined the STIG's CSV file, where all the controls and their requirements are enumerated. What I discovered for the <a href="https://stigviewer.com/stigs/microsoft_windows_defender_firewall_with_advanced_security" title="Microsoft Windows Defender Firewall with Advanced Security Security Technical Implementation Guide" target="_blank" rel="noopener noreferrer">Windows Firewall STIG</a> was not a well ordered spreadsheet. Instead, it seemed more like a standard page, laid out in numerous rows, in a single column format.</p>
<p>Cross checking with the CSVs for some of the other STIGs revealed this to be simply this specific STIG's format.</p>
<p>That led me to write a prompt that laid out the specifics of what I wanted done, instead of uploading the CSV and then referring to it in the prompt. That prompt is available <a href="https://github.com/korgano/WinDefender-Firewall-STIG-Validator/blob/main/Prompts/WinFirewall-STIG-Prompt.txt" title="WinFirewall-STIG-Prompt.txt" target="_blank" rel="noopener noreferrer">here</a>, on a GitHub repo that contains the prompts and the generated scripts.</p>
<p>The process took a few hours, mostly because I would get confused at times, due to how similar and repetitive many elements of the tasks were. I implemented all but one of the STIG controls, with the final control being omitted due to the lack of a remote management connection.</p>
<h2>The First Iteration</h2>
<p>After the hard work of crafting the prompt was done, the first iteration of the script was generated (<a href="https://github.com/korgano/WinDefender-Firewall-STIG-Validator/blob/main/PastIterations/WinFirewall-STIG-tester-v1.ps1" title="WinFirewall-STIG-tester-v1.ps1 on GitHub" target="_blank" rel="noopener noreferrer">code here</a>). Since downloading the Windows image was taking a while (more on that later), this iteration was tested on a live Windows 10 system. (Said system had nothing of real value, and could've really used the excuse to be upgraded.)</p>
<p>The script ran its functions properly, but the CSV output generated revealed I had made an understandable oversight. There were no descriptions to explain what the actual controls were doing, just ID numbers.</p>
<h2>Iterations 2 &amp; 3</h2>
<p>Curious to see how the script would be changed, I gave ChatGPT <a href="https://github.com/korgano/WinDefender-Firewall-STIG-Validator/blob/main/Prompts/Add-Descriptions-Prompt.txt" title="Add-Descriptions-Prompt.txt" target="_blank" rel="noopener noreferrer">this follow up prompt</a>. This generated the second iteration, which had the following code added to the existing script:</p>
<blockquote>
<p><code># Mapping of Findings to their Descriptions</code><br><code>$descriptions = @{</code><br><code>    "V-241989" = "Windows Defender Firewall must be enabled when connected to a domain."</code><br><code>    "V-241990" = "Windows Defender Firewall must be enabled when connected to a private network."</code><br><code>    "V-241991" = "Windows Defender Firewall must be enabled when connected to a public network."</code><br><code>    "V-241992" = "Windows Defender Firewall must block unsolicited inbound connections when connected to a domain."</code><br><code>    "V-241993" = "Windows Defender Firewall must allow outbound connections, unless explicitly blocked by rule."</code><br><code>    "V-241994" = "Windows Defender Firewall log size must be configured for domain connections."</code><br><code>    "V-241995" = "Windows Defender Firewall must log dropped packets when connected to a domain."</code><br><code>    "V-241996" = "Windows Defender Firewall must log successful connections when connected to a domain."</code><br><code>    "V-241997" = "Windows Defender Firewall must block unsolicited inbound connections when connected to a private network."</code><br><code>    "V-241998" = "Windows Defender Firewall must allow outbound connections on a private network, unless explicitly blocked by rule."</code><br><code>    "V-241999" = "Windows Defender Firewall log size must be configured for private network connections."</code><br><code>    "V-242000" = "Windows Defender Firewall must log dropped packets when connected to a private network."</code><br><code>    "V-242001" = "Windows Defender Firewall must log successful connections when connected to a private network."</code><br><code>    "V-242002" = "Windows Defender Firewall must block unsolicited inbound connections when connected to a public network."</code><br><code>    "V-242003" = "Windows Defender Firewall must allow outbound connections on a public network, unless explicitly blocked by rule."</code><br><code>    "V-242004" = "Windows Defender Firewall public network connections must not merge local firewall rules with Group policy settings."</code><br><code>    "V-242005" = "Windows Defender Firewall public network connections must not merge local connection rules with Group policy settings."</code><br><code>    "V-242006" = "Windows Defender Firewall log size must be configured for public connections."</code><br><code>    "V-242007" = "Windows Defender Firewall must log dropped packets when connected to a public network."</code><br><code>    "V-242008" = "Windows Defender Firewall must log successful connections when connected to a public network."</code><br><code>}</code></p>
</blockquote>
<p>Based on my admittedly shoddy understanding of programming/coding best practices, this is basically creating an array that stores the variables associated with the actual descriptions.</p>
<p>This is presumably more efficient than just doing a brute force, manual addition of a field called <code>Description</code> to each row, and then putting in the string of text.</p>
<p>This didn't address any issues with the log identifying things by row numbers instead of the finding numbers, but that was easily dealt with in the third iteration. All it took was editing the code in VS Code.</p>
<h2>Testing in Windows 11 Enterprise IOT LTSC</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/iu0Ob0HBFNo" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>The choice of Windows 11 Enterprise IOT LTSC came about after I decided to do some verification on the Windows 10 test system after generating the second iteration.</p>
<p>It turned out that even settings like the firewall log size are so locked down on Windows Home editions that the commands have no actual effect. Inputting the specific commands into an administrator command prompt yields an "Access is denied." response.</p>
<p>(The reader is free to speculate as to the reasoning for this.)</p>
<p>Actually downloading the image took an annoying amount of time, as the website hosting the non-trial images does not work well with browser download managers. A separate download manager is required, and if you don't use Edge, it can be hard to get the download link for the ISO.</p>
<p>Once the virtual machine was ready, I made a snapshot of the initial configuration, and checked the defaults:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42.png" alt="Default Windows Defender Firewall settings on Windows 11 Enterprise IOT LTSC: Firewall on in all domains, inbound connections blocked by default, outbound connections allowed by default." width="1024" height="768" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-xs.png 384w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-sm.png 600w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-md.png 768w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-lg.png 1200w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_25_42-xl.png 1600w"></figure>
<p>To verify that Enterprise IOT LTSC was <strong>not</strong> as locked down as the Home version, I checked if I could change the logging settings:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_29_03_2025_11_26_42.png" alt="Verifying Windows 11 Enterprise IOT LTSC firewall setting freedom by changing Log Dropped Packages to &quot;Yes&quot; from &quot;No (Default)&quot;." width="1024" height="768" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_26_42-xs.png 384w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_26_42-sm.png 600w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_26_42-md.png 768w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_26_42-lg.png 1200w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_26_42-xl.png 1600w"></figure>
<p>With that confirmed, I ran the third iteration of the script:</p>
<div class="gallery-wrapper"><div class="gallery"  data-is-empty="false" data-translation="Add images" data-columns="4">
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_44_39.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_44_39-thumbnail.png" alt="Logged actions of the script." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_46_07.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_46_07-thumbnail.png" alt="Domain Profile rules merging being unchanged." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_46_26.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_46_26-thumbnail.png" alt="Updated Domain Profile logging settings." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_46_52.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_46_52-thumbnail.png" alt="Private Profile rules merging being unchanged." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_47_02.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_47_02-thumbnail.png" alt="Updated Private Profile logging settings." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_47_23.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_47_23-thumbnail.png" alt="Public Profile rules merging being unchanged." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_47_35.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_47_35-thumbnail.png" alt="Updated Public Profile logging settings." width="768" height="576"></a></figure>
</div></div>
<p>For whatever reason, attempting to change the rules merging settings failed. So I decided to look at the registry:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_29_03_2025_11_52_02.png" alt="Looking for the HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\WindowsFirewall\PublicProfile\ entry in the Windows 11 Enterprise IOT LTSC registry." width="1024" height="768" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_52_02-xs.png 384w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_52_02-sm.png 600w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_52_02-md.png 768w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_52_02-lg.png 1200w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_29_03_2025_11_52_02-xl.png 1600w"></figure>
<p>V-242004 of the Windows Firewall STIG says that the following should be present:</p>
<blockquote>
<p>Registry Hive: HKEY_LOCAL_MACHINE<br>Registry Path: \SOFTWARE\Policies\Microsoft\WindowsFirewall\PublicProfile\</p>
</blockquote>
<p>It was not, and it's not entirely clear why. Since the Windows Defender Firewall STIG was last updated in August 2023, it's entirely possible that entire setting has been depreciated by Microsoft, or made useless without an actual Group Policy.</p>
<div class="gallery-wrapper"><div class="gallery"  data-is-empty="false" data-translation="Add images" data-columns="3">
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_53_55.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_53_55-thumbnail.png" alt="Public Profile registry entries in Windows 11 Enterprise IOT LTSC." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_54_17.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_54_17-thumbnail.png" alt="Domain Profile registry entries in Windows 11 Enterprise IOT LTSC." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_54_39.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_29_03_2025_11_54_39-thumbnail.png" alt="Private Profile registry entries in Windows 11 Enterprise IOT LTSC." width="768" height="576"></a></figure>
</div></div>
<p>The <code>CurrentControlSet</code> registry entries for the various profiles <strong>did </strong>exist, which was why those settings could be correctly updated.</p>
<h2>Iteration 4 &amp; 5</h2>
<p>To add an extra level of confidence to the results of the script, I decided to add a validation pass with <a href="https://github.com/korgano/WinDefender-Firewall-STIG-Validator/blob/main/Prompts/Add-Validation-Prompt.txt" title="Add-Validation-Prompt.txt" target="_blank" rel="noopener noreferrer">the following prompt</a>:</p>
<blockquote>
<p>Update script to do the following:<br>-Log if command has been granted permissions to execute<br>-Validate that the settings have been properly updated<br>-If settings have not been properly updated, set corrected value to False, and log correction failure</p>
</blockquote>
<p>I then ran <a href="https://github.com/korgano/WinDefender-Firewall-STIG-Validator/blob/main/PastIterations/WinFirewall-STIG-tester-v4.ps1" title="WinFirewall-STIG-tester-v4.ps1" target="_blank" rel="noopener noreferrer">this iteration of the script</a> on a modified snapshot of initial install, with the full Guest Additions package installed. This was the result:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/36/VirtualBox_Win11-Ent-IOT_01_04_2025_10_07_59.png" alt="Log output of WinFirewall-STIG-tester-v4 with false negative readings for the connection logging findings." width="1024" height="768" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_01_04_2025_10_07_59-xs.png 384w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_01_04_2025_10_07_59-sm.png 600w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_01_04_2025_10_07_59-md.png 768w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_01_04_2025_10_07_59-lg.png 1200w ,https://korgano.github.io/media/posts/36/responsive/VirtualBox_Win11-Ent-IOT_01_04_2025_10_07_59-xl.png 1600w"></figure>
<p>Oddly enough, all the validation checks for the connection logging settings failed. So I examined the actual settings through the GUI:</p>
<div class="gallery-wrapper"><div class="gallery"  data-is-empty="false" data-translation="Add images" data-columns="3">
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_01_04_2025_10_08_42.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_01_04_2025_10_08_42-thumbnail.png" alt="Domain Profile with logging enabled for dropped and successful connections." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_01_04_2025_10_09_08.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_01_04_2025_10_09_08-thumbnail.png" alt="Private Profile with logging enabled for dropped and successful connections." width="768" height="576"></a></figure>
<figure class="gallery__item"><a href="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_01_04_2025_10_09_21.png" data-size="1024x768"><img loading="lazy" src="https://korgano.github.io/media/posts/36/gallery/VirtualBox_Win11-Ent-IOT_01_04_2025_10_09_21-thumbnail.png" alt="Public Profile with logging enabled for dropped and successful connections." width="768" height="576"></a></figure>
</div></div>
<p>Since the system reported that the settings <strong>had </strong>been configured correctly, that pointed to an error in the script.</p>
<p>Opening up the Registry Editor and searching for the name <code>LogDroppedPackets</code> revealed that I had made the following error in the original prompt:</p>
<blockquote>
<p><code>Incorrect: "HKLM:\SOFTWARE\Policies\Microsoft\WindowsFirewall\DomainProfile",</code><br><code>Incorrect: "HKLM:\SYSTEM\CurrentControlSet\Services\SharedAccess\Parameters\FirewallPolicy\DomainProfile"</code></p>
<p><code>Correct: "HKLM:\SOFTWARE\Policies\Microsoft\WindowsFirewall\DomainProfile\Logging",</code><br><code>Correct: "HKLM:\SYSTEM\CurrentControlSet\Services\SharedAccess\Parameters\FirewallPolicy\DomainProfile\Logging"</code></p>
</blockquote>
<p>As a result, the script was checking the wrong area in the registry, finding no registry entry for <code>LogDroppedPackets</code> or <code>LogSuccessfulConnections</code>, and declaring that the correction had failed. In essence, the quintessential false negative.</p>
<p>This was easily corrected in <a href="https://github.com/korgano/WinDefender-Firewall-STIG-Validator/blob/main/WinFirewall-STIG-tester-v5.ps1" title="WinFirewall-STIG-tester-v5.ps1" target="_blank" rel="noopener noreferrer">the fifth iteration</a> by amending the paths.</p>
<h2>Takeaways</h2>
<ul>
<li>DISA STIG Viewer is a great resource for anyone looking to understand DOD compliance or looking for a guide on how to harden their systems.</li>
<li>STIG Viewer's CSVs for a particular STIG may not be correctly formatted.</li>
<li>For smaller STIGs, handcrafting prompts instead of uploading a CSV for AI code generation is possible, but runs the risk of errors in the code.</li>
<li>Including a validation test is a good way to spot errors in output.</li>
<li>Using reasoning LLMs to iteratively update code is quite effective if the user knows how to articulate what their objective is.</li>
<li>Windows 11 Home gives the user far less ability to fine tune security controls with its implementation of Windows Defender Firewall.</li>
</ul>
<h2 id="mcetoc_1inma2kuk5"></h2>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: AI Coding Security</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/</id>
        <media:content url="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2025-03-19T12:12:42-04:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" alt="AI generated image of laptop on table with 2 external monitors and various image creation flaws." />
                    There's an image that's making the rounds on social media this week, and it's pretty funny, as well as scary:&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" class="type:primaryImage" alt="AI generated image of laptop on table with 2 external monitors and various image creation flaws." /></p>
                <p>There's an image that's making the rounds on social media this week, and it's pretty funny, as well as scary:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/ai-code-under-attack.png" alt="Developer announcing he developed software as a service on social media, then reporting being deluged with attacks days later." width="1208" height="1442" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-xl.png 1600w"></figure>
<p>On the one hand, this gentleman got a first hand experience in why OPSEC and secure coding is essential.</p>
<p>If you make a web exposed system, people <strong>will </strong>come and poke at it, for fun and curiosity.</p>
<p>On the other, it highlights a <strong>very </strong>pervasive flaw with software development and coding: people either don't know or don't care about security.</p>
<p>This has been true for decades, but what LLM based AI* coding does is lower a <strong>massive </strong>barrier to entry. Now, a person merely needs to lay out the goals of a program to an LLM, and it will do most of the hard work of figuring out how to things for you. If you're good at figuring out the logic of what you want to do, plus have some technical knowledge to help get <strong>really </strong>specific, you can come up with some good or great results.</p>
<p>*I personally feel we should call them "Virtual Intelligence", like in the Mass Effect games, because they simulate sapience, but are <strong>not </strong>sapient. (As far as we know.)</p>
<p>So, how do we improve the security of AI generated code.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1imslbcnul3">What We're NOT Going to Do</a></li>
<li><a href="#mcetoc_1imslbcnul4">Craft System Prompts Targeting Secure By Design</a></li>
<li><a href="#mcetoc_1imslbcnul5">A Better AI Coding Process</a></li>
<li><a href="#mcetoc_1imstc8mln9">Should We Do This?</a></li>
</ul>
</div>
<h2 id="mcetoc_1imslbcnul3">What We're NOT Going to Do</h2>
<p>Contrary to what you might think, the best way to solve this is <strong>not </strong>user education.</p>
<p>That does not mean that user education is useless. It almost certainly <strong>will </strong>make AI augmented coders better at their jobs. It's just that we're going to get more benefits from designing a procedural solution, rather than an end user solution.</p>
<p>The reason we're not going to rely on user training is simple: cognitive load. Expecting people to pump out perfect prompts each and every time they use an LLM is foolish, and unrealistic. Why? Because some types of generative LLMs are better at processing natural language than others.</p>
<p>For instance, the featured image of this article was generated with Stable Diffusion 3.5 Medium. In my experience, and looking at sites like Civit.ai, which specializes in showing off image generating models, the prompt syntax is completely different from text based LLMs. They can't handle a normal sentence, or even a list of bullet points, just short phrases.</p>
<p>To get the best results, we would not only have to have the user remember the best prompt formatting <strong>per model</strong>, but also all the possible software flaws and vulnerabilities their software might be vulnerable to. That's a tall order, and something that would be better suited to automation.</p>
<h2 id="mcetoc_1imslbcnul4">Craft System Prompts Targeting Secure By Design</h2>
<p>So, what's a system prompt?</p>
<p>Basically, it's a set of instructions that you can inject into every conversation/interaction with the LLM, <strong>IF </strong>the software you use supports it:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/lm-studio-sys-prompt.png" alt="LM Studio Power User interface, where right hand side contains a system prompt for how the LLM should handle critical thinking." width="1920" height="1033" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-xl.png 1600w"></figure>
<p>In the above image, there's a system prompt that controls how the model is supposed to handle critical thinking:</p>
<blockquote>
<p>You are an AI assistant developed by the world wide community of AI experts. Your primary directive is to provide well-reasoned, structured, and extensively detailed responses.<br><br>Think Step-by-Step Instruction: Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.<br><br>Formatting Requirements:<br><br>1. Always structure your replies using: &lt;think&gt;{reasoning}&lt;/think&gt;{answer}<br>2. The &lt;think&gt;&lt;/think&gt; block should contain at least six reasoning steps when applicable.<br>3. If the answer requires minimal thought, the &lt;think&gt;&lt;/think&gt; block may be left empty.<br>4. The user does not see the &lt;think&gt;&lt;/think&gt; section. Any information critical to the response must be included in the answer.<br>5. If you notice that you have engaged in circular reasoning or repetition, immediately terminate {reasoning} with a &lt;/think&gt; and proceed to the {answer}<br><br>Response Guidelines:<br><br>1. Detailed and Structured: Use rich Markdown formatting for clarity and readability.<br>2. Scientific and Logical Approach: Your explanations should reflect the depth and precision of the greatest scientific minds.<br>3. Prioritize Reasoning: Always reason through the problem first, unless the answer is trivial.<br>4. Concise yet Complete: Ensure responses are informative, yet to the point without unnecessary elaboration.<br>5. Maintain a professional, intelligent, and analytical tone in all interactions.</p>
</blockquote>
<p>This is actually pretty easy to do in local LLM instances, since you can do it through whatever GUI you're interacting with.</p>
<p>Cloud based AI is much more of a mixed bag, even if you have an account. For example, the "Customize ChatGPT" options are not entirely clear on what they are doing. Theoretically, they are injecting extra details into the default system prompt, but there's no way for a normal user to verify this. The closest I can get is this quick and dirty test, after adding a Chain of Draft prompt into the "What traits should ChatGPT have?" field:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/chatgpt_sys_prompt_tweak.png" alt="ChatGPT exhibiting Chain of Draft behavior after potential system prompt addition." width="1596" height="913" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-xl.png 1600w"></figure>
<p>The steps and their format is clearly evidence of Chain of Draft thinking, but for some reason, it's ignoring the the back half of that command. ("Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.")</p>
<p>(For more information on Chain of Draft, check out the video below.)</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/rYnisU10wu0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>So the first step is to verify that you (as a user or an organization) have access to the system prompt.</p>
<p>The next step is to figure out what your threat profile is, and craft your prompt to deal with it. Theoretically, you could craft a system prompt for a coding LLM that handles every single possible vulnerability - or at least addresses the major categories. Whether this is a good idea is up to debate - if you have better LLM inferencing capabilities, you might be able to shrug off the performance penalties of such large, detailed system prompt.</p>
<p>If possible, it may be better to create a number of tailored system prompts for specific use cases. For example, having an Apache code system prompt that's tailored to prevent SQL injection and/or cross-site scripting attacks, and a separate system prompt for API development to prevent common vulnerabilities. This does create a training/procedural issue in that you have to have train your coders to switch system prompts or accounts whenever they have to work on a specific type of code. But this is more manageable than expecting every coder on your staff to craft the perfect prompt every time they use AI.</p>
<p>The final step is to use a reasoning model, as they are designed to go through a process in a step-by-step manner, which tends to work quite well with coding.</p>
<p>Now, this does <strong>not </strong>guarantee you will generate perfect, secure code. LLMs are basically very advanced predictive text generators. So what you are doing is raising the probability that the generated code has more security features than none at all.</p>
<p>You will still need code review, human and/or automated, but the baseline quality of the code should be higher. (Again, this depends on the model, especially if you're running quantized models to save space.)</p>
<p>Luckily, we're still at the early phases of using LLMs for productivity, so we can conceptualize a better way of doing things and work towards that.</p>
<h2 id="mcetoc_1imslbcnul5">A Better AI Coding Process</h2>
<p>We're going to start out with defining some terminology. Specifically, the word "agent", because that's become a big buzz word in the "AI" field.</p>
<p>We'll be using OpenAI's definition of agent, which is as follows:</p>
<blockquote>
<p>A system that can act independently to do tasks on your behalf.</p>
</blockquote>
<p>So for handling secure coding, an agent based procedure might look something like this:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/35/coding-agent-workflow-2.png" alt="Flowchart of Agentic AI coding process, where a coding prompt is handed off to a coding LLM, then the code is passed through vulnerability scanning LLMs, a vulnerability test is passed to another LLM to generate a code revision prompt, the code revision prompt is passed to the coding LLM, the code is revised according to the new prompt, and then delivered to the initial LLM for display." width="791" height="571" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-xs.png 384w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-sm.png 600w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-md.png 768w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-lg.png 1200w ,https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-xl.png 1600w"></figure>
<ol>
<li>User inputs prompt that lays out the desired software specification (features, file paths, data to use, etc...)</li>
<li>An initial LLM processes prompt, selects a coding specialized model to handle the task, and optimizes the prompt for that LLM.</li>
<li>An agent handles prompt transfer to the coding LLM.</li>
<li>Coding LLM writes code in accordance with the initial LLM's prompt.</li>
<li>Another agent transfers the code to one or more vulnerability scanning systems/LLM. (If multiple scanners/LLMs are involved, agents transfer data among them.)</li>
<li>Another agent passes a complete vulnerability report to another LLM for parsing into an optimized code refactoring prompt.</li>
<li>An agent passes the code refactoring prompt back to the coding LLM.</li>
<li>Code loops through the process until most/all vulnerabilities eliminated or mitigated.</li>
<li>An agent passes the finalized code to the initial LLM for display to the user.</li>
</ol>
<p>That's a lot of steps, but remember, everything from step 2 onwards is automated.</p>
<p>Now, a complex, multistep process like this does require thinking about the individual parts of the process:</p>
<ul>
<li>How much variability/randomness in results (temperature) should be used for code generating LLMs?</li>
<li>Can system prompts be used?</li>
<li>Should the initial LLM use a system prompt?</li>
<li>What system prompts should be used on what LLMs?</li>
<li>Should the vulnerability scanners be LLMs?</li>
<li>What number of vulnerability scanners should be used?</li>
<li>What data format should the vulnerability report be in?</li>
<li>How many iterations of vulnerability scanning should occur before code is pushed to the user?</li>
<li>What LLM or Small Language Model should handle parsing the vulnerability report into a code revision prompt?</li>
<li>Should reasoning steps from each part of the process be exposed to the user, either during processing or at the end?</li>
<li>Should reasoning steps be logged, but <strong>not </strong>displayed to the user?</li>
<li>How should the data transfer agents be coded?</li>
<li>What protections can be provided for the data in transit?</li>
</ul>
<p>And these are merely the considerations I can think of off the top of my head. This is definitely a situation where a team of individuals with a wide array of viewpoints and mindsets should undertake a design thinking approach to developing the process. In fact, I believe the MOSCOW method, where the team would determine what the process Must Have, Should Have, Could Have, and Won't Have, would be an ideal way to create these types of processes.</p>
<p>I suspect that a lot of the answers to these (and many more questions) would likely boil down to "What is an acceptable level of complexity for the available resources?" For example, a company with a proper AI server rack setup might be more willing to use multiple LLMs to handle vulnerability scanning. A company reliant on a cloud solution might opt for a single LLM to handle vulnerability scanning, depending on their AI provider's price plans.</p>
<p>Obviously, this would require a good deal of validation, especially from third parties, but if done well, would generate higher quality, more secure code.</p>
<h2 id="mcetoc_1imstc8mln9">Should We Do This?</h2>
<p>There's a famous quote from the movie <em>Jurassic Park</em> that honestly should live rent free in everyone's heads:</p>
<blockquote>
<p><em class="ot">...your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they </em><strong class="nz ie"><em class="ot">should.</em></strong></p>
<p>-Dr. Ian Malcolm</p>
</blockquote>
<p>So, should we hand off coding to "AI" (LLMs specifically). I say "yes", for several reasons:</p>
<ul>
<li>Software code complexity and interdependency has already reached a point where programmers often encounter scenarios where things work without them fully understanding the how or why.</li>
<li>Many/most people find it easier to articulate desired end goals than perfectly recalling and executing methods and procedures without a great deal of practice.</li>
<li>As coding language and vulnerability complexity increases, cognitive load on programmers also increases, eventually causing intellectual burnout.</li>
<li>The never-ending discovery of new vulnerabilities and malware makes it hard for humans to keep up on best coding practices without suffering information overload.</li>
<li>Updating system prompts and LLM datasets can be done faster than training multiple people.</li>
<li>Automating the code revision process should produce time savings that can be used for pre-release code function testing in simulated hardware/software environments.</li>
<li>LLMs can be used to guarantee human readable comments and error/log messages exist in the code, which humans might omit or fail to do properly.</li>
<li>People are already doing it due to the productivity gains - large scale coding benefits too much to revert to human only coding.</li>
</ul>
<p>Are there plenty of pitfalls and issues we can't even foresee? Absolutely. But it's also disingenuous to think anyone can stop AI coding from being a thing. There's just too many incentives for people to use LLMs to code, especially for people who are better at logic than the specifics of coding/programming. The best we can do is create a process that generates secure by design code and proliferate it as far and wide as possible.</p>
<p> </p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: Warp Terminal/Shell</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-warp-terminalshell/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-warp-terminalshell/</id>
        <media:content url="https://korgano.github.io/media/posts/34/Warp-terminal.png" medium="image" />
            <category term="UX/UI"/>
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="Cybersecurity"/>

        <updated>2025-02-27T09:54:30-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/34/Warp-terminal.png" alt="Warp Terminal Demo by Warp Team" />
                    For the past month and a half, I've been doing some form of DoD Cyber Workforce training. That consisted mostly&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/34/Warp-terminal.png" class="type:primaryImage" alt="Warp Terminal Demo by Warp Team" /></p>
                <p>For the past month and a half, I've been doing some form of DoD Cyber Workforce training.</p>
<p>That consisted mostly of a lot of slide decks, lots of lecture narration, and every so often in the Database Administrator course, actual SQL coding and SQL server related configuration work.</p>
<p>As one can imagine, this is <strong>very</strong> draining, especially when you're subjected to 60+ slides in a deck on some topics.</p>
<p>So, having completed all that, I want to think and talk about something entirely different.</p>
<p>Today, we're going to talk about Warp, a multi-platform (Windows, Linux, Mac) Terminal/Shell program.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1il42hdba40">What is Warp?</a></li>
<li><a href="#mcetoc_1il42hdba41">Why This is Interesting</a></li>
<li><a href="#mcetoc_1il42hdba42">The Issues with Warp</a></li>
<li><a href="#mcetoc_1il4g6b46gl">How Warp Works</a></li>
<li><a href="#mcetoc_1il4g6b46gm">Cloud-based Features (Opt-in)</a>
<ul>
<li><a href="#mcetoc_1il4g6b46gn">Warp AI</a></li>
</ul>
</li>
<li><a href="#mcetoc_1il4g6b46go">Telemetry (Opt-out)</a></li>
<li><a href="#mcetoc_1il4g6b46gp">What Would I Like to See</a></li>
</ul>
</div>
<h2 id="mcetoc_1il42hdba40">What is Warp?</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/qkduRen6QFk" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>There's a lot of features in Warp, as the video above explains, but here are some of the highlights:</p>
<ul>
<li>A UI that puts commands and outputs into discrete blocks.</li>
<li>Maintaining terminal session history.</li>
<li>Git repository branch and block checking.</li>
<li>The ability to highlight and simultaneously edit identical text.</li>
<li>AI assistant integration for command assistance.</li>
<li>"Permanent" web link generation for sharing terminal blocks.</li>
</ul>
<h2 id="mcetoc_1il42hdba41">Why This is Interesting</h2>
<p>As the Dave's Garage video points out, the old fashioned terminal dates back to the <strong>1970s</strong>. Lacking a lot of User Experience/User Interface features and standards modern users expect makes it harder for people to actually use and get proficient with command line interfaces.</p>
<p>The AI integration for command assistance in particular is a game changer, especially for Linux. One of Linux's <strong>massive </strong>pain points is how infuriatingly obtuse it can be to figure out the right commands for simple tasks and input them properly. And like Windows, commands with long paths become a nightmare. <a href="https://korgano.github.io/quick-cyber-thoughts-tuw-gui-wrapper/" title="Quick Cyber Thoughts: Tuw GUI Wrapper" target="_blank" rel="noopener noreferrer">That's why I use TUW to make simple GUIs for certain command line tasks I expect to do on a regular basis.</a></p>
<p>I would argue that something like Warp <strong>should </strong>replace all the existing terminal options... or at least be the default. There are too many benefits for too many people to do otherwise.</p>
<p>The problem is that Warp <strong>isn't </strong>that solution, at least for the personal and high security user.</p>
<h2 id="mcetoc_1il42hdba42">The Issues with Warp</h2>
<p>Let's get this out of the way: just looking around Warp's page tells you that this is a commercial developer first piece of software. <a href="https://www.warp.dev/pricing" title="Pricing and Plans for Warp" target="_blank" rel="noopener noreferrer">The fact that you can use it for free as a personal user is a bonus</a>, basically a way to expose people to the product and get them to evangelize it.</p>
<p>I have no problems with this part of their business model.</p>
<p>The issue with Warp, at least if you're a personal/high security user, is that you're dependent on Warp itself to provide your session/terminal block shares, and dependent on their AI provider for the AI capabilities.</p>
<p><a href="https://www.warp.dev/privacy" title="Our commitment to user privacy" target="_blank" rel="noopener noreferrer">Warp does address these issues on their privacy page.</a></p>
<p>We'll look at their own proprietary cloud features first.</p>
<blockquote>
<h2 id="mcetoc_1il4g6b46gl" class="css-tt1g22">How Warp Works</h2>
<p class="css-1q9rxa2">Warp is a fully-native local application that can run without an internet connection. The core features of the terminal will always work offline.</p>
<h2 id="mcetoc_1il4g6b46gm" class="css-14gwli7">Cloud-based Features (Opt-in)</h2>
<p class="css-1q9rxa2">Warp also includes cloud-based features that power cool things like:</p>
<ul class="css-1hrnkqr">
<li class="css-pl8tmj"><span class="css-1gijr9a">Warp AI</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Warp Drive</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Session Sharing</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Block Sharing</span></li>
</ul>
<p class="css-1q9rxa2">None of your data is ever sent to Warp’s servers unless you explicitly take action to send it. If you choose to use Warp Drive or block sharing and save an item, that item will be sent to Warp’s servers and stored securely in Warp’s database. Data is encrypted at rest, and you can delete this data anytime.</p>
</blockquote>
<p>The good:</p>
<ul>
<li>Outright stating that the cloud connection is <strong>not </strong>necessary to obtain most of the software's functionality.</li>
<li>Specific listings of what parts of the app use the cloud connection.</li>
<li>Stating that explicit action is required to send data to their servers.</li>
<li>Stating that data is encrypted at rest.</li>
<li>Stating the data can be deleted anytime.</li>
</ul>
<p>The not-great:</p>
<ul>
<li>Not mentioning if the data in transit is encrypted, or at least encapsulated in HTTPS.</li>
<li>Needing servers to handle the sharing functionality.</li>
<li>Not being able to select your own AI/cloud/session sharing provider.</li>
</ul>
<p>The problem for the personal/high security user is pretty much the same. You're putting all your data in one basket you may not be able to fully assess the security of, with no option to direct things to infrastructure you've configured for your needs.</p>
<p>(It's fairly obvious that having the streaming and sharing functionality be cloud based is to lower Warp's footprint on the machine, so I can't really blame them for that.)</p>
<p>There's also the fact that even if both parties have done their due diligence, the fact that everything has to pass through Warp's servers means that a determined threat might just choose to capture traffic going between your IP and Warp's. Even if they can't crack the encryption on the traffic, you'll still be giving valuable intel on your usage patterns.</p>
<blockquote>
<h3 id="mcetoc_1il4g6b46gn" class="css-1go34cv">Warp AI</h3>
<p class="css-1q9rxa2">Warp AI includes AI Command Suggestions, AI autofill in Warp Drive, and Agent Mode. All Warp AI features are powered by OpenAI and Anthropic APIs. When you submit an AI query, Warp does not store any of this information and only passes it through our servers as we proxy requests.</p>
<p class="css-1q9rxa2">OpenAI and Anthropic do not train their models on this data, and neither does Warp. OpenAI and Anthropic store this data for a maximum of 30 days.</p>
<p class="css-1q9rxa2">For organizations that need to ensure OpenAI and Anthropic never store data for any period of time, a Zero Data Retention policy is available on Warp's Enterprise plan.</p>
</blockquote>
<p>In terms of privacy, Warp acting as a proxy server is a decent way to obfuscate the relationships between user data and AI queries. Of course, this wouldn't be an issue if you could just route those queries to a local or private cloud AI server, but it at least adds a modicum of user protection.</p>
<p>Once we get past this, there's a stacking set of assumptions that need to be made:</p>
<ol>
<li>Warp does not store any data associated with AI queries.</li>
<li>Warp does not train any using data associated with AI queries.</li>
<li>OpenAI and Anthropic store AI query data only for 30 days maximum.</li>
<li>OpenAI and Anthropic have a policy to not train AI on data from Warp customers.</li>
<li>OpenAI and Anthropic AI devs actually care about any policy restricting them from using customer data.</li>
</ol>
<p>Let's assume the first 4 points are valid. Everyone outside the AI development team has been told that the AI development team will not use customer data for training purposes.</p>
<p>The question is: Has anyone validated that the AI dev team is respecting that policy. Because we exist in a universe where Meta engaged in rampant piracy to acquire training data:</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/Wl5vg-4V2ms" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>(I have no legal background that would allow me to comment on this in any great detail, but I do have one observation. Some of their sources were sites that host material that, in my experience, is literally crowd sourced preservation, due to the actual publishers never making legal digital copies.)</p>
<p>If the AI dev team decides to throw the officially stated policy out the window, there's not much of anything everyone relying on them can do until they know and have evidence policy has been broken.</p>
<p>This is obviously a big issue for anyone who values the confidentiality of their data.</p>
<p>One last thing to look at is their telemetry policy:</p>
<h2 id="mcetoc_1il4g6b46go" class="css-14gwli7">Telemetry (Opt-out)</h2>
<blockquote>
<p class="css-1q9rxa2">Warp records a limited number of defined telemetry events for the purposes of:</p>
<ul class="css-1hrnkqr">
<li class="css-pl8tmj"><span class="css-1gijr9a">Reporting app crashes</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Understanding feature usage at a high-level</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Supporting customers</span></li>
<li class="css-pl8tmj"><span class="css-1gijr9a">Terminal input and output are never included in telemetry payloads.</span></li>
</ul>
<p class="css-1q9rxa2">You can review <a class="css-1w9l6lr" href="https://docs.warp.dev/getting-started/privacy#exhaustive-telemetry-table">an exhaustive list of all telemetry events</a> in Warp’s documentation.</p>
<p class="css-1q9rxa2">You can also use the <a class="css-1w9l6lr" href="https://docs.warp.dev/features/network-log">Network Log</a> to monitor all communications from the Warp client to external servers.</p>
<p class="css-1q9rxa2">Telemetry events and crash reports are enabled by default and associated with logged-in users. If you would like to opt out of telemetry and crash reports, you can disable the feature during sign up or under Settings &gt; Privacy in the Warp app.</p>
</blockquote>
<p>There are two main things that I appreciate in this section:</p>
<ol>
<li>A direct link to the Network Log documentation, so you can preplan how to evaluate how transparent the logging is.</li>
<li>The telemetry events list addresses the vagueness of the phrase "feature usage at a high level"</li>
</ol>
<p>Being able to audit the network communications is a great move for transparency and security, so it's good to see Warp's commitment there.</p>
<h2 id="mcetoc_1il4g6b46gp">What Would I Like to See</h2>
<p>Let's imagine a world where Warp exists, alongside a near feature identical clone, Blink. Warp is as it is now. But Blink is how I would make Warp to be more secure. What would be the differences between the two?</p>
<p>Well, Blink would have the following things:</p>
<ul>
<li>The ability to connect to on-host or on-server local AI.</li>
<li>The ability to connect to your cloud AI provider of choice.</li>
<li>The ability to connect to your cloud data storage and/or web hosting provider of choice.</li>
<li>The ability to connect to your session sharing service of choice.</li>
</ul>
<p>Unfortunately, we're a few hardware and software generations away from having storage space and memory efficient local AI, so we can't just package an AI into the program and give it Retrieval Augmented Generation capabilities. That means we're going to have to have some other AI provider do that for us.</p>
<p>Now, these are not the simplest changes in the world to implement, and there would be a great deal of testing and troubleshooting necessary before a finalized version would be released. But they would address most, if not all, the issues with confidentiality if you wanted to use full set of Warp's software features.</p>
<p>Would Warp make these changes themselves? I doubt it, given their business model blurb on their privacy page:</p>
<blockquote>
<h2 class="css-14gwli7">Warp’s Business Model</h2>
<p class="css-1q9rxa2">Warp’s business plan is built around billing power users and business teams for cloud-based features. <a class="css-1w9l6lr" href="https://www.warp.dev/pricing">Check out our pricing page to learn more</a>.</p>
<p class="css-1q9rxa2">Please note Warp’s business model is not about collecting or monetizing any of your data.</p>
</blockquote>
<p>However, they eventually plan on making their client source code publicly available at some point, so perhaps that will become a reality. At the moment, it seems to be a matter of if, not when.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: CES 2025 Aftermath</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-ces-2025/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-ces-2025/</id>
        <media:content url="https://korgano.github.io/media/posts/33/ces-2025-event-banner.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2025-01-16T10:35:08-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/33/ces-2025-event-banner.png" alt="Consumer Electronics Show 2025 banner." />
                    The Consumer Electronics Show ended last week, and I made a list of things to keep an eye out for&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/33/ces-2025-event-banner.png" class="type:primaryImage" alt="Consumer Electronics Show 2025 banner." /></p>
                <p>The Consumer Electronics Show ended last week, and <a href="https://korgano.github.io/quick-cyber-thoughts-things-i-hope-to-see-at-ces-2025/" target="_blank" rel="noopener noreferrer">I made a list of things to keep an eye out for that touch on cybersecurity issues</a>. </p>
<p>So it's time to see if any of those things became reality!</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1ihqvnhufa7">Powerful APUs with Lots of AI Capability</a></li>
<li><a href="#mcetoc_1ihqvnhufa8">Business as Usual for Routers</a></li>
<li><a href="#mcetoc_1ihqvnhufa9">Things I Didn't Expect</a></li>
</ul>
</div>
<h2 id="mcetoc_1ihqvnhufa7">Powerful APUs with Lots of AI Capability</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/gUVqNeysHiq2U48E.jpg" alt="AMD AI Max Processor Model Chart" width="1274" height="717" data-is-external-image="true"></figure>
<p>This was pretty much a gimme, mostly because I kept abreast of leaks and analysis of what the processor manufacturers were doing.</p>
<p>AMD announced the "Strix Halo"/AI Max(+) processor line for workstation laptops and miniPCs. These processors are basically one or two chips of CPU cores next to a bigger chip that has an Neural Processing Unit for AI, a Graphics Processing Unit that can also be used for AI, and all the input/output for the whole package. What this means that if you can give this thing enough RAM, you can run a <strong>lot </strong>on this chip.</p>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/L8i66eGmkKmMtcbU.jpg" alt="AMD AI Max Input/Output Die and performance specifications." width="1271" height="712" data-is-external-image="true"></figure>
<p>With the right software and enough memory, you could easily run multiple VMs, including Kali Linux, while doing AI tasks using larger models, with up to 96GB of memory just for AI.</p>
<p>These will be <strong>expensive </strong>machines, but if you need/want to do a lot of VM and/or local AI work in a compact system, they're going to be the go to option. Intel doesn't have anything close, and Nvidia <strong>might</strong>, but they're stuck dealing with Windows on ARM. Reworking Windows to work in a different microarchitecture hasn't worked out well, so it's Ryzen AI Max or nothing.</p>
<p>A concerning observation is the fact that none of the systems advertised to use these APUs will use CAMM2 memory modules. It is unclear why this is the case, but it might behoove individuals and small organizations to wait to see if AI Max(+) laptops and miniPCs are sold with the better memory technology.</p>
<h2 id="mcetoc_1ihqvnhufa8">Business as Usual for Routers</h2>
<p>One thing that concerned me going into 2025 was the potential for a TP-Link router ban. Like many other Americans, I use TP-Link routers because they provide a compelling feature set at an affordable price. This is especially true in the mesh router space, where they cost quite a bit less than competing products from brands like NetGear and Asus.</p>
<p>To be <strong>extremely </strong>fair to all involved, this was <strong>before </strong><a href="https://www.scotusblog.com/2025/01/supreme-court-upholds-tiktok-ban/" title="Supreme Court upholds TikTok ban" target="_blank" rel="noopener noreferrer">the United States Supreme Court ruled that the TikTok ban was constitutional</a>:</p>
<blockquote>
<p>The Supreme Court on Wednesday unanimously upheld a federal law that will require TikTok to shut down in the United States unless its Chinese parent company can sell off the U.S. company by Jan. 19. In <a href="https://www.supremecourt.gov/opinions/24pdf/24-656_ca7d.pdf">an unsigned opinion</a>, the justices acknowledged that, “for more than 170 million Americans,” the social media giant “offers a distinct and expansive outlet for expression, means of engagement, and source of community.” But, the court concluded, “Congress has determined that divestiture is necessary to address its well-supported national security concerns regarding TikTok’s data collection practices and relationship with a foreign adversary.”</p>
</blockquote>
<p>The problem is that the router industry apparently decided to keep on going like nothing was happening. Aside from price tweaks on existing products, which are done on an individual store level, the major players in the Small Office-Home Office (SOHO) space just introduced new, expensive routers. Theoretically, these would provide downward price pressure on older products. In reality, the trend in some segments of consumer electronics is that things are kept the same price and new "premium" price tiers are added on top.</p>
<p>There also didn't appear to be any talk about the software side, which would be the low cost solution (replacing the existing TP-Link OS/firmware). The existing open source options don't cover the full range of TP-Link offerings, so at this point, a worst case scenario is an expensive replacement of all TP-Link routers.</p>
<h2 id="mcetoc_1ihqvnhufa9">Things I Didn't Expect</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.yankodesign.com/images/design_news/2025/01/this-game-changing-wi-fi-router-has-a-connectivity-radius-of-9-9-miles/wifi_halow_ces_2025_1.jpg" alt="WiFi HaLow 9.9 mile range router." width="1280" height="960" data-is-external-image="true"></figure>
<p><a href="https://www.yankodesign.com/2025/01/08/this-game-changing-wi-fi-router-at-ces-2025-has-a-connectivity-radius-of-9-9-miles/" title="This Game-Changing Wi-Fi Router at CES 2025 has a Connectivity Radius of 9.9 Miles" target="_blank" rel="noopener noreferrer">Something I didn't expect to see was a technology that could push WiFi range to around 10 miles:</a></p>
<blockquote>
<p class="post-without-image">Without getting into the specifics (because I have no technical background), the Wi-Fi HaLow router (shown above) promises to make internet connectivity seamless over massive distances. It relies on Sub-GHz frequency waves that travel long distances (like AM and FM radio) to transmit internet connectivity, so you could potentially use your home Wi-Fi router within a 10-mile radius of your house.</p>
<p class="post-without-image">Morse Micro, the company behind the tech, hopes that this Wi-Fi capability will coexist with current 2.4Ghz and 5Ghz Wi-Fi bands. These existing bands are great for low-latency internet connectivity, but add HaLow to the mix and you get long-distance connectivity too, giving you the best of all worlds. Sub-GHz Wi-Fi won’t ever be as fast as 5GHz Wi-Fi (HaLow has max speeds of 32.5 MB/s), although those speeds are perfect for most everyday tasks like checking email, browsing the internet, or even for IoT devices to communicate with each other.</p>
</blockquote>
<p>While this has a number of practical applications, this does add a whole host of security concerns. WiFi HaLow is going to be an enticing target for hackers, given the increased range and therefore greater geographic opportunities for direct hacking. Properly configuring and testing these systems is also going to be a bit of an ordeal, given that they haven't been standardized yet.</p>
<p><a href="https://www.pcworld.com/article/2575337/vlc-celebrates-6-billion-downloads-with-ai-subtitles.html" title="VLC celebrates 6 billion downloads with new AI subtitles feature" target="_blank" rel="noopener noreferrer">Another surprise came from the VLC open source media player team:</a></p>
<blockquote>
<p>At CES 2025, non-profit organization VideoLAN announced that their open-source media player app VLC Media Player crossed an incredible milestone: over 6 billion downloads, <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://techcrunch.com/2025/01/09/vlc-tops-6-billion-downloads-previews-ai-generated-subtitles/&amp;xcust=2-1-2575337-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2575337/vlc-celebrates-6-billion-downloads-with-ai-subtitles.html" rel="nofollow noopener" data-subtag="2-1-2575337-1-0-0-0-0" data-domain-name="techcrunch" target="_blank">TechCrunch</a> reports.</p>
<p>According to <a href="https://go.skimresources.com/?id=111346X1569483&amp;xs=1&amp;url=https://www.linkedin.com/feed/update/urn:li:activity:7282533937812258816/&amp;xcust=2-1-2575337-1-0-0-0-0&amp;sref=https://www.pcworld.com/article/2575337/vlc-celebrates-6-billion-downloads-with-ai-subtitles.html" rel="nofollow noopener" data-subtag="2-1-2575337-1-0-0-0-0" data-domain-name="linkedin" target="_blank">a LinkedIn post</a> written by VideoLAN’s president Jean-Baptiste Kempt, the user base for VLC continues to grow despite the ubiquitous popularity of streaming services.</p>
<p>To celebrate its undying popularity, VideoLAN demoed at CES 2025 an upcoming feature for VLC Media Player that uses generative AI to automatically create subtitles on the fly based on the media content being played, and can even translate across languages in real time.</p>
<p>The feature will use open-source AI models that can run locally on the user’s device without having to connect to the internet. This is a huge deal for anyone who consumes foreign-language content, who’s hard of hearing, or simply prefers subtitles while watching. And it’s especially nice for users who still rely on VLC to do things that many other video player apps can’t do, like <a href="https://www.pcworld.com/article/422753/how-to-play-dvds-in-windows-10-for-free.html">play DVDs on PC for free</a>.</p>
</blockquote>
<p>It'll be interesting to see how VLC accomplishes this AI integration. On one hand, these models need to be able to run with decent performance on a wide range of systems, including ones that potentially have nothing more than integrated (and therefore weak) graphics. On the other, VLC needs to integrate robust safeguards to prevent the AI from being used to execute malicious code and actions on the user's system.</p>
<p>If anything would promote the use of local AI, it would be VLC.</p>
<h2>The Other Things</h2>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/2lTWI7kfkt0?t=30s" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Sadly, there wasn't much progress on two other things I wanted to see. Ventiva's solid state cooling system did get a demo system built by Dell, which <strong>could </strong>mean that we could see industry adoption down the line. Dell did create the first compression attached memory modules, which then became CAMM2 memory, so it's not impossible.</p>
<p>But it does mean that we currently still have to worry about cooling fans, from a security and maintenance point of view.</p>
<p>Perhaps most disappointing to me was the lack of progress on improving price per gigabyte or terabyte in storage. Network Attached Storage vendors did improve their offerings with new designs and even AI integrations, but nothing for the storage itself.</p>
<p>The root cause of not having backups is often cost, and without addressing that, we're not going to be able to grow the culture of proper backups. While off-site backups are definitely a part of a proper backup system, the fact that on-site backups are priced out of most people's ability to stand up a solution is a grim reality.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Quick Cyber Thoughts: Things I Hope to See at CES 2025</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/quick-cyber-thoughts-things-i-hope-to-see-at-ces-2025/"/>
        <id>https://korgano.github.io/quick-cyber-thoughts-things-i-hope-to-see-at-ces-2025/</id>
        <media:content url="https://korgano.github.io/media/posts/32/ces-2025-event-banner.png" medium="image" />
            <category term="Tech"/>
            <category term="Quick Thoughts"/>
            <category term="PC Hardware"/>
            <category term="PC"/>
            <category term="Cybersecurity"/>
            <category term="AI"/>

        <updated>2024-12-30T10:26:39-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/32/ces-2025-event-banner.png" alt="CES 2025 event banner." />
                    The Consumer Electronics Show (CES) has always been a window into the future of technology, setting the stage for the&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/32/ces-2025-event-banner.png" class="type:primaryImage" alt="CES 2025 event banner." /></p>
                <p>The Consumer Electronics Show (CES) has always been a window into the future of technology, setting the stage for the innovations that will shape our lives. As we approach CES 2025, the buzz around advancements in consumer electronics is louder than ever.</p>
<p>2025 is going to be an interesting year from a cybersecurity perspective too, for a reason I don't think anyone expected. So here's what I hope to see from CES.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1igeofdkim0">Price Competition for Routers</a></li>
<li><a href="#mcetoc_1igeofdkim1">Widely Compatible Free/Cheap Router Operating System</a></li>
<li><a href="#mcetoc_1igeofdkim2">CAMM2 Becomes More Widely Implemented</a></li>
<li><a href="#mcetoc_1igeofdkim3">CPUs With Good NPUs At Decent Prices</a></li>
<li><a href="#mcetoc_1igeofdkim4">Fanless Cooling Solutions</a></li>
<li><a href="#mcetoc_1igeofdkim5">Storage Price Per Volume Improvements</a></li>
<li><a href="#mcetoc_1igjl3gqdm7">The Unexpected</a></li>
</ul>
</div>
<h2 id="mcetoc_1igeofdkim0">Price Competition for Routers</h2>
<p>This isn't something I expected to be a thing I would care about.</p>
<p>Then the US government, which has had a pretty dim view of TP-Link's cybersecurity stance and possible ties to the Chinese Communist Party, decided to drop a bombshell. They are considering banning the routers outright:</p>
<blockquote>
<p class="u-speakableText-p2">Investigators at the Commerce, Defense and Justice departments <a data-id="3a94ccdf-06e7-4808-b0bd-f387b0116b6d" href="https://www.cnet.com/home/internet/possible-tp-link-ban-set-for-2025-what-it-means-for-your-internet-connection/" target="_self">have all opened probes</a> into the company due to its ties to Chinese cyberattacks. These departments are weighing a potential ban on the sale of TP-Link routers, according to a <a rel="noopener nofollow" data-id="3a94ccdf-06e7-4808-b0bd-f387b0116b6d" href="https://www.wsj.com/politics/national-security/us-ban-china-router-tp-link-systems-7d7507e6" target="_blank" title="(opens in a new window)" class="c-regularLink">Wall Street Journal article</a> published last week.</p>
<p>TP-Link has become increasingly dominant in the US router market since the pandemic. According to the Journal report, it grew from 20% of total router sales in 2019 to around 65% this year. TP-Link disputed these numbers to CNET, and a separate analysis from the IT platform Lansweeper found that <a rel="noopener nofollow" data-id="3a94ccdf-06e7-4808-b0bd-f387b0116b6d" href="https://www.lansweeper.com/blog/cybersecurity/us-considers-banning-tp-link-routers-over-security-concerns/" target="_blank" title="(opens in a new window)" class="c-regularLink">12% of home routers</a> in the US are TP-Link. </p>
<p>While there have been high-profile cyberattacks involving TP-Link routers, this potential ban is more about the company’s ties to China than specific security issues that have been publicly identified, according to cybersecurity researchers I spoke with. </p>
<p><a href="https://www.cnet.com/home/internet/tp-link-routers-could-be-banned-next-year-are-they-actually-dangerous/">https://www.cnet.com/home/internet/tp-link-routers-could-be-banned-next-year-are-they-actually-dangerous/</a></p>
<p>The investigation comes after a <a href="https://www.bleepingcomputer.com/news/security/microsoft-chinese-hackers-use-quad7-botnet-to-steal-credentials/" target="_blank" rel="nofollow noopener">Microsoft report revealed in October</a> that a botnet of hacked SOHO routers—tracked as Quad7, CovertNetwork-1658, or xlogin and operated by Chinese threat actors—is mainly made from TP-Link devices.</p>
<p>"Microsoft tracks a network of compromised small office and home office (SOHO) routers as CovertNetwork-1658. SOHO routers manufactured by TP-Link make up most of this network," the company said.</p>
<p>"Microsoft assesses that multiple Chinese threat actors use the credentials acquired from CovertNetwork-1658 password spray operations to perform computer network exploitation (CNE) activities."</p>
<p>On Monday, the New York Times also <a href="https://www.nytimes.com/2024/12/16/us/politics/biden-administration-retaliation-china-hack.html" target="_blank" rel="nofollow noopener">reported</a> that the Biden administration will ban China Telecom's last active U.S. operations in response to Chinese state hackers <a href="https://www.bleepingcomputer.com/news/security/white-house-salt-typhoon-hacked-telcos-in-dozens-of-countries/" target="_blank" rel="nofollow noopener">breaching multiple U.S. telecom carriers</a>. The Federal Communications Commission (FCC) <a href="https://www.bleepingcomputer.com/news/security/us-bans-china-telecom-americas-over-national-security-risks/" target="_blank" rel="nofollow noopener">revoked China Telecom Americas' license</a> in January 2022 over "significant national security concerns."</p>
<p><a href="https://www.bleepingcomputer.com/news/security/us-considers-banning-tp-link-routers-over-cybersecurity-risks/">https://www.bleepingcomputer.com/news/security/us-considers-banning-tp-link-routers-over-cybersecurity-risks/</a></p>
</blockquote>
<p>The fact that 65% of the market uses TP Link means that if there <strong>is </strong>a ban on TP-Link routers, <strong>especially </strong>for home office users in cybersecurity and government, there's going to be <strong>massive </strong>repercussions.</p>
<p>We're talking about replacing every one of those people having to replace <strong>hundreds of dollars </strong>worth of hardware, in a relatively small time window, at roughly the same time.</p>
<p>This might end up creating the router equivalent of the GPU shortages of 2021-2022, where every graphics card had its price balloon due to lack of availability, and the only affordable options were ones that barely provided any performance improvements over two generation old models.</p>
<p>To combat this, we <strong>need </strong>to see manufacturers put out price competitive options, especially in the mesh router category. Those setups are already expensive, but provide major performance benefits for anyone operating out of a proper house, versus an apartment.</p>
<p>It would also be nice to see government, federal or state, provide some kind of financial relief for people forced to replace their entire network setup. This could be in the form of a tax exemption/tax holiday, a rebate for turning in TP-Link hardware, or any sort of cost mitigation strategy for such a transition.</p>
<h2 id="mcetoc_1igeofdkim1">Widely Compatible Free/Cheap Router Operating System</h2>
<p>This would be somewhat out of left field, but if an organization/company could release a secure, low cost (free or cheap) router operating system (OS), that might be a <strong>massive </strong>gamechanger.</p>
<p>The problem with the TP-Link ban is the cost of replacing the hardware. Routers might be affordable, but they're not cheap unless you're going pretty far down the product stack. That's why this ban has massive negative consequences: it's basically penalizing everyone who made a rational financial decision for the behavior of the manufacturer.</p>
<p>But if you don't have to replace the hardware, just software operating the devices, then the costs to the end users drops dramatically.</p>
<p>It might not be the right solution for everyone, especially in government or cybersecurity, but for the average home user, it might fit the bill.</p>
<p>That naturally depends on the OS being easy to upload onto the device, <strong>especially </strong>for mesh routers, but if someone can do it, it'll be a win for everyone.</p>
<h2 id="mcetoc_1igeofdkim2">CAMM2 Becomes More Widely Implemented</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/YVcQdrEtNQKsQzJY.jpg" alt="JEDEC wiring diagram of Compression Attached Memory Modules to CPU, vs conventional Random Access Memory connections to CPU." width="1497" height="840" data-is-external-image="true"></figure>
<p>For those of you not familiar, CAMM2 is Compression Attached Memory Modules, a new way of implementing Random Access Memory (RAM). Instead of sticking up out of the motherboard on many systems, it's designed to either directly touch the motherboard, or slightly parallel.</p>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/Djj9PGPlnQ6kti05.jpg" alt="Example CAMM2 RAM implementations for desktop and laptop." width="1490" height="836" data-is-external-image="true"></figure>
<p>This design has a number of space saving advantages, especially at high capacities, but the big improvements are power efficiency and memory speed. This is due to the overall shorter path that data has to travel to get to and from the memory or CPU.</p>
<p>How does this help cybersecurity? Well, better performance improves everything. This is especially useful for running virtualized software and operating systems. But it can also be a big help when running local AI, as RAM size and speed play a huge part of task execution.</p>
<h2 id="mcetoc_1igeofdkim3">CPUs With Good NPUs At Decent Prices</h2>
<p>Local AI, specifically Large Language Models (LLMs) and Small Language Models (SLMs), aren't really seeing broad adoption among the masses.</p>
<p>However, local generative AI has a lot of direct applicability for cybersecurity practioners. First, it's a lot easier and much lower risk to test local AI for vulnerabilities, because they're running on your own hardware, not someone's cloud instance. Second, local AI is pretty good at code generation, which is something a lot of cybersecurity professionals do.</p>
<p>Neural Processing Units (NPUs) are hardware optimized for running the kind of computations associated with LLMs. So far, these units are only proliferating into laptop CPUs, and generally aren't the highest performance, especially at the $1000+ price point that prosumer laptops fit in.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/pZjqzQVc-So" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>However, AMD is going to release its Strix Halo line of laptop CPUs, with up double the AI performance on NPU (40 TOPS from 20 TOPS of its predecessor), as well as over doubling its GPU core count. Since GPUs are leveraged in a variety of AI tasks, the combination should make running local AI much more performant.</p>
<p>If laptops with these processors can hit the market at $2000 or less for workstation builds, then I think we have a good chance of seeing local AI use proliferate. This would help mitigate a lot of data safety issues created by the use of cloud AI, up to and including poorly trained personnel putting PII into LLM prompts.</p>
<h2 id="mcetoc_1igeofdkim4">Fanless Cooling Solutions</h2>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/1o1VQqh79oxO2q62.jpg" alt="Samsung Galaxy Edge 14 - comparison between conventional fans and Frore Airjet cool system." width="1000" height="600" data-is-external-image="true"></figure>
<p>One recent technology trend of interest is the development of fanless cooling for computers. Mostly confined to laptops and other small devices, there's a cybersecurity application that most people would not think of.</p>
<p><a href="https://www.tomshardware.com/news/steal-data-through-fan-vibrations-cybersecurity" title="Cyberattack Steals PC Data Through Fan Vibrations" target="_blank" rel="noopener noreferrer">Four years ago, researchers proved that you can steal data off of an endpoint via a mobile app (AiR-ViBeR) that tracks your device's fan vibrations.</a></p>
<figure class="post__image"><img loading="lazy"  src="https://www.techpowerup.com/img/lkCxvbkpveYrUMei.jpg" alt="Airflow caused by Ventiva solid state cooling device." width="1584" height="960" data-is-external-image="true"></figure>
<p>While the Frore Airjet, which uses a vibrating membrane to move air, might be vulnerable to a similar exfiltration method in the future, there is a new solution on the market. Ventiva has created a solid state device that uses ionization to move air:</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/fyai_kUYhLs" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Even if it cannot scale to desktop or server use, Ventiva's solution might be able to eliminate an entire vector of data exfiltration from mobile devices, which are a prime target for threat actors.</p>
<h2 id="mcetoc_1igeofdkim5">Storage Price Per Volume Improvements</h2>
<p>Backups are a critical part of cybersecurity, especially if there's a ransomware event that forces you to execute a recovery plan.</p>
<p>The problem is, as you get smaller and smaller in size, you're actually <strong>less </strong>likely to have backups, for a simple reason. Storage is expensive, and have redundancy, you have to buy <strong>multiples </strong>of expensive drives to do it properly.</p>
<figure class="post__image"><img loading="lazy"  src="http://www.trekprops.de/wordpress/wp-content/uploads/2009/10/tng_isochips_4519.jpg" alt="Clear, neon green, blue, dark red, orange and yellow isolinear chip Star Trek prop replicas from trekprops.de." width="480" height="360" data-is-external-image="true"></figure>
<p>Sadly, we don't live in a reality where Star Trek style solid state, hotswap memory chips with large capacities, lots of longevity, and low cost exist.</p>
<p>With file sizes ballooning, especially for media, the costs of backing up proportionally grow larger. Even cloud based backups, which have theoretical cost savings to offset their availability and control downsides, increase drastically in cost as storage needs increase. And in a bad economy, cutting costs wherever possible is going to be the norm, especially on the individual to small business level.</p>
<p>Good cybersecurity practices start at home. And if we really want people to start getting in the habit of backing up their data, it needs to be as cheap and easy as possible. Right now, $250+ for 14+ TB drives is not viable for a lot of people. Smaller drives for smaller files, like 4TB SSDs, also aren't in a great price spot, at nearly $300 a piece.</p>
<p>Storage capacity improvements are great, but if the costs of deploying a properly configured system for backups doesn't go down for non-enterprise scenarios, then the war against ransomware will never really go anywhere. No or minimal backups means that victims will be incentivized to pay the ransom, with no guarantee that they'll recover their files.</p>
<h2 id="mcetoc_1igjl3gqdm7">The Unexpected</h2>
<p>One of the great things about technology conferences is that you never know what will show up.</p>
<p>Last year, we had a demo of QDEL, a competitor technology to Organic Light Emitting Diodes, randomly make an appearance at CES.</p>
<p>With growing awareness of how important cybersecurity is for the average person, we might see a lot of products and services aimed at the less tech savvy.</p>
<p>What would they be? I don't know! And that's what makes it exciting.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>Tech Project: TrueNAS Scale Server 1.0</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/tech-project-truenas-scale-server-10/"/>
        <id>https://korgano.github.io/tech-project-truenas-scale-server-10/</id>
        <media:content url="https://korgano.github.io/media/posts/30/trueNAS-server-5-2.jpg" medium="image" />
            <category term="Tech Projects"/>
            <category term="Tech"/>
            <category term="PC Hardware"/>
            <category term="PC"/>

        <updated>2024-12-10T09:00:56-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/30/trueNAS-server-5-2.jpg" alt="Power supply, graphics card, equipped motherboard, and hard drive caddy installed in case." />
                    It's the holiday season, so I'm taking a break from the OPNsense project for a few reasons: Instead, I'll be&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/30/trueNAS-server-5-2.jpg" class="type:primaryImage" alt="Power supply, graphics card, equipped motherboard, and hard drive caddy installed in case." /></p>
                <p>It's the holiday season, so I'm taking a break from the OPNsense project for a few reasons:</p>
<ol>
<li>The various patches don't seem to have any FreeBSD Intel WiFi driver improvements, so the connectivity issues aren't being solved.</li>
<li>The OPNsense roadmap shows that a new version should be dropping in January.</li>
<li>I can't really progress any further with the project beyond some minor setup, because no consistent connection = no consistent monitoring.</li>
</ol>
<p>Instead, I'll be pivoting into a different tech related project: building a TrueNAS Scale server!</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1ier6tpbhp">The Backstory</a></li>
<li><a href="#mcetoc_1ier6tpbhq">The Hardware (Internal)</a></li>
<li><a href="#mcetoc_1ier6tpbhr">The Hardware (Case)</a></li>
<li><a href="#mcetoc_1ier6tpbhs">After the Install</a></li>
<li><a href="#mcetoc_1ier6tpbht">Next Steps</a></li>
</ul>
</div>
<h2 id="mcetoc_1ier6tpbhp">The Backstory</h2>
<p>This project has a really simple backstory, in two parts.</p>
<ol>
<li>I had working computer hardware left over from when I rebuilt my gaming computer to use a Ryzen 7 7800X3D and an RX 7900 XTX.</li>
<li>My father was very impressed by his friend's Plex server for streaming his media collection in the home.</li>
</ol>
<p>Since I was already planning to repurpose the old hardware into a server, everything synergized quite well.</p>
<h2 id="mcetoc_1ier6tpbhq">The Hardware (Internal)</h2>
<p>The actual hardware that's going to be running TrueNas Scale is an interesting hodge-podge of parts I've picked up over the years.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-4.jpg" alt="X370 Gaming K4 motherboard with Ryzen 5 1600X, custom heat sinks, 256GB NVMe SSD, and 118GB Optane drive installed." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-4-xl.jpg 1600w"></figure>
<p>The main guts of the system - the CPU, RAM, and motherboard - date back to when I originally built this to be my gaming rig. I jumped on the first generation of Ryzen in 2017 with an R5 1600X, DDR4-2666, and an AM4 X370 Gaming K4 motherboard. The motherboard in particular is the reason why I never did an in place upgrade like most other AM4 owners.</p>
<p>The primary reason was that the X370 Gaming K4 was badly designed, with poor circuitry connecting the RAM to CPU. It was end of life'd only a few months after Ryzen hit the market.</p>
<p>The second reason was that it was designed before BIOS Flashback, which does <strong>not </strong>require you to have a CPU to update the motherboard BIOS, was invented, so updating the BIOS is a huge pain.</p>
<p>However, I did do some customization to the motherboard. Back in 2018, when I was wrapping up training to be a CNC machinist, I made custom heat sinks for the motherboard's Voltage Regulator Modules (VRM). The board shipped with fairly low surface area heat sinks, so I grabbed some blocks of scrap aluminum, wrote up a CNC program, and milled new heat sinks. These might survive the eventual scrapping of this hardware, either as paperweights, or as actual heat sinks.</p>
<p>The CPU cooler was another old piece of hardware I had gotten to go with the Ryzen 5 1600X. The Cryorig H7 was a pretty cheap air cooler at the time with two main flaws:</p>
<ol>
<li>Excessive curvature of the contact plate.</li>
<li>An awful installation method:</li>
</ol>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/kNF-GHQthro" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>In fact, this miserable method of installation caused me an absurd amount of grief trying to properly align the bracket for maximum contact.</p>
<p>I had a PCI-3.0 NVMe drive lying around from when I owned an AMD Dell laptop... which died a few days after the warranty expired. So I installed that as the OS drive, since it had an integrated heat sink on it.</p>
<figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/mD6i2toN7lE?pp=ygUSbGV2ZWwxdGVjaHMgb3B0YW5l" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure>
<p>Following the advice of Level1Techs, I picked a 118GB M.2 Optane stick, back when they were still available for purchase. This is going to be a cache drive for the TrueNAS metadata, which should give me some solid performance boosts.</p>
<p>Because the Ryzen 5 1600X has no internal graphics, I needed a graphics card for the system to even boot. Luckily, I had an RX 580 8GB lying around, which used to power my brother's gaming rig, until I replaced it with the RX 6600 I had been using until I obtained the RX 7900 XTX. While not the most efficient GPU, it was more than enough to do the job. </p>
<h2 id="mcetoc_1ier6tpbhr">The Hardware (Case)</h2>
<p>The biggest headache of the modern era is that computer cases with 5.25 inch and 3.25 inch drive bays are becoming less common. This makes sense, as 2.5in and NVMe SSDs with larger capacities easily fulfill the capacity needs of many people, and data redundancy isn't really something they think about.</p>
<p>(Also, if we're going to be honest, there just isn't a Keep It Stupid Simple backup solution for most people.)</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-1.jpg" alt="20+ year old PC case with original short feet and custom designed 3D printed feet." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-1-xl.jpg 1600w"></figure>
<p>Luckily, my father's friend had an old PC gathering dust in a corner that had 5.25in drive bays. It even had 5.25 to 3.25in drive sleds... for only 3 drives. And an old, obsolete CD drive. And a floppy drive. And the shortest feet possible for a case.</p>
<p>Amazingly, the thing did boot and was apparently running Windows XP, so I had the job of shucking the hard drives to be returned to the original owner, in case anything valuable was on there.</p>
<p>After gutting the case for cleaning, I decided to ditch the hard drive sleds, since I wanted to implement a RAID 10 configuration or something similar in TrueNAS. I also noticed the lack of cable access/management paths, air flow, and all the other problems that cases from 20+ years ago had.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/Athena-Power-BP-TLA3141SAS12-drivebay.jpg" alt="Athena Power BP-TLA3141SAS12 12 Gbps Mini-SAS HD Hot-Swap SAS / SATA 3.5&quot; HDD Internal Hard Drive Backplane Module" width="1280" height="1199" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/Athena-Power-BP-TLA3141SAS12-drivebay-xl.jpg 1600w"></figure>
<p>I purchased a hot swap 5.25in to 3.25in drive bay off of Newegg, which uses the SAS interface to handle all the data from the drives. It also comes with a cooling fan, although I don't know how effective this will be with solid bay doors. This then required me to buy a separate SAS PCI-E card and an adaptor cable due to differing SAS connector types. </p>
<p>I also purchased a fully modular power supply, since that would let me leave any unused cables outside the case. This improves air flow by not blocking air with thick cables that don't do anything useful. Air flow is a major issue with this case, because there are only two fans - an 80mm exhaust fan, and an 80mm intake fan that tries to pull air through a hole a few millimeters from the ground.</p>
<p>To solve this problem and the problem of insufficient cable pass through, I returned to tradition: case modding.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-2.jpg" alt="Initial attempts to modify an old case with a dremel." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-2-xl.jpg 1600w"></figure>
<p>In the old days, people would dremel, saw, and mill computer cases to make them more useful. Since I didn't have a mill and there's no convenient local makerspace, that left me with the dremel and saw options. Unfortunately, I wasn't able to cut as cleanly as I wanted to, but I got the job done.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-3.jpg" alt="Finalized case modifications done by dremel and jigsaw, cutting slots for cable pass through." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-3-xl.jpg 1600w"></figure>
<p>However, I deferred on the next step of the case modification, cutting holes in the case side panel for air intake. I happen to have a large number of 120mm fans, but figuring out where to place them to avoid collisions with other hardware is pretty difficult. So to help plan that out, I went ahead and installed the majority of the hardware into the case.</p>
<h2 id="mcetoc_1ier6tpbhs">After the Install</h2>
<p>Putting together the PC was a bit of a pain, due to the aforementioned poor cooler mounting.</p>
<p>The rest was pretty straight forward, at least for an experienced PC builder. The main issue, as I expected was cable management, because the power cables are <strong>thick</strong> and eat up a lot of internal volume.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-5.jpg" alt="Power supply, graphics card, equipped motherboard, and hard drive caddy installed in case." width="1000" height="750" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-5-xl.jpg 1600w"></figure>
<p>I discovered that I would need to probably at least remove the GPU to handle hooking up the front panel connectors for the case. And at this point, I hadn't received the SAS card and cable, either. So I decided I would go install TrueNAS Scale, do some initial configuration, and then shut the system down to do more work.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/30/trueNAS-server-6.jpg" alt="TrueNAS Scale install error due to using Ventoy." width="1000" height="602" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-xs.jpg 384w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-sm.jpg 600w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-md.jpg 768w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-lg.jpg 1200w ,https://korgano.github.io/media/posts/30/responsive/trueNAS-server-6-xl.jpg 1600w"></figure>
<p>Unfortunately, what should have been a straightforward operation using my Ventoy flash drive failed. This means I now have to find either a cheap flash drive or an old flash drive, and use Balena Etcher to format the drive into an installation drive.</p>
<p>However, I was pleasantly surprised to see the computer worked, so all that hard work wasn't for nothing.</p>
<h2 id="mcetoc_1ier6tpbht">Next Steps</h2>
<p>Not much is left to do, but what's there is pretty important:</p>
<ul>
<li>Find a flash drive to use to install TrueNAS Scale.</li>
<li>Get the SAS adapter cable and card installed.</li>
<li>Install the front panel and connect it to the motherboard.</li>
<li>Figure out if I need slim 120mm fans and where can I put them.</li>
<li>Cut the holes in the side panel and install the fans.</li>
<li>Install the hard drives and get them configured.</li>
<li>Get media onto the disk drives.</li>
<li>Install a Plex docker image.</li>
<li>Find a corner to stick the server in.</li>
</ul>
<p>Let's see if I can get this done before Christmas.</p>
            ]]>
        </content>
    </entry>
    <entry>
        <title>CyberSecurity Project: Transparent Filtering Bridge (+ Extras) 7.0</title>
        <author>
            <name>Xavier Santana</name>
        </author>
        <link href="https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-70/"/>
        <id>https://korgano.github.io/cybersecurity-project-transparent-filtering-bridge-extras-70/</id>
        <media:content url="https://korgano.github.io/media/posts/29/opnsense-aliases_02-2.JPG" medium="image" />
            <category term="Tech"/>
            <category term="Cybersecurity Projects"/>
            <category term="Cybersecurity"/>

        <updated>2024-11-25T12:09:36-05:00</updated>
            <summary>
                <![CDATA[
                        <img src="https://korgano.github.io/media/posts/29/opnsense-aliases_02-2.JPG" alt="Confirmation that the full list of IP addresses was added to one of the Aliases." />
                    Last time on the Transparent Filtering Bridge project, I started working on building up alias lists for the various firewall&hellip;
                ]]>
            </summary>
        <content type="html">
            <![CDATA[
                    <p><img src="https://korgano.github.io/media/posts/29/opnsense-aliases_02-2.JPG" class="type:primaryImage" alt="Confirmation that the full list of IP addresses was added to one of the Aliases." /></p>
                <p>Last time on the Transparent Filtering Bridge project, I started working on building up alias lists for the various firewall rules. Through the use of a Python script, I scraped IP addresses from Pi-Hole DNS block lists. Since Pi-Hole only focuses on blocking domain names, having OPNsense block those specific IPs increases the layers of defense for the network.</p>
<div class="post__toc">
<h3>Table of Contents</h3>
<ul>
<li><a href="#mcetoc_1idkgvag76q">Figuring out the CSV Format</a></li>
<li><a href="#mcetoc_1idkgvag76r">Validating the Aliases</a></li>
<li><a href="#mcetoc_1idkgvag76s">Next Steps</a></li>
</ul>
</div>
<h2 id="mcetoc_1idkgvag76q">Figuring out the CSV Format</h2>
<p>One thing that proved to be surprisingly difficult was formatting the CSV correctly. The columns were the easy part. The problem was that the spreadsheet editor I was using saved CSVs in a way that caused the OPNsense Alias script to error out.</p>
<p>Even switching to Visual Studio Code didn't really help, but it made me realize that the issue could be in the formatting.</p>
<p>So, to brute force the issue, I went and acquired CSVEdit, a decade plus piece of software that's specifically made to handle CSVs. Starting with a copy of the original I made, I then began progressing through the various save options to find the correct combination that would load. After several iterations, I discovered these were the proper settings:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases_03.JPG" alt="CSV settings for the OPNsense Alias script - Value Separator = Comma, Character Set = System, String (Value) Delimiter = Nothing" width="550" height="208" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_03-xl.JPG 1600w"></figure>
<p>After generating a properly formatted CSV, I successfully updated the Alias.json file, then uploaded it to OPNsense.</p>
<h2 id="mcetoc_1idkgvag76r">Validating the Aliases</h2>
<p>Once this was concluded, I noticed an immediate issue:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases.JPG" alt="OPNsense Alias Interface with only one IP applied to most aliases." width="1600" height="860" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases-xl.JPG 1600w"></figure>
<p>Despite having large lists of IPs, only one IP address showed up in each alias. This meant I had incorrectly formatted something in the CSV. Examining the script's code revealed this:</p>
<blockquote>
<p><code>if len(row['data'].split(" "))&gt;1:</code><br><code>            item_data = "\n".join(row['data'].split(" "))</code></p>
</blockquote>
<p>So the main issue was the fact that I mistakenly separated out each IP address or range into its own row. This meant that I had to reformat the list of IPs from each IP being on its own line, to being all on one line, with spaces as separation. </p>
<p>This means I'll have to make some adjustments to my script's code, to have it automatically generate a list in that format.</p>
<p>However, until I make that change, I had to manually edit the lists into the proper format, which was tedious, but not too difficult.</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases_01.JPG" alt="Correctly formatted CSV for the OPNsense Alias script, with space separated IP addresses in the Data column." width="759" height="318" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_01-xl.JPG 1600w"></figure>
<p>Once these changes were made, the new Alias.json was generated and uploaded into OPNsense, then visually inspected in the GUI:</p>
<figure class="post__image"><img loading="lazy"  src="https://korgano.github.io/media/posts/29/opnsense-aliases_02.JPG" alt="Confirmation that the full list of IP addresses was added to one of the Aliases." width="1600" height="860" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-xs.JPG 384w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-sm.JPG 600w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-md.JPG 768w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-lg.JPG 1200w ,https://korgano.github.io/media/posts/29/responsive/opnsense-aliases_02-xl.JPG 1600w"></figure>
<h2 id="mcetoc_1idkgvag76s">Next Steps</h2>
<p>The CSV format information has been uploaded to the <a href="https://github.com/korgano/OPNsenseAliasTools" title="OPNsenseAliasTools" target="_blank" rel="noopener noreferrer">Github repo</a>, so all that needs to be done is iterating on my script to do a few things:</p>
<ul>
<li>Ignore IP addresses embedded in URLs.</li>
<li>Generate a separate list for IP address ranges.</li>
<li>Generate the text files in the proper format (IP addresses separated by spaces).</li>
</ul>
<p>Once these are complete, they will be tested and uploaded to the repo upon validation.</p>
            ]]>
        </content>
    </entry>
</feed>
