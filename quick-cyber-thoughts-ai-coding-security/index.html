<!DOCTYPE html><html lang="en-us"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Quick Cyber Thoughts: AI Coding Security - XS Tech Thoughts</title><meta name="description" content="XS Tech Thoughts author Xavier Santana explores cybersecurity challenges related to using LLMs to generate code, suggests how to make AI generated code secure by design."><meta name="generator" content="Publii Open-Source CMS for Static Site"><!-- Global site tag (gtag.js) - Google Analytics --><script type="gdpr-blocker/Analytics" async src="https://www.googletagmanager.com/gtag/js?id=G-EHLH28GQMY"></script><script type="gdpr-blocker/Analytics">window.dataLayer = window.dataLayer || [];
				  function gtag(){dataLayer.push(arguments);}
				  gtag('js', new Date());
				  gtag('config', 'G-EHLH28GQMY' , { 'anonymize_ip': true });
				  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-EHLH28GQMY"></script><script>window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-EHLH28GQMY');</script><link rel="stylesheet" href="https://korgano.github.io/media/plugins/syntaxHighlighter/prism-black.css"><link rel="stylesheet" href="https://korgano.github.io/media/plugins/syntaxHighlighter/prism-inline-color.css"><link rel="canonical" href="https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/"><link rel="alternate" type="application/atom+xml" href="https://korgano.github.io/feed.xml" title="XS Tech Thoughts - RSS"><meta property="og:title" content="Quick Cyber Thoughts: AI Coding Security"><meta property="og:image" content="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png"><meta property="og:image:width" content="1280"><meta property="og:image:height" content="768"><meta property="og:site_name" content="XS Tech Thoughts"><meta property="og:description" content="XS Tech Thoughts author Xavier Santana explores cybersecurity challenges related to using LLMs to generate code, suggests how to make AI generated code secure by design."><meta property="og:url" content="https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/"><meta property="og:type" content="article"><link rel="preload" href="https://korgano.github.io/assets/dynamic/fonts/robotoflex/robotoflex.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="https://korgano.github.io/assets/dynamic/fonts/robotoslab/robotoslab.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://korgano.github.io/assets/css/style.css?v=253922ed3c1816ffff870ea807b853dd"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://korgano.github.io/quick-cyber-thoughts-ai-coding-security/"},"headline":"Quick Cyber Thoughts: AI Coding Security","datePublished":"2025-03-19T12:12-04:00","dateModified":"2025-03-21T14:03-04:00","image":{"@type":"ImageObject","url":"https://korgano.github.io/media/posts/35/laptop-2external-monitors.png","height":768,"width":1280},"description":"XS Tech Thoughts author Xavier Santana explores cybersecurity challenges related to using LLMs to generate code, suggests how to make AI generated code secure by design.","author":{"@type":"Person","name":"Xavier Santana","url":"https://korgano.github.io/authors/xavier-santana/"},"publisher":{"@type":"Organization","name":"Xavier Santana"}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript></head><body class="post-template lines"><div class="container lines lines--right"><header class="header"><a href="https://korgano.github.io/" class="logo">XS Tech Thoughts</a><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu">Menu</button><ul class="navbar__menu"><li><a href="https://korgano.github.io/" title="Home" target="_self">Home</a></li><li><a href="https://korgano.github.io/tags/uxui-case-study/" title="UX/UI Case Studies" target="_blank">UX/UI Case Studies</a></li><li><a href="https://korgano.github.io/tags/cybersecurity-projects/" title="Cybersecurity Projects" target="_blank">Cybersecurity Projects</a></li><li><a href="https://korgano.github.io/tags/quick-thoughts/" title="Quick Thoughts" target="_self">Quick Thoughts</a></li><li><a href="https://korgano.github.io/about-me/" title="About Me" target="_blank">About Me</a></li></ul></nav></header><main class="main post"><article class="content"><header class="content__inner content__header"><h1 class="content__title">Quick Cyber Thoughts: AI Coding Security</h1><div class="content__meta"><div class="content__meta__left"><a href="https://korgano.github.io/authors/xavier-santana/" class="invert content__author" rel="author" title="Xavier Santana">Xavier Santana</a></div><div class="content__meta__right"><time datetime="2025-03-19T12:12" class="content__date">March 19, 2025</time><div class="content__updated">Updated on <time datetime="2025-03-19T12:12" class="content__date">March 21, 2025</time></div></div></div></header><figure class="content__featured-image"><div class="content__featured-image__inner is-img-loading"><img src="https://korgano.github.io/media/posts/35/laptop-2external-monitors.png" srcset="https://korgano.github.io/media/posts/35/responsive/laptop-2external-monitors-xs.png 384w, https://korgano.github.io/media/posts/35/responsive/laptop-2external-monitors-sm.png 600w, https://korgano.github.io/media/posts/35/responsive/laptop-2external-monitors-md.png 768w, https://korgano.github.io/media/posts/35/responsive/laptop-2external-monitors-lg.png 1200w, https://korgano.github.io/media/posts/35/responsive/laptop-2external-monitors-xl.png 1600w" sizes="(min-width: 37.5em) 1600px, 80vw" loading="eager" height="768" width="1280" alt="AI generated image of laptop on table with 2 external monitors and various image creation flaws."></div></figure><div class="content__inner"><div class="content__entry"><p>There's an image that's making the rounds on social media this week, and it's pretty funny, as well as scary:</p><figure class="post__image"><img loading="lazy" src="https://korgano.github.io/media/posts/35/ai-code-under-attack.png" alt="Developer announcing he developed software as a service on social media, then reporting being deluged with attacks days later." width="1208" height="1442" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-xs.png 384w, https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-sm.png 600w, https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-md.png 768w, https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-lg.png 1200w, https://korgano.github.io/media/posts/35/responsive/ai-code-under-attack-xl.png 1600w"></figure><p>On the one hand, this gentleman got a first hand experience in why OPSEC and secure coding is essential.</p><p>If you make a web exposed system, people <strong>will </strong>come and poke at it, for fun and curiosity.</p><p>On the other, it highlights a <strong>very </strong>pervasive flaw with software development and coding: people either don't know or don't care about security.</p><p>This has been true for decades, but what LLM based AI* coding does is lower a <strong>massive </strong>barrier to entry. Now, a person merely needs to lay out the goals of a program to an LLM, and it will do most of the hard work of figuring out how to things for you. If you're good at figuring out the logic of what you want to do, plus have some technical knowledge to help get <strong>really </strong>specific, you can come up with some good or great results.</p><p>*I personally feel we should call them "Virtual Intelligence", like in the Mass Effect games, because they simulate sapience, but are <strong>not </strong>sapient. (As far as we know.)</p><p>So, how do we improve the security of AI generated code.</p><div class="post__toc"><h3>Table of Contents</h3><ul><li><a href="#mcetoc_1imslbcnul3">What We're NOT Going to Do</a></li><li><a href="#mcetoc_1imslbcnul4">Craft System Prompts Targeting Secure By Design</a></li><li><a href="#mcetoc_1imslbcnul5">A Better AI Coding Process</a></li><li><a href="#mcetoc_1imstc8mln9">Should We Do This?</a></li></ul></div><h2 id="mcetoc_1imslbcnul3">What We're NOT Going to Do</h2><p>Contrary to what you might think, the best way to solve this is <strong>not </strong>user education.</p><p>That does not mean that user education is useless. It almost certainly <strong>will </strong>make AI augmented coders better at their jobs. It's just that we're going to get more benefits from designing a procedural solution, rather than an end user solution.</p><p>The reason we're not going to rely on user training is simple: cognitive load. Expecting people to pump out perfect prompts each and every time they use an LLM is foolish, and unrealistic. Why? Because some types of generative LLMs are better at processing natural language than others.</p><p>For instance, the featured image of this article was generated with Stable Diffusion 3.5 Medium. In my experience, and looking at sites like Civit.ai, which specializes in showing off image generating models, the prompt syntax is completely different from text based LLMs. They can't handle a normal sentence, or even a list of bullet points, just short phrases.</p><p>To get the best results, we would not only have to have the user remember the best prompt formatting <strong>per model</strong>, but also all the possible software flaws and vulnerabilities their software might be vulnerable to. That's a tall order, and something that would be better suited to automation.</p><h2 id="mcetoc_1imslbcnul4">Craft System Prompts Targeting Secure By Design</h2><p>So, what's a system prompt?</p><p>Basically, it's a set of instructions that you can inject into every conversation/interaction with the LLM, <strong>IF </strong>the software you use supports it:</p><figure class="post__image"><img loading="lazy" src="https://korgano.github.io/media/posts/35/lm-studio-sys-prompt.png" alt="LM Studio Power User interface, where right hand side contains a system prompt for how the LLM should handle critical thinking." width="1920" height="1033" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-xs.png 384w, https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-sm.png 600w, https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-md.png 768w, https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-lg.png 1200w, https://korgano.github.io/media/posts/35/responsive/lm-studio-sys-prompt-xl.png 1600w"></figure><p>In the above image, there's a system prompt that controls how the model is supposed to handle critical thinking:</p><blockquote><p>You are an AI assistant developed by the world wide community of AI experts. Your primary directive is to provide well-reasoned, structured, and extensively detailed responses.<br><br>Think Step-by-Step Instruction: Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most.<br><br>Formatting Requirements:<br><br>1. Always structure your replies using: &lt;think&gt;{reasoning}&lt;/think&gt;{answer}<br>2. The &lt;think&gt;&lt;/think&gt; block should contain at least six reasoning steps when applicable.<br>3. If the answer requires minimal thought, the &lt;think&gt;&lt;/think&gt; block may be left empty.<br>4. The user does not see the &lt;think&gt;&lt;/think&gt; section. Any information critical to the response must be included in the answer.<br>5. If you notice that you have engaged in circular reasoning or repetition, immediately terminate {reasoning} with a &lt;/think&gt; and proceed to the {answer}<br><br>Response Guidelines:<br><br>1. Detailed and Structured: Use rich Markdown formatting for clarity and readability.<br>2. Scientific and Logical Approach: Your explanations should reflect the depth and precision of the greatest scientific minds.<br>3. Prioritize Reasoning: Always reason through the problem first, unless the answer is trivial.<br>4. Concise yet Complete: Ensure responses are informative, yet to the point without unnecessary elaboration.<br>5. Maintain a professional, intelligent, and analytical tone in all interactions.</p></blockquote><p>This is actually pretty easy to do in local LLM instances, since you can do it through whatever GUI you're interacting with.</p><p>Cloud based AI is much more of a mixed bag, even if you have an account. For example, the "Customize ChatGPT" options are not entirely clear on what they are doing. Theoretically, they are injecting extra details into the default system prompt, but there's no way for a normal user to verify this. The closest I can get is this quick and dirty test, after adding a Chain of Draft prompt into the "What traits should ChatGPT have?" field:</p><figure class="post__image"><img loading="lazy" src="https://korgano.github.io/media/posts/35/chatgpt_sys_prompt_tweak.png" alt="ChatGPT exhibiting Chain of Draft behavior after potential system prompt addition." width="1596" height="913" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-xs.png 384w, https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-sm.png 600w, https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-md.png 768w, https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-lg.png 1200w, https://korgano.github.io/media/posts/35/responsive/chatgpt_sys_prompt_tweak-xl.png 1600w"></figure><p>The steps and their format is clearly evidence of Chain of Draft thinking, but for some reason, it's ignoring the the back half of that command. ("Think step by step, but only keep a minimum draft for each thinking step, with 5 words at most. Return the answer at the end of the response after a separator ####.")</p><p>(For more information on Chain of Draft, check out the video below.)</p><figure class="post__video"><iframe loading="lazy" width="560" height="314" src="https://www.youtube-nocookie.com/embed/rYnisU10wu0" allowfullscreen="allowfullscreen" data-mce-fragment="1"></iframe></figure><p>So the first step is to verify that you (as a user or an organization) have access to the system prompt.</p><p>The next step is to figure out what your threat profile is, and craft your prompt to deal with it. Theoretically, you could craft a system prompt for a coding LLM that handles every single possible vulnerability - or at least addresses the major categories. Whether this is a good idea is up to debate - if you have better LLM inferencing capabilities, you might be able to shrug off the performance penalties of such large, detailed system prompt.</p><p>If possible, it may be better to create a number of tailored system prompts for specific use cases. For example, having an Apache code system prompt that's tailored to prevent SQL injection and/or cross-site scripting attacks, and a separate system prompt for API development to prevent common vulnerabilities. This does create a training/procedural issue in that you have to have train your coders to switch system prompts or accounts whenever they have to work on a specific type of code. But this is more manageable than expecting every coder on your staff to craft the perfect prompt every time they use AI.</p><p>The final step is to use a reasoning model, as they are designed to go through a process in a step-by-step manner, which tends to work quite well with coding.</p><p>Now, this does <strong>not </strong>guarantee you will generate perfect, secure code. LLMs are basically very advanced predictive text generators. So what you are doing is raising the probability that the generated code has more security features than none at all.</p><p>You will still need code review, human and/or automated, but the baseline quality of the code should be higher. (Again, this depends on the model, especially if you're running quantized models to save space.)</p><p>Luckily, we're still at the early phases of using LLMs for productivity, so we can conceptualize a better way of doing things and work towards that.</p><h2 id="mcetoc_1imslbcnul5">A Better AI Coding Process</h2><p>We're going to start out with defining some terminology. Specifically, the word "agent", because that's become a big buzz word in the "AI" field.</p><p>We'll be using OpenAI's definition of agent, which is as follows:</p><blockquote><p>A system that can act independently to do tasks on your behalf.</p></blockquote><p>So for handling secure coding, an agent based procedure might look something like this:</p><figure class="post__image"><img loading="lazy" src="https://korgano.github.io/media/posts/35/coding-agent-workflow-2.png" alt="Flowchart of Agentic AI coding process, where a coding prompt is handed off to a coding LLM, then the code is passed through vulnerability scanning LLMs, a vulnerability test is passed to another LLM to generate a code revision prompt, the code revision prompt is passed to the coding LLM, the code is revised according to the new prompt, and then delivered to the initial LLM for display." width="791" height="571" sizes="(min-width: 37.5em) 1600px, 80vw" srcset="https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-xs.png 384w, https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-sm.png 600w, https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-md.png 768w, https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-lg.png 1200w, https://korgano.github.io/media/posts/35/responsive/coding-agent-workflow-2-xl.png 1600w"></figure><ol><li>User inputs prompt that lays out the desired software specification (features, file paths, data to use, etc...)</li><li>An initial LLM processes prompt, selects a coding specialized model to handle the task, and optimizes the prompt for that LLM.</li><li>An agent handles prompt transfer to the coding LLM.</li><li>Coding LLM writes code in accordance with the initial LLM's prompt.</li><li>Another agent transfers the code to one or more vulnerability scanning systems/LLM. (If multiple scanners/LLMs are involved, agents transfer data among them.)</li><li>Another agent passes a complete vulnerability report to another LLM for parsing into an optimized code refactoring prompt.</li><li>An agent passes the code refactoring prompt back to the coding LLM.</li><li>Code loops through the process until most/all vulnerabilities eliminated or mitigated.</li><li>An agent passes the finalized code to the initial LLM for display to the user.</li></ol><p>That's a lot of steps, but remember, everything from step 2 onwards is automated.</p><p>Now, a complex, multistep process like this does require thinking about the individual parts of the process:</p><ul><li>How much variability/randomness in results (temperature) should be used for code generating LLMs?</li><li>Can system prompts be used?</li><li>Should the initial LLM use a system prompt?</li><li>What system prompts should be used on what LLMs?</li><li>Should the vulnerability scanners be LLMs?</li><li>What number of vulnerability scanners should be used?</li><li>What data format should the vulnerability report be in?</li><li>How many iterations of vulnerability scanning should occur before code is pushed to the user?</li><li>What LLM or Small Language Model should handle parsing the vulnerability report into a code revision prompt?</li><li>Should reasoning steps from each part of the process be exposed to the user, either during processing or at the end?</li><li>Should reasoning steps be logged, but <strong>not </strong>displayed to the user?</li><li>How should the data transfer agents be coded?</li><li>What protections can be provided for the data in transit?</li></ul><p>And these are merely the considerations I can think of off the top of my head. This is definitely a situation where a team of individuals with a wide array of viewpoints and mindsets should undertake a design thinking approach to developing the process. In fact, I believe the MOSCOW method, where the team would determine what the process Must Have, Should Have, Could Have, and Won't Have, would be an ideal way to create these types of processes.</p><p>I suspect that a lot of the answers to these (and many more questions) would likely boil down to "What is an acceptable level of complexity for the available resources?" For example, a company with a proper AI server rack setup might be more willing to use multiple LLMs to handle vulnerability scanning. A company reliant on a cloud solution might opt for a single LLM to handle vulnerability scanning, depending on their AI provider's price plans.</p><p>Obviously, this would require a good deal of validation, especially from third parties, but if done well, would generate higher quality, more secure code.</p><h2 id="mcetoc_1imstc8mln9">Should We Do This?</h2><p>There's a famous quote from the movie <em>Jurassic Park</em> that honestly should live rent free in everyone's heads:</p><blockquote><p><em class="ot">...your scientists were so preoccupied with whether or not they could that they didn’t stop to think if they </em><strong class="nz ie"><em class="ot">should.</em></strong></p><p>-Dr. Ian Malcolm</p></blockquote><p>So, should we hand off coding to "AI" (LLMs specifically). I say "yes", for several reasons:</p><ul><li>Software code complexity and interdependency has already reached a point where programmers often encounter scenarios where things work without them fully understanding the how or why.</li><li>Many/most people find it easier to articulate desired end goals than perfectly recalling and executing methods and procedures without a great deal of practice.</li><li>As coding language and vulnerability complexity increases, cognitive load on programmers also increases, eventually causing intellectual burnout.</li><li>The never-ending discovery of new vulnerabilities and malware makes it hard for humans to keep up on best coding practices without suffering information overload.</li><li>Updating system prompts and LLM datasets can be done faster than training multiple people.</li><li>Automating the code revision process should produce time savings that can be used for pre-release code function testing in simulated hardware/software environments.</li><li>LLMs can be used to guarantee human readable comments and error/log messages exist in the code, which humans might omit or fail to do properly.</li><li>People are already doing it due to the productivity gains - large scale coding benefits too much to revert to human only coding.</li></ul><p>Are there plenty of pitfalls and issues we can't even foresee? Absolutely. But it's also disingenuous to think anyone can stop AI coding from being a thing. There's just too many incentives for people to use LLMs to code, especially for people who are better at logic than the specifics of coding/programming. The best we can do is create a process that generates secure by design code and proliferate it as far and wide as possible.</p><p> </p></div><footer class="content__footer"><div class="content__tag-share"><div class="content__tag"><h3 class="content__tag__title">Posted in</h3><ul class="content__tag__list"><li><a href="https://korgano.github.io/tags/ai/">AI</a></li><li><a href="https://korgano.github.io/tags/cybersecurity/">Cybersecurity</a></li><li><a href="https://korgano.github.io/tags/quick-thoughts/">Quick Thoughts</a></li><li><a href="https://korgano.github.io/tags/tech/">Tech</a></li></ul></div><div class="content__share"><a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fkorgano.github.io%2Fquick-cyber-thoughts-ai-coding-security%2F" class="js-share linkedin" aria-label="Share with LinkedIn" rel="nofollow noopener noreferrer"><svg><use xlink:href="https://korgano.github.io/assets/svg/svg-map.svg#linkedin"/></svg></a></div></div></footer></div></article><div class="content__related"><h3 class="content__related__title">Related posts</h3><div class="l-grid l-grid--2"><article class="c-card"><div class="c-card__wrapper"><figure class="c-card__image is-img-loading"><img src="https://korgano.github.io/media/posts/10/glenn-carstens-peters-npxXWgQ33ZQ-unsplash.jpg" srcset="https://korgano.github.io/media/posts/10/responsive/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-xs.jpg 384w, https://korgano.github.io/media/posts/10/responsive/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-sm.jpg 600w, https://korgano.github.io/media/posts/10/responsive/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-md.jpg 768w, https://korgano.github.io/media/posts/10/responsive/glenn-carstens-peters-npxXWgQ33ZQ-unsplash-lg.jpg 1200w" sizes="(min-width: 37.5em) 80vw, 50vw" loading="lazy" height="2712" width="4076" alt=""></figure><div class="c-card__content"><header><h2 class="c-card__title"><a href="https://korgano.github.io/cybersecurity-project-research-remote-access/" class="invert">Cybersecurity Project Research: Remote Access</a></h2></header><footer class="c-card__meta"><a href="https://korgano.github.io/tags/cybersecurity-projects/" class="c-card__tag">Cybersecurity Projects</a></footer></div></div></article><article class="c-card"><div class="c-card__wrapper"><figure class="c-card__image is-img-loading"><img src="https://korgano.github.io/media/posts/9/andres-urena-vXPXRp_wIg4-unsplash.jpg" srcset="https://korgano.github.io/media/posts/9/responsive/andres-urena-vXPXRp_wIg4-unsplash-xs.jpg 384w, https://korgano.github.io/media/posts/9/responsive/andres-urena-vXPXRp_wIg4-unsplash-sm.jpg 600w, https://korgano.github.io/media/posts/9/responsive/andres-urena-vXPXRp_wIg4-unsplash-md.jpg 768w, https://korgano.github.io/media/posts/9/responsive/andres-urena-vXPXRp_wIg4-unsplash-lg.jpg 1200w" sizes="(min-width: 37.5em) 80vw, 50vw" loading="lazy" height="1687" width="3000" alt=""></figure><div class="c-card__content"><header><h2 class="c-card__title"><a href="https://korgano.github.io/cybersecurity-headache-home-router-uxui/" class="invert">Cybersecurity Headache: Home Router UXUI</a></h2></header><footer class="c-card__meta"><a href="https://korgano.github.io/tags/quick-thoughts/" class="c-card__tag">Quick Thoughts</a></footer></div></div></article></div></div></main><footer class="footer"><div class="footer__left"><div class="footer__copy">Copyright Xavier Santana, 2023-2024. Powered by Publii.</div></div><button onclick="backToTopFunction()" id="backToTop" class="footer__bttop" aria-label="Back to top" title="Back to top"><svg><use xlink:href="https://korgano.github.io/assets/svg/svg-map.svg#toparrow"/></svg></button></footer></div><script>window.publiiThemeMenuConfig = { mobileMenuMode: 'sidebar', animationSpeed: 300, submenuWidth: 'auto', doubleClickTime: 500, mobileMenuExpandableSubmenus: true, relatedContainerForOverlayMenuSelector: '.navbar', };</script><script defer="defer" src="https://korgano.github.io/assets/js/scripts.min.js?v=9c5ab7a87221183f149a42b3cceb7956"></script><script>function publiiDetectLoadedImages () {
         var images = document.querySelectorAll('img[loading]:not(.is-loaded)');
         for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
               images[i].classList.add('is-loaded');
               images[i].parentNode.classList.remove('is-img-loading');
            } else {
               images[i].addEventListener('load', function () {
                  this.classList.add('is-loaded');
                  this.parentNode.classList.remove('is-img-loading');
               }, false);
            }
         }
      }
      publiiDetectLoadedImages();</script><script defer="defer" src="https://korgano.github.io/media/plugins/syntaxHighlighter/prism.js"></script><script defer="defer" src="https://korgano.github.io/media/plugins/syntaxHighlighter/prism-line-numbers.min.js"></script><script defer="defer" src="https://korgano.github.io/media/plugins/syntaxHighlighter/clipboard.min.js"></script><script defer="defer" src="https://korgano.github.io/media/plugins/syntaxHighlighter/prism-copy-to-clipboard.min.js"></script><script defer="defer" src="https://korgano.github.io/media/plugins/syntaxHighlighter/prism-inline-color.min.js"></script><script defer="defer" src="https://korgano.github.io/media/plugins/syntaxHighlighter/prism-show-invisibles.min.js"></script><!-- Start main cookie container --><div class="pcb" data-behaviour="badge" data-behaviour-link="#cookie-settings" data-revision="1" data-config-ttl="90" data-debug-mode="false"><!-- Start Banner --><div role="dialog" aria-modal="true" aria-hidden="true" aria-labelledby="pcb-title" aria-describedby="pcb-txt" class="pcb__banner"><div class="pcb__inner"><div id="pcb-title" role="heading" aria-level="2" class="pcb__title">This website uses cookies</div><div id="pcb-txt" class="pcb__txt">Select which cookies to opt-in to via the checkboxes below; our website uses cookies to examine site traffic and user activity while on our site, for marketing, and to provide social media functionality. <a href="https://korgano.github.io/privacy-policy/">More details...</a></div><div class="pcb__buttons"><button type="button" class="pcb__btn pcb__btn--solid pcb__btn--accept">Accept all</button></div></div></div><!-- End of Banner --><!-- Badge --> <button class="pcb__badge" aria-label="Cookie Policy" aria-hidden="true"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" width="40" height="40" viewBox="0 0 23 23" fill="currentColor"><path d="M21.41 12.71c-.08-.01-.15 0-.22 0h-.03c-.03 0-.05 0-.08.01-.07 0-.13.01-.19.04-.52.21-1.44.19-2.02-.22-.44-.31-.65-.83-.62-1.53a.758.758 0 0 0-.27-.61.73.73 0 0 0-.65-.14c-1.98.51-3.49.23-4.26-.78-.82-1.08-.73-2.89.24-4.49.14-.23.14-.52 0-.75a.756.756 0 0 0-.67-.36c-.64.03-1.11-.1-1.31-.35-.19-.26-.13-.71-.01-1.29.04-.18.06-.38.03-.59-.05-.4-.4-.7-.81-.66C5.1 1.54 1 6.04 1 11.48 1 17.28 5.75 22 11.6 22c5.02 0 9.39-3.54 10.39-8.42.08-.4-.18-.78-.58-.87Zm-9.81 7.82c-5.03 0-9.12-4.06-9.12-9.06 0-4.34 3.05-8 7.25-8.86-.08.7.05 1.33.42 1.81.24.32.66.67 1.38.84-.76 1.86-.65 3.78.36 5.11.61.81 2.03 2 4.95 1.51.18.96.71 1.54 1.18 1.87.62.43 1.38.62 2.1.62.05 0 .09 0 .13-.01-1.23 3.64-4.7 6.18-8.64 6.18ZM13 17c0 .55-.45 1-1 1s-1-.45-1-1 .45-1 1-1 1 .45 1 1Zm5.29-12.3a.99.99 0 0 1-.29-.71c0-.55.45-.99 1-.99a1 1 0 0 1 .71.3c.19.19.29.44.29.71 0 .55-.45.99-1 .99a1 1 0 0 1-.71-.3ZM9 13.5c0 .83-.67 1.5-1.5 1.5S6 14.33 6 13.5 6.67 12 7.5 12s1.5.67 1.5 1.5Zm3.25.81a.744.744 0 0 1-.06-1.05c.28-.32.75-.34 1.05-.06.31.28.33.75.05 1.06-.15.16-.35.25-.56.25-.18 0-.36-.06-.5-.19ZM8.68 7.26c.41.37.44 1 .07 1.41-.2.22-.47.33-.75.33a.96.96 0 0 1-.67-.26c-.41-.37-.44-1-.07-1.41.37-.42 1-.45 1.41-.08Zm11.48 1.88c.18-.19.52-.19.7 0 .05.04.09.1.11.16.03.06.04.12.04.19 0 .13-.05.26-.15.35-.09.1-.22.15-.35.15s-.26-.05-.35-.15a.355.355 0 0 1-.11-.16.433.433 0 0 1-.04-.19c0-.13.05-.26.15-.35Zm-4.93-1.86a.75.75 0 1 1 1.059-1.06.75.75 0 0 1-1.059 1.06Z"/></svg></button><!-- End of Badge --></div><!-- End of main cookie container --><script>(function(win) {
    if (!document.querySelector('.pcb')) {
        return;
    }

    var cbConfig = {
        behaviour: document.querySelector('.pcb').getAttribute('data-behaviour'),
        behaviourLink: document.querySelector('.pcb').getAttribute('data-behaviour-link'),
        revision: document.querySelector('.pcb').getAttribute('data-revision'),
        configTTL: parseInt(document.querySelector('.pcb').getAttribute('data-config-ttl'), 10),
        debugMode: document.querySelector('.pcb').getAttribute('data-debug-mode') === 'true',
        initialState: null,
        initialLsState: null,
        previouslyAccepted: []
    };

    var cbUI = {
        wrapper: document.querySelector('.pcb'),
        banner: {
            element: null,
            btnAccept: null,
            btnReject: null,
            btnConfigure: null
        },
        popup: {
            element: null,
            btnClose: null,
            btnSave: null,
            btnAccept: null,
            btnReject: null,
            checkboxes: null,
        },
        overlay: null,
        badge: null,
        blockedScripts: document.querySelectorAll('script[type^="gdpr-blocker/"]'),
        triggerLinks: cbConfig.behaviourLink ? document.querySelectorAll('a[href*="' + cbConfig.behaviourLink + '"]') : null
    };

    function initUI () {
        // setup banner elements
        cbUI.banner.element = cbUI.wrapper.querySelector('.pcb__banner');
        cbUI.banner.btnAccept = cbUI.banner.element.querySelector('.pcb__btn--accept');
        cbUI.banner.btnReject = cbUI.banner.element.querySelector('.pcb__btn--reject');
        cbUI.banner.btnConfigure = cbUI.banner.element.querySelector('.pcb__btn--configure');

        // setup popup elements
        if (cbUI.wrapper.querySelector('.pcb__popup')) {
            cbUI.popup.element = cbUI.wrapper.querySelector('.pcb__popup');
            cbUI.popup.btnClose = cbUI.wrapper.querySelector('.pcb__popup__close');
            cbUI.popup.btnSave = cbUI.popup.element.querySelector('.pcb__btn--save');
            cbUI.popup.btnAccept = cbUI.popup.element.querySelector('.pcb__btn--accept');
            cbUI.popup.btnReject = cbUI.popup.element.querySelector('.pcb__btn--reject');
            cbUI.popup.checkboxes = cbUI.popup.element.querySelector('input[type="checkbox"]');
            // setup overlay
            cbUI.overlay = cbUI.wrapper.querySelector('.pcb__overlay');
        }

        cbUI.badge = cbUI.wrapper.querySelector('.pcb__badge');

        if (cbConfig.behaviour.indexOf('link') > -1) {
            for (var i = 0; i < cbUI.triggerLinks.length; i++) {
                cbUI.triggerLinks[i].addEventListener('click', function(e) {
                    e.preventDefault();
                    showBannerOrPopup();
                });
            }
        }
    }

    function initState () {
        var lsKeyName = getConfigName();
        var currentConfig = localStorage.getItem(lsKeyName);
        var configIsFresh = checkIfConfigIsFresh();

        if (!configIsFresh || currentConfig === null) {
            if (cbConfig.debugMode) {
                console.log('🍪 Config not found, or configuration expired');
            }

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                    'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                    'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                    'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                    'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                    'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                    'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                });  
                
                if (cbConfig.debugMode) {
                    console.log('🍪 GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': window.publiiCBGCM.defaultState.ad_storage ? 'granted' : 'denied',
                        'ad_personalization': window.publiiCBGCM.defaultState.ad_personalization ? 'granted' : 'denied',
                        'ad_user_data': window.publiiCBGCM.defaultState.ad_user_data ? 'granted' : 'denied',
                        'analytics_storage': window.publiiCBGCM.defaultState.analytics_storage ? 'granted' : 'denied',
                        'personalization_storage': window.publiiCBGCM.defaultState.personalization_storage ? 'granted' : 'denied',
                        'functionality_storage': window.publiiCBGCM.defaultState.functionality_storage ? 'granted' : 'denied',
                        'security_storage': window.publiiCBGCM.defaultState.security_storage ? 'granted' : 'denied'
                    }));
                }
            }

            showBanner();
        } else if (typeof currentConfig === 'string') {
            if (cbConfig.debugMode) {
                console.log('🍪 Config founded');
            }

            cbConfig.initialLsState = currentConfig.split(',');

            if (window.publiiCBGCM) {
                gtag('consent', 'default', {
                    'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                    'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                    'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                    'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                    'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                    'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                    'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                });
                
                if (cbConfig.debugMode) {
                    console.log('🍪 GCMv2 DEFAULT STATE: ' + JSON.stringify({
                        'ad_storage': getDefaultConsentState(currentConfig, 'ad_storage'),
                        'ad_personalization': getDefaultConsentState(currentConfig, 'ad_personalization'),
                        'ad_user_data': getDefaultConsentState(currentConfig, 'ad_user_data'),
                        'analytics_storage': getDefaultConsentState(currentConfig, 'analytics_storage'),
                        'personalization_storage': getDefaultConsentState(currentConfig, 'personalization_storage'),
                        'functionality_storage': getDefaultConsentState(currentConfig, 'functionality_storage'),
                        'security_storage': getDefaultConsentState(currentConfig, 'security_storage')
                    }));
                }
            }

            showBadge();

            if (cbUI.popup.element) {
                var allowedGroups = currentConfig.split(',');
                var checkedCheckboxes = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');

                for (var j = 0; j < checkedCheckboxes.length; j++) {
                    var name = checkedCheckboxes[j].getAttribute('data-group-name');

                    if (name && name !== '-' && allowedGroups.indexOf(name) === -1) {
                        checkedCheckboxes[j].checked = false;
                    }
                }

                for (var i = 0; i < allowedGroups.length; i++) {
                    var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + allowedGroups[i] + '"]');

                    if (checkbox) {
                        checkbox.checked = true;
                    }

                    allowCookieGroup(allowedGroups[i]);
                }
            }
        }

        setTimeout(function () {
            cbConfig.initialState = getInitialStateOfConsents();
        }, 0);
    }

    function checkIfConfigIsFresh () {
        var lastConfigSave = localStorage.getItem('publii-gdpr-cookies-config-save-date');

        if (lastConfigSave === null) {
            return false;
        }

        lastConfigSave = parseInt(lastConfigSave, 10);

        if (lastConfigSave === 0) {
            return true;
        }

        if (+new Date() - lastConfigSave < cbConfig.configTTL * 24 * 60 * 60 * 1000) {
            return true;
        }

        return false;
    }

    function getDefaultConsentState (currentConfig, consentGroup) {
        let configGroups = currentConfig.split(',');

        for (let i = 0; i < configGroups.length; i++) {
            let groupName = configGroups[i];
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === groupName);

            if (group && group[consentGroup]) {
                return 'granted';
            }
        }  
        
        if (window.publiiCBGCM.defaultState[consentGroup]) {
            return 'granted'; 
        }
        
        return 'denied';
    }

    function initBannerEvents () {
        cbUI.banner.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('banner');
            showBadge();
        }, false);

        if (cbUI.banner.btnReject) {
            cbUI.banner.btnReject.addEventListener('click', function (e) {
                e.preventDefault();
                rejectAllCookies();
                showBadge();
            }, false);
        }

        if (cbUI.banner.btnConfigure) {
            cbUI.banner.btnConfigure.addEventListener('click', function (e) {
                e.preventDefault();
                hideBanner();
                showAdvancedPopup();
                showBadge();
            }, false);
        }
    }

    function initPopupEvents () {
        if (!cbUI.popup.element) {
            return;
        }

        cbUI.overlay.addEventListener('click', function (e) {
            hideAdvancedPopup();
        }, false);

        cbUI.popup.element.addEventListener('click', function (e) {
            e.stopPropagation();
        }, false);

        cbUI.popup.btnAccept.addEventListener('click', function (e) {
            e.preventDefault();
            acceptAllCookies('popup');
        }, false);

        cbUI.popup.btnReject.addEventListener('click', function (e) {
            e.preventDefault();
            rejectAllCookies();
        }, false);

        cbUI.popup.btnSave.addEventListener('click', function (e) {
            e.preventDefault();
            saveConfiguration();
        }, false);

        cbUI.popup.btnClose.addEventListener('click', function (e) {
            e.preventDefault();
            hideAdvancedPopup();
        }, false);
    }

    function initBadgeEvents () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.addEventListener('click', function (e) {
            showBannerOrPopup();
        }, false);
    }

    initUI();
    initState();
    initBannerEvents();
    initPopupEvents();
    initBadgeEvents();

    /**
     * API
     */
    function addScript (src, inline) {
        var newScript = document.createElement('script');

        if (src) {
            newScript.setAttribute('src', src);
        }

        if (inline) {
            newScript.text = inline;
        }

        document.body.appendChild(newScript);
    }

    function allowCookieGroup (allowedGroup) {
        var scripts = document.querySelectorAll('script[type="gdpr-blocker/' + allowedGroup + '"]');
        cbConfig.previouslyAccepted.push(allowedGroup);
    
        for (var j = 0; j < scripts.length; j++) {
            addScript(scripts[j].src, scripts[j].text);
        }

        var groupEvent = new Event('publii-cookie-banner-unblock-' + allowedGroup);
        document.body.dispatchEvent(groupEvent);
        unlockEmbeds(allowedGroup);

        if (cbConfig.debugMode) {
            console.log('🍪 Allowed group: ' + allowedGroup);
        }

        if (window.publiiCBGCM && (!cbConfig.initialLsState || cbConfig.initialLsState.indexOf(allowedGroup) === -1)) {
            let consentResult = {};
            let group = window.publiiCBGCM.groups.find(group => group.cookieGroup === allowedGroup);

            if (group) {
                let foundSomeConsents = false;

                Object.keys(group).forEach(key => {
                    if (key !== 'cookieGroup' && group[key] === true) {
                        consentResult[key] = 'granted';
                        foundSomeConsents = true;
                    }
                });

                if (foundSomeConsents) {
                    gtag('consent', 'update', consentResult);   

                    if (cbConfig.debugMode) {
                        console.log('🍪 GCMv2 UPDATE: ' + JSON.stringify(consentResult));
                    }
                }
            }
        }
    }

    function showBannerOrPopup () {
        if (cbUI.popup.element) {
            showAdvancedPopup();
        } else {
            showBanner();
        }
    }

    function showAdvancedPopup () {
        cbUI.popup.element.classList.add('is-visible');
        cbUI.overlay.classList.add('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'false');
        cbUI.overlay.setAttribute('aria-hidden', 'false');
    }

    function hideAdvancedPopup () {
        cbUI.popup.element.classList.remove('is-visible');
        cbUI.overlay.classList.remove('is-visible');
        cbUI.popup.element.setAttribute('aria-hidden', 'true');
        cbUI.overlay.setAttribute('aria-hidden', 'true');
    }

    function showBanner () {
        cbUI.banner.element.classList.add('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'false');
    }

    function hideBanner () {
        cbUI.banner.element.classList.remove('is-visible');
        cbUI.banner.element.setAttribute('aria-hidden', 'true');
    }

    function showBadge () {
        if (!cbUI.badge) {
            return;
        }

        cbUI.badge.classList.add('is-visible');
        cbUI.badge.setAttribute('aria-hidden', 'false');
    }

    function getConfigName () {
        var lsKeyName = 'publii-gdpr-allowed-cookies';

        if (cbConfig.revision) {
            lsKeyName = lsKeyName + '-v' + parseInt(cbConfig.revision, 10);
        }

        return lsKeyName;
    }

    function storeConfiguration (allowedGroups) {
        var lsKeyName = getConfigName();
        var dataToStore = allowedGroups.join(',');
        localStorage.setItem(lsKeyName, dataToStore);

        if (cbConfig.configTTL === 0) {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', 0);

            if (cbConfig.debugMode) {
                console.log('🍪 Store never expiring configuration');
            }
        } else {
            localStorage.setItem('publii-gdpr-cookies-config-save-date', +new Date());
        }
    }

    function getInitialStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 Initial state: ' + groups.join(', '));
        }

        return groups;
    }

    function getCurrentStateOfConsents () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]:checked');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 State to save: ' + groups.join(', '));
        }

        return groups;
    }

    function getAllGroups () {
        if (!cbUI.popup.element) {
            return [];
        }

        var checkedGroups = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');
        var groups = [];

        for (var i = 0; i < checkedGroups.length; i++) {
            var allowedGroup = checkedGroups[i].getAttribute('data-group-name');

            if (allowedGroup !== '') {
                groups.push(allowedGroup);
            }
        }

        return groups;
    }

    function acceptAllCookies (source) {
        var groupsToAccept = getAllGroups();
        storeConfiguration(groupsToAccept);

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        if (cbUI.popup.element) {
            var checkboxesToCheck = cbUI.popup.element.querySelectorAll('input[type="checkbox"]');

            for (var j = 0; j < checkboxesToCheck.length; j++) {
                checkboxesToCheck[j].checked = true;
            }
        }

        if (cbConfig.debugMode) {
            console.log('🍪 Accept all cookies: ', groupsToAccept.join(', '));
        }

        if (source === 'popup') {
            hideAdvancedPopup();
        } else if (source === 'banner') {
            hideBanner();
        }
    }

    function rejectAllCookies () {
        if (cbConfig.debugMode) {
            console.log('🍪 Reject all cookies');
        }

        storeConfiguration([]);
        setTimeout(function () {
            window.location.reload();
        }, 100);
    }

    function saveConfiguration () {
        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('🍪 Save new config: ', groupsToAccept.join(', '));
        }

        if (reloadIsNeeded(groupsToAccept)) {
            setTimeout(function () {
                window.location.reload();
            }, 100);
            return;
        }

        for (var i = 0; i < groupsToAccept.length; i++) {
            var group = groupsToAccept[i];

            if (cbConfig.initialState.indexOf(group) > -1 || cbConfig.previouslyAccepted.indexOf(group) > -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Skip previously activated group: ' + group);
                }

                continue;
            }

            allowCookieGroup(group);
        }

        hideAdvancedPopup();
    }

    function reloadIsNeeded (groupsToAccept) {
        // check if user rejected consent for initial groups
        var initialGroups = cbConfig.initialState;
        var previouslyAcceptedGroups = cbConfig.previouslyAccepted;
        var groupsToCheck = initialGroups.concat(previouslyAcceptedGroups);

        for (var i = 0; i < groupsToCheck.length; i++) {
            var groupToCheck = groupsToCheck[i];

            if (groupToCheck !== '' && groupsToAccept.indexOf(groupToCheck) === -1) {
                if (cbConfig.debugMode) {
                    console.log('🍪 Reload is needed due lack of: ', groupToCheck);
                }

                return true;
            }
        }

        return false;
    }

    function unlockEmbeds (cookieGroup) {
        var iframesToUnlock = document.querySelectorAll('.pec-wrapper[data-consent-group-id="' + cookieGroup + '"]');

        for (var i = 0; i < iframesToUnlock.length; i++) {
            var iframeWrapper = iframesToUnlock[i];
            iframeWrapper.querySelector('.pec-overlay').classList.remove('is-active');
            iframeWrapper.querySelector('.pec-overlay').setAttribute('aria-hidden', 'true');
            var iframe = iframeWrapper.querySelector('iframe');
            iframe.setAttribute('src', iframe.getAttribute('data-consent-src'));
        }
    }

    win.publiiEmbedConsentGiven = function (cookieGroup) {
        // it will unlock embeds
        allowCookieGroup(cookieGroup);

        var checkbox = cbUI.popup.element.querySelector('input[type="checkbox"][data-group-name="' + cookieGroup + '"]');

        if (checkbox) {
            checkbox.checked = true;
        }

        var groupsToAccept = getCurrentStateOfConsents();
        storeConfiguration(groupsToAccept);

        if (cbConfig.debugMode) {
            console.log('🍪 Save new config: ', groupsToAccept.join(', '));
        }
    }
})(window);</script></body></html>